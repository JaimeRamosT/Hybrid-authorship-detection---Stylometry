{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee47bd19",
   "metadata": {},
   "source": [
    "# Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86af06",
   "metadata": {},
   "source": [
    "## Librerías utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aabf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fc379",
   "metadata": {},
   "source": [
    "## Lectura del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2877e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPath = \"dataframe/train_df.csv\"\n",
    "train_df = pd.read_csv(dfPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de documentos con solo una oración: 36\n",
      "Total de documentos: 1000\n",
      "Porcentaje: 3.60%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>POS_ADV</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>POS_INTJ</th>\n",
       "      <th>...</th>\n",
       "      <th>RE</th>\n",
       "      <th>ASF</th>\n",
       "      <th>ASM</th>\n",
       "      <th>OM</th>\n",
       "      <th>RCI</th>\n",
       "      <th>DMC</th>\n",
       "      <th>OR</th>\n",
       "      <th>QAS</th>\n",
       "      <th>PA</th>\n",
       "      <th>PR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0.279762</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.294430</td>\n",
       "      <td>0.233422</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.239216</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.074510</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>0.184466</td>\n",
       "      <td>0.072816</td>\n",
       "      <td>0.087379</td>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.156463</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.115646</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt</td>\n",
       "      <td>books</td>\n",
       "      <td>0.249453</td>\n",
       "      <td>0.260394</td>\n",
       "      <td>0.124726</td>\n",
       "      <td>0.111597</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.013129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.303922</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>326</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.192547</td>\n",
       "      <td>0.291925</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.031056</td>\n",
       "      <td>0.167702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>351</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt</td>\n",
       "      <td>news</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>371</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.214953</td>\n",
       "      <td>0.154206</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4469</th>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4642</th>\n",
       "      <td>424</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.229545</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.084091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5043</th>\n",
       "      <td>463</td>\n",
       "      <td>0</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.193750</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>477</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.195205</td>\n",
       "      <td>0.023973</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.106164</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.319249</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.065728</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5656</th>\n",
       "      <td>520</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>530</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.226852</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.078704</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.032407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>549</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7029</th>\n",
       "      <td>635</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.367713</td>\n",
       "      <td>0.058296</td>\n",
       "      <td>0.040359</td>\n",
       "      <td>0.049327</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031390</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7165</th>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.401316</td>\n",
       "      <td>0.200658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.200658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7166</th>\n",
       "      <td>650</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>686</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-chat</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>687</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9482</th>\n",
       "      <td>788</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>news</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>816</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.181395</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.037209</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.097674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10118</th>\n",
       "      <td>827</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt</td>\n",
       "      <td>news</td>\n",
       "      <td>0.231250</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.056250</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>841</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.222727</td>\n",
       "      <td>0.190909</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664</th>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>cohere</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.197279</td>\n",
       "      <td>0.251701</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11321</th>\n",
       "      <td>928</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.242798</td>\n",
       "      <td>0.144033</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.045267</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12005</th>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0.213483</td>\n",
       "      <td>0.129213</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.061798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12074</th>\n",
       "      <td>991</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>poetry</td>\n",
       "      <td>0.290476</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentence_num         model     domain  POS_VERB  POS_NOUN  \\\n",
       "615     49             0         human     reddit  0.279762  0.130952   \n",
       "688     59             0           mpt     poetry  0.294430  0.233422   \n",
       "976     87             0         human     poetry  0.239216  0.133333   \n",
       "1156   107             0         human     poetry  0.174757  0.184466   \n",
       "1412   126             0          gpt2     poetry  0.000000  0.500000   \n",
       "1413   127             0      mpt-chat     poetry  0.156463  0.231293   \n",
       "2682   232             0           mpt      books  0.249453  0.260394   \n",
       "3252   285             0         human     reddit  0.265000  0.035000   \n",
       "3563   314             0    llama-chat     poetry  0.166667  0.303922   \n",
       "3714   326             0         human     reddit  0.000000  0.008130   \n",
       "3922   348             0    llama-chat     poetry  0.192547  0.291925   \n",
       "3945   351             0           mpt       news  0.268908  0.168067   \n",
       "4163   371             0         human     poetry  0.214953  0.154206   \n",
       "4469   405             0          gpt2     poetry  0.200000  0.196721   \n",
       "4642   424             0         human     poetry  0.229545  0.145455   \n",
       "5043   463             0    llama-chat     poetry  0.193750  0.225000   \n",
       "5156   477             0         human     poetry  0.175000  0.245000   \n",
       "5331   491             0         human     poetry  0.250000  0.195205   \n",
       "5563   512             0         human     poetry  0.319249  0.154930   \n",
       "5656   520             0          gpt3     poetry  0.437500  0.125000   \n",
       "5803   530             0         human     poetry  0.166667  0.271429   \n",
       "5982   536             0         human     reddit  0.222222  0.226852   \n",
       "6090   549             0       mistral     poetry  0.279070  0.011628   \n",
       "7029   635             0         human     poetry  0.367713  0.058296   \n",
       "7165   649             0          gpt2     poetry  0.401316  0.200658   \n",
       "7166   650             0           mpt  abstracts  0.127907  0.395349   \n",
       "7560   686             0  mistral-chat     poetry  0.228571  0.185714   \n",
       "7561   687             0         human     reddit  0.187500  0.164062   \n",
       "9482   788             0          gpt2       news  0.400826  0.198347   \n",
       "9859   816             0         human     poetry  0.181395  0.186047   \n",
       "10118  827             0           mpt       news  0.231250  0.162500   \n",
       "10312  841             0         human     poetry  0.222727  0.190909   \n",
       "10664  866             0        cohere     poetry  0.197279  0.251701   \n",
       "11321  928             0         human     poetry  0.242798  0.144033   \n",
       "12005  984             0         human     poetry  0.134831  0.213483   \n",
       "12074  991             0         human     poetry  0.290476  0.171429   \n",
       "\n",
       "        POS_ADJ   POS_ADV   POS_DET  POS_INTJ  ...        RE       ASF  \\\n",
       "615    0.071429  0.047619  0.083333  0.000000  ...  0.000000  0.000000   \n",
       "688    0.005305  0.005305  0.172414  0.000000  ...  0.000000  0.000000   \n",
       "976    0.078431  0.050980  0.074510  0.007843  ...  0.000000  0.000000   \n",
       "1156   0.072816  0.087379  0.077670  0.004854  ...  0.000000  0.004854   \n",
       "1412   0.000000  0.000000  0.231481  0.000000  ...  0.000000  0.000000   \n",
       "1413   0.040816  0.068027  0.115646  0.006803  ...  0.000000  0.000000   \n",
       "2682   0.124726  0.111597  0.021882  0.008753  ...  0.002188  0.002188   \n",
       "3252   0.060000  0.060000  0.025000  0.030000  ...  0.000000  0.000000   \n",
       "3563   0.117647  0.039216  0.117647  0.000000  ...  0.000000  0.009804   \n",
       "3714   0.000000  0.000000  0.000000  0.016260  ...  0.000000  0.000000   \n",
       "3922   0.049689  0.031056  0.167702  0.000000  ...  0.000000  0.000000   \n",
       "3945   0.067227  0.067227  0.042017  0.000000  ...  0.016807  0.000000   \n",
       "4163   0.093458  0.084112  0.084112  0.000000  ...  0.000000  0.000000   \n",
       "4469   0.196721  0.000000  0.196721  0.000000  ...  0.000000  0.000000   \n",
       "4642   0.079545  0.063636  0.084091  0.009091  ...  0.000000  0.002273   \n",
       "5043   0.100000  0.087500  0.156250  0.000000  ...  0.000000  0.000000   \n",
       "5156   0.090000  0.050000  0.150000  0.000000  ...  0.000000  0.000000   \n",
       "5331   0.023973  0.027397  0.106164  0.003425  ...  0.000000  0.003425   \n",
       "5563   0.028169  0.065728  0.042254  0.000000  ...  0.000000  0.000000   \n",
       "5656   0.062500  0.000000  0.125000  0.000000  ...  0.000000  0.000000   \n",
       "5803   0.033333  0.028571  0.047619  0.000000  ...  0.000000  0.000000   \n",
       "5982   0.037037  0.078704  0.037037  0.004630  ...  0.000000  0.013889   \n",
       "6090   0.279070  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "7029   0.040359  0.049327  0.017937  0.000000  ...  0.000000  0.000000   \n",
       "7165   0.000000  0.197368  0.200658  0.000000  ...  0.000000  0.000000   \n",
       "7166   0.174419  0.023256  0.011628  0.000000  ...  0.011628  0.000000   \n",
       "7560   0.085714  0.085714  0.114286  0.000000  ...  0.000000  0.000000   \n",
       "7561   0.085938  0.054688  0.062500  0.000000  ...  0.000000  0.000000   \n",
       "9482   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "9859   0.037209  0.079070  0.097674  0.000000  ...  0.000000  0.000000   \n",
       "10118  0.068750  0.056250  0.018750  0.000000  ...  0.000000  0.000000   \n",
       "10312  0.068182  0.050000  0.063636  0.000000  ...  0.000000  0.000000   \n",
       "10664  0.051020  0.027211  0.119048  0.000000  ...  0.003401  0.000000   \n",
       "11321  0.028807  0.045267  0.074074  0.000000  ...  0.000000  0.004115   \n",
       "12005  0.129213  0.028090  0.061798  0.000000  ...  0.000000  0.005618   \n",
       "12074  0.057143  0.057143  0.047619  0.000000  ...  0.000000  0.000000   \n",
       "\n",
       "            ASM        OM  RCI       DMC        OR       QAS   PA        PR  \n",
       "615    0.053571  0.000000  0.0  0.000000  0.000000  0.059524  0.0  0.000000  \n",
       "688    0.000000  0.122016  0.0  0.000000  0.000000  0.172414  0.0  0.000000  \n",
       "976    0.019608  0.011765  0.0  0.007843  0.000000  0.031373  0.0  0.000000  \n",
       "1156   0.019417  0.019417  0.0  0.000000  0.000000  0.063107  0.0  0.000000  \n",
       "1412   0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "1413   0.013605  0.000000  0.0  0.000000  0.000000  0.061224  0.0  0.000000  \n",
       "2682   0.017505  0.004376  0.0  0.008753  0.002188  0.013129  0.0  0.002188  \n",
       "3252   0.035000  0.005000  0.0  0.000000  0.000000  0.025000  0.0  0.000000  \n",
       "3563   0.029412  0.000000  0.0  0.000000  0.000000  0.078431  0.0  0.000000  \n",
       "3714   0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "3922   0.012422  0.000000  0.0  0.000000  0.000000  0.105590  0.0  0.000000  \n",
       "3945   0.008403  0.000000  0.0  0.008403  0.000000  0.008403  0.0  0.000000  \n",
       "4163   0.014019  0.009346  0.0  0.000000  0.000000  0.023364  0.0  0.000000  \n",
       "4469   0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "4642   0.009091  0.002273  0.0  0.011364  0.002273  0.034091  0.0  0.004545  \n",
       "5043   0.006250  0.000000  0.0  0.000000  0.000000  0.093750  0.0  0.000000  \n",
       "5156   0.015000  0.000000  0.0  0.000000  0.000000  0.125000  0.0  0.000000  \n",
       "5331   0.013699  0.003425  0.0  0.003425  0.000000  0.095890  0.0  0.000000  \n",
       "5563   0.014085  0.014085  0.0  0.004695  0.000000  0.028169  0.0  0.000000  \n",
       "5656   0.062500  0.000000  0.0  0.000000  0.000000  0.125000  0.0  0.000000  \n",
       "5803   0.000000  0.004762  0.0  0.000000  0.000000  0.014286  0.0  0.000000  \n",
       "5982   0.032407  0.000000  0.0  0.000000  0.000000  0.009259  0.0  0.000000  \n",
       "6090   0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "7029   0.031390  0.004484  0.0  0.000000  0.000000  0.013453  0.0  0.000000  \n",
       "7165   0.000000  0.000000  0.0  0.000000  0.000000  0.200658  0.0  0.000000  \n",
       "7166   0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "7560   0.000000  0.042857  0.0  0.000000  0.000000  0.014286  0.0  0.000000  \n",
       "7561   0.015625  0.007812  0.0  0.000000  0.000000  0.054688  0.0  0.000000  \n",
       "9482   0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.000000  \n",
       "9859   0.009302  0.004651  0.0  0.000000  0.000000  0.060465  0.0  0.004651  \n",
       "10118  0.037500  0.006250  0.0  0.000000  0.000000  0.012500  0.0  0.000000  \n",
       "10312  0.018182  0.009091  0.0  0.004545  0.000000  0.054545  0.0  0.000000  \n",
       "10664  0.017007  0.000000  0.0  0.000000  0.000000  0.095238  0.0  0.000000  \n",
       "11321  0.037037  0.004115  0.0  0.004115  0.000000  0.045267  0.0  0.000000  \n",
       "12005  0.005618  0.005618  0.0  0.000000  0.000000  0.056180  0.0  0.000000  \n",
       "12074  0.028571  0.004762  0.0  0.000000  0.000000  0.028571  0.0  0.000000  \n",
       "\n",
       "[36 rows x 200 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verificar documentos con solo una oración (sentence_num = 0 únicamente)\n",
    "# Contar oraciones por ID\n",
    "oraciones_por_id = train_df.groupby('id')['sentence_num'].apply(lambda x: x.tolist())\n",
    "\n",
    "# Filtrar IDs que solo tienen sentence_num = 0\n",
    "ids_una_oracion = oraciones_por_id[oraciones_por_id.apply(lambda x: x == [0])].index\n",
    "\n",
    "print(f\"Número de documentos con solo una oración: {len(ids_una_oracion)}\")\n",
    "print(f\"Total de documentos: {train_df['id'].nunique()}\")\n",
    "print(f\"Porcentaje: {len(ids_una_oracion) / train_df['id'].nunique() * 100:.2f}%\")\n",
    "\n",
    "# Mostrar df con los documentos con solo una oración\n",
    "df_una_oracion = train_df[train_df['id'].isin(ids_una_oracion)]\n",
    "display(df_una_oracion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8ad33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del dataset de entrenamiento:\n",
      "Forma del dataset: (12168, 200)\n",
      "Columnas: ['id', 'sentence_num', 'model', 'domain', 'POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP', 'POS_PRO', 'L_REF', 'L_HASHTAG', 'L_MENTION', 'L_RT', 'L_LINKS', 'L_CONT_A', 'L_FUNC_A', 'L_CONT_T', 'L_FUNC_T', 'L_PLURAL_NOUNS', 'L_SINGULAR_NOUNS', 'L_PROPER_NAME', 'L_PERSONAL_NAME', 'L_NOUN_PHRASES', 'L_PUNCT', 'L_PUNCT_DOT', 'L_PUNCT_COM', 'L_PUNCT_SEMC', 'L_PUNCT_COL', 'L_PUNCT_DASH', 'L_POSSESSIVES', 'L_ADJ_POSITIVE', 'L_ADJ_COMPARATIVE', 'L_ADJ_SUPERLATIVE', 'L_ADV_POSITIVE', 'L_ADV_COMPARATIVE', 'L_ADV_SUPERLATIVE', 'PS_CONTRADICTION', 'PS_AGREEMENT', 'PS_EXAMPLES', 'PS_CONSEQUENCE', 'PS_CAUSE', 'PS_LOCATION', 'PS_TIME', 'PS_CONDITION', 'PS_MANNER', 'SY_QUESTION', 'SY_NARRATIVE', 'SY_NEGATIVE_QUESTIONS', 'SY_SPECIAL_QUESTIONS', 'SY_TAG_QUESTIONS', 'SY_GENERAL_QUESTIONS', 'SY_EXCLAMATION', 'SY_IMPERATIVE', 'SY_SUBORD_SENT', 'SY_SUBORD_SENT_PUNCT', 'SY_COORD_SENT', 'SY_COORD_SENT_PUNCT', 'SY_SIMPLE_SENT', 'SY_INVERSE_PATTERNS', 'SY_SIMILE', 'SY_FRONTING', 'SY_IRRITATION', 'SY_INTENSIFIER', 'SY_QUOT', 'VT_PRESENT_SIMPLE', 'VT_PRESENT_PROGRESSIVE', 'VT_PRESENT_PERFECT', 'VT_PRESENT_PERFECT_PROGR', 'VT_PRESENT_SIMPLE_PASSIVE', 'VT_PRESENT_PROGR_PASSIVE', 'VT_PRESENT_PERFECT_PASSIVE', 'VT_PAST_SIMPLE', 'VT_PAST_SIMPLE_BE', 'VT_PAST_PROGR', 'VT_PAST_PERFECT', 'VT_PAST_PERFECT_PROGR', 'VT_PAST_SIMPLE_PASSIVE', 'VT_PAST_POGR_PASSIVE', 'VT_PAST_PERFECT_PASSIVE', 'VT_FUTURE_SIMPLE', 'VT_FUTURE_PROGRESSIVE', 'VT_FUTURE_PERFECT', 'VT_FUTURE_PERFECT_PROGR', 'VT_FUTURE_SIMPLE_PASSIVE', 'VT_FUTURE_PROGR_PASSIVE', 'VT_FUTURE_PERFECT_PASSIVE', 'VT_WOULD', 'VT_WOULD_PASSIVE', 'VT_WOULD_PROGRESSIVE', 'VT_WOULD_PERFECT', 'VT_WOULD_PERFECT_PASSIVE', 'VT_SHOULD', 'VT_SHOULD_PASSIVE', 'VT_SHALL', 'VT_SHALL_PASSIVE', 'VT_SHOULD_PROGRESSIVE', 'VT_SHOULD_PERFECT', 'VT_SHOULD_PERFECT_PASSIVE', 'VT_MUST', 'VT_MUST_PASSIVE', 'VT_MUST_PROGRESSIVE', 'VT_MUST_PERFECT', 'VT_MST_PERFECT_PASSIVE', 'VT_CAN', 'VT_CAN_PASSIVE', 'VT_COULD', 'VT_COULD_PASSIVE', 'VT_CAN_PROGRESSIVE', 'VT_COULD_PROGRESSIVE', 'VT_COULD_PERFECT', 'VT_COULD_PERFECT_PASSIVE', 'VT_MAY', 'VT_MAY_PASSIVE', 'VT_MIGHT', 'VT_MIGHT_PASSIVE', 'VT_MAY_PROGRESSIVE', 'VT_MIGTH_PERFECT', 'VT_MIGHT_PERFECT_PASSIVE', 'VT_MAY_PERFECT_PASSIVE', 'ST_TYPE_TOKEN_RATIO_LEMMAS', 'ST_HERDAN_TTR', 'ST_MASS_TTR', 'ST_SENT_WRDSPERSENT', 'ST_SENT_DIFFERENCE', 'ST_REPETITIONS_WORDS', 'ST_REPETITIONS_SENT', 'ST_SENT_D_VP', 'ST_SENT_D_NP', 'ST_SENT_D_PP', 'ST_SENT_D_ADJP', 'ST_SENT_D_ADVP', 'L_I_PRON', 'L_HE_PRON', 'L_SHE_PRON', 'L_IT_PRON', 'L_YOU_PRON', 'L_WE_PRON', 'L_THEY_PRON', 'L_ME_PRON', 'L_YOU_OBJ_PRON', 'L_HIM_PRON', 'L_HER_OBJECT_PRON', 'L_IT_OBJECT_PRON', 'L_US_PRON', 'L_THEM_PRON', 'L_MY_PRON', 'L_YOUR_PRON', 'L_HIS_PRON', 'L_HER_PRON', 'L_ITS_PRON', 'L_OUR_PRON', 'L_THEIR_PRON', 'L_YOURS_PRON', 'L_THEIRS_PRON', 'L_HERS_PRON', 'L_OURS_PRON', 'L_MYSELF_PRON', 'L_YOURSELF_PRON', 'L_HIMSELF_PRON', 'L_HERSELF_PRON', 'L_ITSELF_PRON', 'L_OURSELVES_PRON', 'L_YOURSELVES_PRON', 'L_THEMSELVES_PRON', 'L_FIRST_PERSON_SING_PRON', 'L_SECOND_PERSON_PRON', 'L_THIRD_PERSON_SING_PRON', 'L_THIRD_PERSON_PLURAL_PRON', 'VF_INFINITIVE', 'G_PASSIVE', 'G_ACTIVE', 'G_PRESENT', 'G_PAST', 'G_FUTURE', 'G_MODALS_SIMPLE', 'G_MODALS_CONT', 'G_MODALS_PERFECT', 'AN', 'DDP', 'SVP', 'CDS', 'DDF', 'IS', 'PS', 'RE', 'ASF', 'ASM', 'OM', 'RCI', 'DMC', 'OR', 'QAS', 'PA', 'PR']\n",
      "Modelos unicos: ['human' 'gpt3' 'gpt2' 'mpt-chat' 'mistral-chat' 'mpt' 'llama-chat'\n",
      " 'mistral' 'chatgpt' 'cohere' 'cohere-chat' 'gpt4']\n",
      "Dominios unicos: ['reviews' 'abstracts' 'wiki' 'books' 'news' 'reddit' 'poetry']\n"
     ]
    }
   ],
   "source": [
    "print(\"Información del dataset de entrenamiento:\")\n",
    "print(f\"Forma del dataset: {train_df.shape}\")\n",
    "print(f\"Columnas: {list(train_df.columns)}\")\n",
    "print(f\"Modelos unicos: {train_df['model'].unique()}\")\n",
    "print(f\"Dominios unicos: {train_df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e3c21",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0478a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "model\n",
      "True     6839\n",
      "False    5329\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total de oraciones: 12168\n"
     ]
    }
   ],
   "source": [
    "# 1. Crear etiqueta binaria: 1 = Humano (model == 'human'), 0 = IA (resto)\n",
    "print(\"Distribución de clases:\")\n",
    "print((train_df['model'] == 'human').value_counts())\n",
    "print(f\"\\nTotal de oraciones: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a98d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de features: 196\n",
      "Primeros 10 features: ['POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP']\n"
     ]
    }
   ],
   "source": [
    "# 2. Definir columnas de features (excluir metadatos y target)\n",
    "# Excluir: id, sentence_num, model, domain\n",
    "metadata_cols = ['id', 'sentence_num', 'model', 'domain']\n",
    "feature_columns = [col for col in train_df.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"\\nTotal de features: {len(feature_columns)}\")\n",
    "print(f\"Primeros 10 features: {feature_columns[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9398457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de documentos únicos: 1000\n"
     ]
    }
   ],
   "source": [
    "# 3. Obtener IDs únicos y crear mapping de ID a clase\n",
    "unique_ids = train_df['id'].unique()\n",
    "print(f\"\\nTotal de documentos únicos: {len(unique_ids)}\")\n",
    "\n",
    "# Mapping de ID a clase (binaria: humano vs IA)\n",
    "id_to_class = train_df.groupby('id')['model'].first().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac531c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de documentos por clase:\n",
      "human           500\n",
      "gpt2             64\n",
      "mistral-chat     62\n",
      "mpt-chat         61\n",
      "mpt              58\n",
      "mistral          54\n",
      "llama-chat       50\n",
      "chatgpt          35\n",
      "gpt3             34\n",
      "cohere           31\n",
      "cohere-chat      27\n",
      "gpt4             24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clase 0 (IA): 500 documentos\n",
      "Clase 1 (Humano): 64 documentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23852\\198586682.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"\\nClase 0 (IA): {class_distribution.get(0, 0)} documentos\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23852\\198586682.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"Clase 1 (Humano): {class_distribution.get(1, 0)} documentos\")\n"
     ]
    }
   ],
   "source": [
    "# Verificar cuántos documentos hay por clase\n",
    "print(\"Distribución de documentos por clase:\")\n",
    "class_distribution = pd.Series(id_to_class.values()).value_counts()\n",
    "print(class_distribution)\n",
    "print(f\"\\nClase 0 (IA): {class_distribution.get(0, 0)} documentos\")\n",
    "print(f\"Clase 1 (Humano): {class_distribution.get(1, 0)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97c5150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Usando stratified split (mantiene proporción de clases)\n",
      "\n",
      "Documentos en train: 800\n",
      "Documentos en test: 200\n"
     ]
    }
   ],
   "source": [
    "# 4. Split de IDs (no de oraciones) - IMPORTANTE para evitar data leakage\n",
    "ids_list = list(id_to_class.keys())\n",
    "labels_list = [id_to_class[id_] for id_ in ids_list]\n",
    "\n",
    "# Verificar si podemos usar stratify\n",
    "class_counts = pd.Series(labels_list).value_counts()\n",
    "can_stratify = class_counts.min() >= 2\n",
    "\n",
    "if can_stratify:\n",
    "    print(\"✓ Usando stratified split (mantiene proporción de clases)\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels_list\n",
    "    )\n",
    "else:\n",
    "    print(f\"⚠️ No se puede usar stratify (clase mínima: {class_counts.min()} documentos)\")\n",
    "    print(\"Usando split aleatorio sin stratify\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=None  # Sin estratificación\n",
    "    )\n",
    "\n",
    "print(f\"\\nDocumentos en train: {len(train_ids)}\")\n",
    "print(f\"Documentos en test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "effebb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oraciones en train: 9594\n",
      "Oraciones en test: 2574\n",
      "\n",
      "Distribución en train:\n",
      "model\n",
      "human           5395\n",
      "gpt2             745\n",
      "llama-chat       491\n",
      "mistral          467\n",
      "mistral-chat     466\n",
      "mpt              441\n",
      "chatgpt          382\n",
      "mpt-chat         290\n",
      "cohere           275\n",
      "gpt4             251\n",
      "gpt3             209\n",
      "cohere-chat      182\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución en test:\n",
      "model\n",
      "human           1444\n",
      "gpt2             270\n",
      "mpt              129\n",
      "chatgpt          123\n",
      "mistral-chat     117\n",
      "llama-chat       116\n",
      "mistral           96\n",
      "mpt-chat          74\n",
      "gpt4              66\n",
      "gpt3              62\n",
      "cohere            45\n",
      "cohere-chat       32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5. Filtrar oraciones según los IDs\n",
    "train_sentences = train_df[train_df['id'].isin(train_ids)]\n",
    "test_sentences = train_df[train_df['id'].isin(test_ids)]\n",
    "\n",
    "print(f\"\\nOraciones en train: {len(train_sentences)}\")\n",
    "print(f\"Oraciones en test: {len(test_sentences)}\")\n",
    "\n",
    "# Verificar distribución en cada conjunto\n",
    "print(f\"\\nDistribución en train:\")\n",
    "# print(train_sentences['is_human'].value_counts())\n",
    "print(train_sentences['model'].value_counts())\n",
    "print(f\"\\nDistribución en test:\")\n",
    "# print(test_sentences['is_human'].value_counts())\n",
    "print(test_sentences['model'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b469c06",
   "metadata": {},
   "source": [
    "## Preparación de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8668febf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Shape de X_train: (9594, 196)\n",
      "Shape de y_train: (9594,)\n",
      "Shape de X_test: (2574, 196)\n",
      "Shape de y_test: (2574,)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. Preparar X e y\n",
    "X_train = train_sentences[feature_columns].values\n",
    "y_train = train_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "X_test = test_sentences[feature_columns].values\n",
    "y_test = test_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Shape de X_train: {X_train.shape}\")\n",
    "print(f\"Shape de y_train: {y_train.shape}\")\n",
    "print(f\"Shape de X_test: {X_test.shape}\")\n",
    "print(f\"Shape de y_test: {y_test.shape}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebaa8044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando valores NaN en el dataset...\n",
      "\n",
      "NaNs en train_df: 39\n",
      "NaNs en feature_columns:\n",
      "ST_HERDAN_TTR    39\n",
      "dtype: int64\n",
      "\n",
      "NaNs en X_train: 31\n",
      "NaNs en X_test: 8\n",
      "NaNs en y_train: 0\n",
      "NaNs en y_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay valores NaN en los datos\n",
    "print(\"Verificando valores NaN en el dataset...\")\n",
    "print(f\"\\nNaNs en train_df: {train_df.isna().sum().sum()}\")\n",
    "print(f\"NaNs en feature_columns:\")\n",
    "nan_features = train_df[feature_columns].isna().sum()\n",
    "nan_features_with_nans = nan_features[nan_features > 0]\n",
    "if len(nan_features_with_nans) > 0:\n",
    "    print(nan_features_with_nans)\n",
    "else:\n",
    "    print(\"No hay NaNs en las features ✓\")\n",
    "\n",
    "print(f\"\\nNaNs en X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"NaNs en X_test: {np.isnan(X_test).sum()}\")\n",
    "print(f\"NaNs en y_train: {np.isnan(y_train).sum()}\")\n",
    "print(f\"NaNs en y_test: {np.isnan(y_test).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "338c0d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Valores NaN imputados con la media de cada feature\n",
      "NaNs en X_train_clean: 0\n",
      "NaNs en X_test_clean: 0\n"
     ]
    }
   ],
   "source": [
    "# Solución: Imputar valores NaN antes de entrenar\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Opción 1: Imputar con la media de cada feature\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Ajustar el imputer con los datos de entrenamiento\n",
    "X_train_clean = imputer.fit_transform(X_train)\n",
    "# Transformar los datos de test con el mismo imputer\n",
    "X_test_clean = imputer.transform(X_test)\n",
    "\n",
    "print(\"✓ Valores NaN imputados con la media de cada feature\")\n",
    "print(f\"NaNs en X_train_clean: {np.isnan(X_train_clean).sum()}\")\n",
    "print(f\"NaNs en X_test_clean: {np.isnan(X_test_clean).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e8566",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d225607",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b966c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de componentes para explicar el 95% de la varianza: 24\n"
     ]
    }
   ],
   "source": [
    "pca = PCA().fit(X_train_clean)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Número de componentes para explicar el 95% de la varianza: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9707f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train_clean)\n",
    "X_test_pca = pca.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0590d",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b66c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Aplicar LDA para reducir la dimensionalidad\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)  # La cantidad de componentes debe ser <= número de clases - 1\n",
    "x_train_lda = lda.fit_transform(X_train_clean, y_train)\n",
    "x_test_lda = lda.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36bb11",
   "metadata": {},
   "source": [
    "### FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a709f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of factors to retain: 1\n"
     ]
    }
   ],
   "source": [
    "fa = FactorAnalysis().fit(X_train_clean)\n",
    "\n",
    "singular_values = fa.components_\n",
    "explained_variance = np.var(singular_values, axis=1) / np.var(X_train_clean, axis=0).sum()\n",
    "\n",
    "# Calcula la varianza explicada acumulativa\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Determina el número de factores necesarios para explicar al menos el 95% de la varianza\n",
    "num_factors = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of factors to retain: {num_factors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d8c1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(n_components=1)\n",
    "x_train_fa = fa.fit_transform(X_train_clean)\n",
    "x_test_fa = fa.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ad4e2",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03fdbc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain: 25\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=min(X_train_clean.shape) - 1)\n",
    "svd.fit(X_train_clean)\n",
    "\n",
    "# Calcula la varianza explicada por los componentes\n",
    "explained_variance = svd.explained_variance_ratio_\n",
    "\n",
    "# Calcula la varianza explicada acumulativa\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Determina el número de componentes necesarios para explicar al menos el 95% de la varianza\n",
    "num_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of components to retain: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6467c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=25)\n",
    "x_train_svd = svd.fit_transform(X_train_clean)\n",
    "x_test_svd = svd.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd95c0a",
   "metadata": {},
   "source": [
    "### JL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59773a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain using JL: 226\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.9\n",
    "\n",
    "# Calcula el número mínimo de componentes necesarios usando la Proyección de Johnson-Lindenstrauss\n",
    "n_samples = X_train_clean.shape[0]\n",
    "n_components = johnson_lindenstrauss_min_dim(n_samples, eps=epsilon)\n",
    "print(f\"Number of components to retain using JL: {n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ac6f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\random_projection.py:411: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (196 < 224).The dimensionality of the problem will not be reduced.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jl = SparseRandomProjection(n_components=224)\n",
    "x_train_jl = jl.fit_transform(X_train_clean)\n",
    "x_test_jl = jl.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88a0ac",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b32e34",
   "metadata": {},
   "source": [
    "### Función de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fb810cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(y_test, y_pred):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RESULTADOS EN TEST SET (Split por Documento)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['IA', 'Humano'], digits=4))\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(f\"\\nInterpretación:\")\n",
    "    print(f\"  TN (IA correctamente clasificada): {cm[0,0]}\")\n",
    "    print(f\"  FP (IA clasificada como Humano): {cm[0,1]}\")\n",
    "    print(f\"  FN (Humano clasificado como IA): {cm[1,0]}\")\n",
    "    print(f\"  TP (Humano correctamente clasificado): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6783a0",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4709d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo entrenado exitosamente\n"
     ]
    }
   ],
   "source": [
    "trainX, testX = X_train_clean, X_test_clean\n",
    "# trainX, testX = X_train_pca, X_test_pca\n",
    "# trainX, testX = x_train_lda, x_test_lda\n",
    "# trainX, testX = x_train_fa, x_test_fa\n",
    "# trainX, testX = x_train_svd, x_test_svd\n",
    "# trainX, testX = x_train_jl, x_test_jl\n",
    "\n",
    "C = 10\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=C, gamma='scale', random_state=42)\n",
    "svm_model.fit(trainX, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(testX)\n",
    "\n",
    "print(\"✓ Modelo entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0a3c4",
   "metadata": {},
   "source": [
    "### Mostrar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c3180b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTADOS EN TEST SET (Split por Documento)\n",
      "============================================================\n",
      "\n",
      "Accuracy: 0.6418\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          IA     0.6368    0.4283    0.5122      1130\n",
      "      Humano     0.6439    0.8089    0.7170      1444\n",
      "\n",
      "    accuracy                         0.6418      2574\n",
      "   macro avg     0.6404    0.6186    0.6146      2574\n",
      "weighted avg     0.6408    0.6418    0.6271      2574\n",
      "\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[ 484  646]\n",
      " [ 276 1168]]\n",
      "\n",
      "Interpretación:\n",
      "  TN (IA correctamente clasificada): 484\n",
      "  FP (IA clasificada como Humano): 646\n",
      "  FN (Humano clasificado como IA): 276\n",
      "  TP (Humano correctamente clasificado): 1168\n"
     ]
    }
   ],
   "source": [
    "showResults(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
