{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee47bd19",
   "metadata": {},
   "source": [
    "# Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86af06",
   "metadata": {},
   "source": [
    "## Librer√≠as utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7aabf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fc379",
   "metadata": {},
   "source": [
    "## Lectura del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2877e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPath = \"dataframe/train_df.csv\"\n",
    "train_df = pd.read_csv(dfPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>POS_ADV</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>POS_INTJ</th>\n",
       "      <th>...</th>\n",
       "      <th>RE</th>\n",
       "      <th>ASF</th>\n",
       "      <th>ASM</th>\n",
       "      <th>OM</th>\n",
       "      <th>RCI</th>\n",
       "      <th>DMC</th>\n",
       "      <th>OR</th>\n",
       "      <th>QAS</th>\n",
       "      <th>PA</th>\n",
       "      <th>PR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>human</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>human</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>human</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>human</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12163</th>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12164</th>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12165</th>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12166</th>\n",
       "      <td>999</td>\n",
       "      <td>3</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>999</td>\n",
       "      <td>4</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12168 rows √ó 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentence_num     model     domain  POS_VERB  POS_NOUN   POS_ADJ  \\\n",
       "0        0             0     human    reviews  0.176471  0.117647  0.058824   \n",
       "1        0             1     human    reviews  0.250000  0.125000  0.000000   \n",
       "2        0             2     human    reviews  0.250000  0.125000  0.250000   \n",
       "3        0             3     human    reviews  0.230769  0.230769  0.153846   \n",
       "4        0             4     human    reviews  0.333333  0.142857  0.047619   \n",
       "...    ...           ...       ...        ...       ...       ...       ...   \n",
       "12163  999             0  mpt-chat  abstracts  0.153846  0.346154  0.230769   \n",
       "12164  999             1  mpt-chat  abstracts  0.250000  0.343750  0.062500   \n",
       "12165  999             2  mpt-chat  abstracts  0.232143  0.321429  0.160714   \n",
       "12166  999             3  mpt-chat  abstracts  0.260870  0.304348  0.130435   \n",
       "12167  999             4  mpt-chat  abstracts  0.225000  0.425000  0.125000   \n",
       "\n",
       "        POS_ADV   POS_DET  POS_INTJ  ...        RE  ASF       ASM        OM  \\\n",
       "0      0.058824  0.117647       0.0  ...  0.000000  0.0  0.000000  0.000000   \n",
       "1      0.250000  0.000000       0.0  ...  0.000000  0.0  0.000000  0.000000   \n",
       "2      0.125000  0.125000       0.0  ...  0.000000  0.0  0.000000  0.000000   \n",
       "3      0.153846  0.153846       0.0  ...  0.000000  0.0  0.000000  0.000000   \n",
       "4      0.047619  0.095238       0.0  ...  0.000000  0.0  0.047619  0.000000   \n",
       "...         ...       ...       ...  ...       ...  ...       ...       ...   \n",
       "12163  0.000000  0.076923       0.0  ...  0.000000  0.0  0.000000  0.000000   \n",
       "12164  0.000000  0.093750       0.0  ...  0.000000  0.0  0.031250  0.000000   \n",
       "12165  0.035714  0.035714       0.0  ...  0.017857  0.0  0.017857  0.017857   \n",
       "12166  0.086957  0.021739       0.0  ...  0.000000  0.0  0.000000  0.021739   \n",
       "12167  0.175000  0.025000       0.0  ...  0.000000  0.0  0.025000  0.000000   \n",
       "\n",
       "       RCI    DMC   OR       QAS        PA   PR  \n",
       "0      0.0  0.000  0.0  0.000000  0.000000  0.0  \n",
       "1      0.0  0.000  0.0  0.000000  0.000000  0.0  \n",
       "2      0.0  0.000  0.0  0.125000  0.000000  0.0  \n",
       "3      0.0  0.000  0.0  0.076923  0.000000  0.0  \n",
       "4      0.0  0.000  0.0  0.095238  0.000000  0.0  \n",
       "...    ...    ...  ...       ...       ...  ...  \n",
       "12163  0.0  0.000  0.0  0.038462  0.000000  0.0  \n",
       "12164  0.0  0.000  0.0  0.031250  0.000000  0.0  \n",
       "12165  0.0  0.000  0.0  0.000000  0.017857  0.0  \n",
       "12166  0.0  0.000  0.0  0.000000  0.000000  0.0  \n",
       "12167  0.0  0.025  0.0  0.000000  0.000000  0.0  \n",
       "\n",
       "[12168 rows x 200 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8ad33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informaci√≥n del dataset de entrenamiento:\n",
      "Forma del dataset: (12168, 200)\n",
      "Columnas: ['id', 'sentence_num', 'model', 'domain', 'POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP', 'POS_PRO', 'L_REF', 'L_HASHTAG', 'L_MENTION', 'L_RT', 'L_LINKS', 'L_CONT_A', 'L_FUNC_A', 'L_CONT_T', 'L_FUNC_T', 'L_PLURAL_NOUNS', 'L_SINGULAR_NOUNS', 'L_PROPER_NAME', 'L_PERSONAL_NAME', 'L_NOUN_PHRASES', 'L_PUNCT', 'L_PUNCT_DOT', 'L_PUNCT_COM', 'L_PUNCT_SEMC', 'L_PUNCT_COL', 'L_PUNCT_DASH', 'L_POSSESSIVES', 'L_ADJ_POSITIVE', 'L_ADJ_COMPARATIVE', 'L_ADJ_SUPERLATIVE', 'L_ADV_POSITIVE', 'L_ADV_COMPARATIVE', 'L_ADV_SUPERLATIVE', 'PS_CONTRADICTION', 'PS_AGREEMENT', 'PS_EXAMPLES', 'PS_CONSEQUENCE', 'PS_CAUSE', 'PS_LOCATION', 'PS_TIME', 'PS_CONDITION', 'PS_MANNER', 'SY_QUESTION', 'SY_NARRATIVE', 'SY_NEGATIVE_QUESTIONS', 'SY_SPECIAL_QUESTIONS', 'SY_TAG_QUESTIONS', 'SY_GENERAL_QUESTIONS', 'SY_EXCLAMATION', 'SY_IMPERATIVE', 'SY_SUBORD_SENT', 'SY_SUBORD_SENT_PUNCT', 'SY_COORD_SENT', 'SY_COORD_SENT_PUNCT', 'SY_SIMPLE_SENT', 'SY_INVERSE_PATTERNS', 'SY_SIMILE', 'SY_FRONTING', 'SY_IRRITATION', 'SY_INTENSIFIER', 'SY_QUOT', 'VT_PRESENT_SIMPLE', 'VT_PRESENT_PROGRESSIVE', 'VT_PRESENT_PERFECT', 'VT_PRESENT_PERFECT_PROGR', 'VT_PRESENT_SIMPLE_PASSIVE', 'VT_PRESENT_PROGR_PASSIVE', 'VT_PRESENT_PERFECT_PASSIVE', 'VT_PAST_SIMPLE', 'VT_PAST_SIMPLE_BE', 'VT_PAST_PROGR', 'VT_PAST_PERFECT', 'VT_PAST_PERFECT_PROGR', 'VT_PAST_SIMPLE_PASSIVE', 'VT_PAST_POGR_PASSIVE', 'VT_PAST_PERFECT_PASSIVE', 'VT_FUTURE_SIMPLE', 'VT_FUTURE_PROGRESSIVE', 'VT_FUTURE_PERFECT', 'VT_FUTURE_PERFECT_PROGR', 'VT_FUTURE_SIMPLE_PASSIVE', 'VT_FUTURE_PROGR_PASSIVE', 'VT_FUTURE_PERFECT_PASSIVE', 'VT_WOULD', 'VT_WOULD_PASSIVE', 'VT_WOULD_PROGRESSIVE', 'VT_WOULD_PERFECT', 'VT_WOULD_PERFECT_PASSIVE', 'VT_SHOULD', 'VT_SHOULD_PASSIVE', 'VT_SHALL', 'VT_SHALL_PASSIVE', 'VT_SHOULD_PROGRESSIVE', 'VT_SHOULD_PERFECT', 'VT_SHOULD_PERFECT_PASSIVE', 'VT_MUST', 'VT_MUST_PASSIVE', 'VT_MUST_PROGRESSIVE', 'VT_MUST_PERFECT', 'VT_MST_PERFECT_PASSIVE', 'VT_CAN', 'VT_CAN_PASSIVE', 'VT_COULD', 'VT_COULD_PASSIVE', 'VT_CAN_PROGRESSIVE', 'VT_COULD_PROGRESSIVE', 'VT_COULD_PERFECT', 'VT_COULD_PERFECT_PASSIVE', 'VT_MAY', 'VT_MAY_PASSIVE', 'VT_MIGHT', 'VT_MIGHT_PASSIVE', 'VT_MAY_PROGRESSIVE', 'VT_MIGTH_PERFECT', 'VT_MIGHT_PERFECT_PASSIVE', 'VT_MAY_PERFECT_PASSIVE', 'ST_TYPE_TOKEN_RATIO_LEMMAS', 'ST_HERDAN_TTR', 'ST_MASS_TTR', 'ST_SENT_WRDSPERSENT', 'ST_SENT_DIFFERENCE', 'ST_REPETITIONS_WORDS', 'ST_REPETITIONS_SENT', 'ST_SENT_D_VP', 'ST_SENT_D_NP', 'ST_SENT_D_PP', 'ST_SENT_D_ADJP', 'ST_SENT_D_ADVP', 'L_I_PRON', 'L_HE_PRON', 'L_SHE_PRON', 'L_IT_PRON', 'L_YOU_PRON', 'L_WE_PRON', 'L_THEY_PRON', 'L_ME_PRON', 'L_YOU_OBJ_PRON', 'L_HIM_PRON', 'L_HER_OBJECT_PRON', 'L_IT_OBJECT_PRON', 'L_US_PRON', 'L_THEM_PRON', 'L_MY_PRON', 'L_YOUR_PRON', 'L_HIS_PRON', 'L_HER_PRON', 'L_ITS_PRON', 'L_OUR_PRON', 'L_THEIR_PRON', 'L_YOURS_PRON', 'L_THEIRS_PRON', 'L_HERS_PRON', 'L_OURS_PRON', 'L_MYSELF_PRON', 'L_YOURSELF_PRON', 'L_HIMSELF_PRON', 'L_HERSELF_PRON', 'L_ITSELF_PRON', 'L_OURSELVES_PRON', 'L_YOURSELVES_PRON', 'L_THEMSELVES_PRON', 'L_FIRST_PERSON_SING_PRON', 'L_SECOND_PERSON_PRON', 'L_THIRD_PERSON_SING_PRON', 'L_THIRD_PERSON_PLURAL_PRON', 'VF_INFINITIVE', 'G_PASSIVE', 'G_ACTIVE', 'G_PRESENT', 'G_PAST', 'G_FUTURE', 'G_MODALS_SIMPLE', 'G_MODALS_CONT', 'G_MODALS_PERFECT', 'AN', 'DDP', 'SVP', 'CDS', 'DDF', 'IS', 'PS', 'RE', 'ASF', 'ASM', 'OM', 'RCI', 'DMC', 'OR', 'QAS', 'PA', 'PR']\n",
      "Modelos unicos: ['human' 'gpt3' 'gpt2' 'mpt-chat' 'mistral-chat' 'mpt' 'llama-chat'\n",
      " 'mistral' 'chatgpt' 'cohere' 'cohere-chat' 'gpt4']\n",
      "Dominios unicos: ['reviews' 'abstracts' 'wiki' 'books' 'news' 'reddit' 'poetry']\n"
     ]
    }
   ],
   "source": [
    "print(\"Informaci√≥n del dataset de entrenamiento:\")\n",
    "print(f\"Forma del dataset: {train_df.shape}\")\n",
    "print(f\"Columnas: {list(train_df.columns)}\")\n",
    "print(f\"Modelos unicos: {train_df['model'].unique()}\")\n",
    "print(f\"Dominios unicos: {train_df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e3c21",
   "metadata": {},
   "source": [
    "## Preparaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0478a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci√≥n de clases:\n",
      "model\n",
      "True     6839\n",
      "False    5329\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total de oraciones: 12168\n"
     ]
    }
   ],
   "source": [
    "# 1. Crear etiqueta binaria: 1 = Humano (model == 'human'), 0 = IA (resto)\n",
    "print(\"Distribuci√≥n de clases:\")\n",
    "print((train_df['model'] == 'human').value_counts())\n",
    "print(f\"\\nTotal de oraciones: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a98d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de features: 196\n",
      "Primeros 10 features: ['POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP']\n"
     ]
    }
   ],
   "source": [
    "# 2. Definir columnas de features (excluir metadatos y target)\n",
    "# Excluir: id, sentence_num, model, domain\n",
    "metadata_cols = ['id', 'sentence_num', 'model', 'domain']\n",
    "feature_columns = [col for col in train_df.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"\\nTotal de features: {len(feature_columns)}\")\n",
    "print(f\"Primeros 10 features: {feature_columns[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9398457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de documentos √∫nicos: 1000\n"
     ]
    }
   ],
   "source": [
    "# 3. Obtener IDs √∫nicos y crear mapping de ID a clase\n",
    "unique_ids = train_df['id'].unique()\n",
    "print(f\"\\nTotal de documentos √∫nicos: {len(unique_ids)}\")\n",
    "\n",
    "# Mapping de ID a clase (binaria: humano vs IA)\n",
    "id_to_class = train_df.groupby('id')['model'].first().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac531c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci√≥n de documentos por clase:\n",
      "human           500\n",
      "gpt2             64\n",
      "mistral-chat     62\n",
      "mpt-chat         61\n",
      "mpt              58\n",
      "mistral          54\n",
      "llama-chat       50\n",
      "chatgpt          35\n",
      "gpt3             34\n",
      "cohere           31\n",
      "cohere-chat      27\n",
      "gpt4             24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clase 0 (IA): 500 documentos\n",
      "Clase 1 (Humano): 64 documentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_26216\\198586682.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"\\nClase 0 (IA): {class_distribution.get(0, 0)} documentos\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_26216\\198586682.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"Clase 1 (Humano): {class_distribution.get(1, 0)} documentos\")\n"
     ]
    }
   ],
   "source": [
    "# Verificar cu√°ntos documentos hay por clase\n",
    "print(\"Distribuci√≥n de documentos por clase:\")\n",
    "class_distribution = pd.Series(id_to_class.values()).value_counts()\n",
    "print(class_distribution)\n",
    "print(f\"\\nClase 0 (IA): {class_distribution.get(0, 0)} documentos\")\n",
    "print(f\"Clase 1 (Humano): {class_distribution.get(1, 0)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b97c5150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Usando stratified split (mantiene proporci√≥n de clases)\n",
      "\n",
      "Documentos en train: 800\n",
      "Documentos en test: 200\n"
     ]
    }
   ],
   "source": [
    "# 4. Split de IDs (no de oraciones) - IMPORTANTE para evitar data leakage\n",
    "ids_list = list(id_to_class.keys())\n",
    "labels_list = [id_to_class[id_] for id_ in ids_list]\n",
    "\n",
    "# Verificar si podemos usar stratify\n",
    "class_counts = pd.Series(labels_list).value_counts()\n",
    "can_stratify = class_counts.min() >= 2\n",
    "\n",
    "if can_stratify:\n",
    "    print(\"‚úì Usando stratified split (mantiene proporci√≥n de clases)\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels_list\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se puede usar stratify (clase m√≠nima: {class_counts.min()} documentos)\")\n",
    "    print(\"Usando split aleatorio sin stratify\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=None  # Sin estratificaci√≥n\n",
    "    )\n",
    "\n",
    "print(f\"\\nDocumentos en train: {len(train_ids)}\")\n",
    "print(f\"Documentos en test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effebb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oraciones en train: 9594\n",
      "Oraciones en test: 2574\n",
      "\n",
      "Distribuci√≥n en train:\n",
      "model\n",
      "human           5395\n",
      "gpt2             745\n",
      "llama-chat       491\n",
      "mistral          467\n",
      "mistral-chat     466\n",
      "mpt              441\n",
      "chatgpt          382\n",
      "mpt-chat         290\n",
      "cohere           275\n",
      "gpt4             251\n",
      "gpt3             209\n",
      "cohere-chat      182\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuci√≥n en test:\n",
      "model\n",
      "human           1444\n",
      "gpt2             270\n",
      "mpt              129\n",
      "chatgpt          123\n",
      "mistral-chat     117\n",
      "llama-chat       116\n",
      "mistral           96\n",
      "mpt-chat          74\n",
      "gpt4              66\n",
      "gpt3              62\n",
      "cohere            45\n",
      "cohere-chat       32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5. Filtrar oraciones seg√∫n los IDs\n",
    "train_sentences = train_df[train_df['id'].isin(train_ids)]\n",
    "test_sentences = train_df[train_df['id'].isin(test_ids)]\n",
    "\n",
    "print(f\"\\nOraciones en train: {len(train_sentences)}\")\n",
    "print(f\"Oraciones en test: {len(test_sentences)}\")\n",
    "\n",
    "# Verificar distribuci√≥n en cada conjunto\n",
    "print(f\"\\nDistribuci√≥n en train:\")\n",
    "# print(train_sentences['is_human'].value_counts())\n",
    "print(train_sentences['model'].value_counts())\n",
    "print(f\"\\nDistribuci√≥n en test:\")\n",
    "# print(test_sentences['is_human'].value_counts())\n",
    "print(test_sentences['model'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b469c06",
   "metadata": {},
   "source": [
    "## Preparaci√≥n de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8668febf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Shape de X_train: (9594, 196)\n",
      "Shape de y_train: (9594,)\n",
      "Shape de X_test: (2574, 196)\n",
      "Shape de y_test: (2574,)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. Preparar X e y\n",
    "X_train = train_sentences[feature_columns].values\n",
    "y_train = train_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "X_test = test_sentences[feature_columns].values\n",
    "y_test = test_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Shape de X_train: {X_train.shape}\")\n",
    "print(f\"Shape de y_train: {y_train.shape}\")\n",
    "print(f\"Shape de X_test: {X_test.shape}\")\n",
    "print(f\"Shape de y_test: {y_test.shape}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebaa8044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando valores NaN en el dataset...\n",
      "\n",
      "NaNs en train_df: 39\n",
      "NaNs en feature_columns:\n",
      "ST_HERDAN_TTR    39\n",
      "dtype: int64\n",
      "\n",
      "NaNs en X_train: 31\n",
      "NaNs en X_test: 8\n",
      "NaNs en y_train: 0\n",
      "NaNs en y_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay valores NaN en los datos\n",
    "print(\"Verificando valores NaN en el dataset...\")\n",
    "print(f\"\\nNaNs en train_df: {train_df.isna().sum().sum()}\")\n",
    "print(f\"NaNs en feature_columns:\")\n",
    "nan_features = train_df[feature_columns].isna().sum()\n",
    "nan_features_with_nans = nan_features[nan_features > 0]\n",
    "if len(nan_features_with_nans) > 0:\n",
    "    print(nan_features_with_nans)\n",
    "else:\n",
    "    print(\"No hay NaNs en las features ‚úì\")\n",
    "\n",
    "print(f\"\\nNaNs en X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"NaNs en X_test: {np.isnan(X_test).sum()}\")\n",
    "print(f\"NaNs en y_train: {np.isnan(y_train).sum()}\")\n",
    "print(f\"NaNs en y_test: {np.isnan(y_test).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "338c0d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Valores NaN imputados con la media de cada feature\n",
      "NaNs en X_train_clean: 0\n",
      "NaNs en X_test_clean: 0\n"
     ]
    }
   ],
   "source": [
    "# Soluci√≥n: Imputar valores NaN antes de entrenar\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Opci√≥n 1: Imputar con la media de cada feature\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Ajustar el imputer con los datos de entrenamiento\n",
    "X_train_clean = imputer.fit_transform(X_train)\n",
    "# Transformar los datos de test con el mismo imputer\n",
    "X_test_clean = imputer.transform(X_test)\n",
    "\n",
    "print(\"‚úì Valores NaN imputados con la media de cada feature\")\n",
    "print(f\"NaNs en X_train_clean: {np.isnan(X_train_clean).sum()}\")\n",
    "print(f\"NaNs en X_test_clean: {np.isnan(X_test_clean).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e8566",
   "metadata": {},
   "source": [
    "## Reducci√≥n de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d225607",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b966c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de componentes para explicar el 95% de la varianza: 24\n"
     ]
    }
   ],
   "source": [
    "pca = PCA().fit(X_train_clean)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"N√∫mero de componentes para explicar el 95% de la varianza: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9707f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train_clean)\n",
    "X_test_pca = pca.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0590d",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b66c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Aplicar LDA para reducir la dimensionalidad\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)  # La cantidad de componentes debe ser <= n√∫mero de clases - 1\n",
    "x_train_lda = lda.fit_transform(X_train_clean, y_train)\n",
    "x_test_lda = lda.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36bb11",
   "metadata": {},
   "source": [
    "### FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a709f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of factors to retain: 1\n"
     ]
    }
   ],
   "source": [
    "fa = FactorAnalysis().fit(X_train_clean)\n",
    "\n",
    "singular_values = fa.components_\n",
    "explained_variance = np.var(singular_values, axis=1) / np.var(X_train_clean, axis=0).sum()\n",
    "\n",
    "# Calcula la varianza explicada acumulativa\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Determina el n√∫mero de factores necesarios para explicar al menos el 95% de la varianza\n",
    "num_factors = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of factors to retain: {num_factors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d8c1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(n_components=1)\n",
    "x_train_fa = fa.fit_transform(X_train_clean)\n",
    "x_test_fa = fa.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ad4e2",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03fdbc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain: 25\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=min(X_train_clean.shape) - 1)\n",
    "svd.fit(X_train_clean)\n",
    "\n",
    "# Calcula la varianza explicada por los componentes\n",
    "explained_variance = svd.explained_variance_ratio_\n",
    "\n",
    "# Calcula la varianza explicada acumulativa\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Determina el n√∫mero de componentes necesarios para explicar al menos el 95% de la varianza\n",
    "num_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of components to retain: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6467c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=25)\n",
    "x_train_svd = svd.fit_transform(X_train_clean)\n",
    "x_test_svd = svd.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd95c0a",
   "metadata": {},
   "source": [
    "### JL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59773a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain using JL: 226\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.9\n",
    "\n",
    "# Calcula el n√∫mero m√≠nimo de componentes necesarios usando la Proyecci√≥n de Johnson-Lindenstrauss\n",
    "n_samples = X_train_clean.shape[0]\n",
    "n_components = johnson_lindenstrauss_min_dim(n_samples, eps=epsilon)\n",
    "print(f\"Number of components to retain using JL: {n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ac6f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\random_projection.py:411: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (196 < 224).The dimensionality of the problem will not be reduced.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jl = SparseRandomProjection(n_components=224)\n",
    "x_train_jl = jl.fit_transform(X_train_clean)\n",
    "x_test_jl = jl.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88a0ac",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b32e34",
   "metadata": {},
   "source": [
    "### Funci√≥n de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fb810cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(y_test, y_pred):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RESULTADOS EN TEST SET (Split por Documento)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['IA', 'Humano'], digits=4))\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(f\"\\nInterpretaci√≥n:\")\n",
    "    print(f\"  TN (IA correctamente clasificada): {cm[0,0]}\")\n",
    "    print(f\"  FP (IA clasificada como Humano): {cm[0,1]}\")\n",
    "    print(f\"  FN (Humano clasificado como IA): {cm[1,0]}\")\n",
    "    print(f\"  TP (Humano correctamente clasificado): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6783a0",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4709d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚úì Modelo entrenado exitosamente\n"
     ]
    }
   ],
   "source": [
    "# trainX, testX = X_train_pca, X_test_pca\n",
    "# trainX, testX = x_train_lda, x_test_lda\n",
    "# trainX, testX = x_train_fa, x_test_fa\n",
    "# trainX, testX = x_train_svd, x_test_svd\n",
    "trainX, testX = x_train_jl, x_test_jl\n",
    "\n",
    "C = 10\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=C, gamma='scale', random_state=42)\n",
    "svm_model.fit(trainX, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(testX)\n",
    "\n",
    "print(\"‚úì Modelo entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0a3c4",
   "metadata": {},
   "source": [
    "### Mostrar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c3180b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTADOS EN TEST SET (Split por Documento)\n",
      "============================================================\n",
      "\n",
      "Accuracy: 0.6445\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          IA     0.6409    0.4327    0.5166      1130\n",
      "      Humano     0.6461    0.8102    0.7189      1444\n",
      "\n",
      "    accuracy                         0.6445      2574\n",
      "   macro avg     0.6435    0.6215    0.6178      2574\n",
      "weighted avg     0.6438    0.6445    0.6301      2574\n",
      "\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[ 489  641]\n",
      " [ 274 1170]]\n",
      "\n",
      "Interpretaci√≥n:\n",
      "  TN (IA correctamente clasificada): 489\n",
      "  FP (IA clasificada como Humano): 641\n",
      "  FN (Humano clasificado como IA): 274\n",
      "  TP (Humano correctamente clasificado): 1170\n"
     ]
    }
   ],
   "source": [
    "showResults(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
