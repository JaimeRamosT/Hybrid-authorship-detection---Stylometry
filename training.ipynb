{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee47bd19",
   "metadata": {},
   "source": [
    "# Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aabf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2877e60f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dfPath = \u001b[33m\"\u001b[39m\u001b[33mdataframe/train_df.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_df = \u001b[43mpd\u001b[49m.read_csv(dfPath)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "dfPath = \"dataframe/train_df.csv\"\n",
    "train_df = pd.read_csv(dfPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>POS_ADV</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>POS_INTJ</th>\n",
       "      <th>...</th>\n",
       "      <th>RE</th>\n",
       "      <th>ASF</th>\n",
       "      <th>ASM</th>\n",
       "      <th>OM</th>\n",
       "      <th>RCI</th>\n",
       "      <th>DMC</th>\n",
       "      <th>OR</th>\n",
       "      <th>QAS</th>\n",
       "      <th>PA</th>\n",
       "      <th>PR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10031</th>\n",
       "      <td>999</td>\n",
       "      <td>11</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10032</th>\n",
       "      <td>999</td>\n",
       "      <td>12</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10033</th>\n",
       "      <td>999</td>\n",
       "      <td>13</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10034</th>\n",
       "      <td>999</td>\n",
       "      <td>14</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10035</th>\n",
       "      <td>999</td>\n",
       "      <td>15</td>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10036 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentence_num    model domain  POS_VERB  POS_NOUN   POS_ADJ  \\\n",
       "0        0             0  mistral  books  0.121212  0.212121  0.060606   \n",
       "1        0             1  mistral  books  0.240000  0.160000  0.120000   \n",
       "2        0             2  mistral  books  0.222222  0.111111  0.111111   \n",
       "3        0             3  mistral  books  0.285714  0.190476  0.047619   \n",
       "4        0             4  mistral  books  0.250000  0.062500  0.062500   \n",
       "...    ...           ...      ...    ...       ...       ...       ...   \n",
       "10031  999            11  mistral  books  0.214286  0.142857  0.071429   \n",
       "10032  999            12  mistral  books  0.166667  0.166667  0.166667   \n",
       "10033  999            13  mistral  books  0.166667  0.166667  0.047619   \n",
       "10034  999            14  mistral  books  0.250000  0.187500  0.062500   \n",
       "10035  999            15  mistral  books  0.157895  0.157895  0.052632   \n",
       "\n",
       "        POS_ADV   POS_DET  POS_INTJ  ...   RE  ASF       ASM    OM  RCI  DMC  \\\n",
       "0      0.090909  0.151515       0.0  ...  0.0  0.0  0.030303  0.00  0.0  0.0   \n",
       "1      0.040000  0.200000       0.0  ...  0.0  0.0  0.000000  0.04  0.0  0.0   \n",
       "2      0.000000  0.222222       0.0  ...  0.0  0.0  0.000000  0.00  0.0  0.0   \n",
       "3      0.000000  0.142857       0.0  ...  0.0  0.0  0.000000  0.00  0.0  0.0   \n",
       "4      0.000000  0.187500       0.0  ...  0.0  0.0  0.062500  0.00  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...       ...   ...  ...  ...   \n",
       "10031  0.000000  0.142857       0.0  ...  0.0  0.0  0.000000  0.00  0.0  0.0   \n",
       "10032  0.000000  0.111111       0.0  ...  0.0  0.0  0.000000  0.00  0.0  0.0   \n",
       "10033  0.047619  0.166667       0.0  ...  0.0  0.0  0.023810  0.00  0.0  0.0   \n",
       "10034  0.062500  0.187500       0.0  ...  0.0  0.0  0.000000  0.00  0.0  0.0   \n",
       "10035  0.052632  0.157895       0.0  ...  0.0  0.0  0.052632  0.00  0.0  0.0   \n",
       "\n",
       "        OR       QAS   PA        PR  \n",
       "0      0.0  0.121212  0.0  0.000000  \n",
       "1      0.0  0.160000  0.0  0.040000  \n",
       "2      0.0  0.111111  0.0  0.111111  \n",
       "3      0.0  0.095238  0.0  0.000000  \n",
       "4      0.0  0.125000  0.0  0.000000  \n",
       "...    ...       ...  ...       ...  \n",
       "10031  0.0  0.071429  0.0  0.000000  \n",
       "10032  0.0  0.055556  0.0  0.000000  \n",
       "10033  0.0  0.119048  0.0  0.000000  \n",
       "10034  0.0  0.187500  0.0  0.000000  \n",
       "10035  0.0  0.105263  0.0  0.000000  \n",
       "\n",
       "[10036 rows x 200 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8ad33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del dataset de entrenamiento:\n",
      "Forma del dataset: (10036, 200)\n",
      "Columnas: ['id', 'sentence_num', 'model', 'domain', 'POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP', 'POS_PRO', 'L_REF', 'L_HASHTAG', 'L_MENTION', 'L_RT', 'L_LINKS', 'L_CONT_A', 'L_FUNC_A', 'L_CONT_T', 'L_FUNC_T', 'L_PLURAL_NOUNS', 'L_SINGULAR_NOUNS', 'L_PROPER_NAME', 'L_PERSONAL_NAME', 'L_NOUN_PHRASES', 'L_PUNCT', 'L_PUNCT_DOT', 'L_PUNCT_COM', 'L_PUNCT_SEMC', 'L_PUNCT_COL', 'L_PUNCT_DASH', 'L_POSSESSIVES', 'L_ADJ_POSITIVE', 'L_ADJ_COMPARATIVE', 'L_ADJ_SUPERLATIVE', 'L_ADV_POSITIVE', 'L_ADV_COMPARATIVE', 'L_ADV_SUPERLATIVE', 'PS_CONTRADICTION', 'PS_AGREEMENT', 'PS_EXAMPLES', 'PS_CONSEQUENCE', 'PS_CAUSE', 'PS_LOCATION', 'PS_TIME', 'PS_CONDITION', 'PS_MANNER', 'SY_QUESTION', 'SY_NARRATIVE', 'SY_NEGATIVE_QUESTIONS', 'SY_SPECIAL_QUESTIONS', 'SY_TAG_QUESTIONS', 'SY_GENERAL_QUESTIONS', 'SY_EXCLAMATION', 'SY_IMPERATIVE', 'SY_SUBORD_SENT', 'SY_SUBORD_SENT_PUNCT', 'SY_COORD_SENT', 'SY_COORD_SENT_PUNCT', 'SY_SIMPLE_SENT', 'SY_INVERSE_PATTERNS', 'SY_SIMILE', 'SY_FRONTING', 'SY_IRRITATION', 'SY_INTENSIFIER', 'SY_QUOT', 'VT_PRESENT_SIMPLE', 'VT_PRESENT_PROGRESSIVE', 'VT_PRESENT_PERFECT', 'VT_PRESENT_PERFECT_PROGR', 'VT_PRESENT_SIMPLE_PASSIVE', 'VT_PRESENT_PROGR_PASSIVE', 'VT_PRESENT_PERFECT_PASSIVE', 'VT_PAST_SIMPLE', 'VT_PAST_SIMPLE_BE', 'VT_PAST_PROGR', 'VT_PAST_PERFECT', 'VT_PAST_PERFECT_PROGR', 'VT_PAST_SIMPLE_PASSIVE', 'VT_PAST_POGR_PASSIVE', 'VT_PAST_PERFECT_PASSIVE', 'VT_FUTURE_SIMPLE', 'VT_FUTURE_PROGRESSIVE', 'VT_FUTURE_PERFECT', 'VT_FUTURE_PERFECT_PROGR', 'VT_FUTURE_SIMPLE_PASSIVE', 'VT_FUTURE_PROGR_PASSIVE', 'VT_FUTURE_PERFECT_PASSIVE', 'VT_WOULD', 'VT_WOULD_PASSIVE', 'VT_WOULD_PROGRESSIVE', 'VT_WOULD_PERFECT', 'VT_WOULD_PERFECT_PASSIVE', 'VT_SHOULD', 'VT_SHOULD_PASSIVE', 'VT_SHALL', 'VT_SHALL_PASSIVE', 'VT_SHOULD_PROGRESSIVE', 'VT_SHOULD_PERFECT', 'VT_SHOULD_PERFECT_PASSIVE', 'VT_MUST', 'VT_MUST_PASSIVE', 'VT_MUST_PROGRESSIVE', 'VT_MUST_PERFECT', 'VT_MST_PERFECT_PASSIVE', 'VT_CAN', 'VT_CAN_PASSIVE', 'VT_COULD', 'VT_COULD_PASSIVE', 'VT_CAN_PROGRESSIVE', 'VT_COULD_PROGRESSIVE', 'VT_COULD_PERFECT', 'VT_COULD_PERFECT_PASSIVE', 'VT_MAY', 'VT_MAY_PASSIVE', 'VT_MIGHT', 'VT_MIGHT_PASSIVE', 'VT_MAY_PROGRESSIVE', 'VT_MIGTH_PERFECT', 'VT_MIGHT_PERFECT_PASSIVE', 'VT_MAY_PERFECT_PASSIVE', 'ST_TYPE_TOKEN_RATIO_LEMMAS', 'ST_HERDAN_TTR', 'ST_MASS_TTR', 'ST_SENT_WRDSPERSENT', 'ST_SENT_DIFFERENCE', 'ST_REPETITIONS_WORDS', 'ST_REPETITIONS_SENT', 'ST_SENT_D_VP', 'ST_SENT_D_NP', 'ST_SENT_D_PP', 'ST_SENT_D_ADJP', 'ST_SENT_D_ADVP', 'L_I_PRON', 'L_HE_PRON', 'L_SHE_PRON', 'L_IT_PRON', 'L_YOU_PRON', 'L_WE_PRON', 'L_THEY_PRON', 'L_ME_PRON', 'L_YOU_OBJ_PRON', 'L_HIM_PRON', 'L_HER_OBJECT_PRON', 'L_IT_OBJECT_PRON', 'L_US_PRON', 'L_THEM_PRON', 'L_MY_PRON', 'L_YOUR_PRON', 'L_HIS_PRON', 'L_HER_PRON', 'L_ITS_PRON', 'L_OUR_PRON', 'L_THEIR_PRON', 'L_YOURS_PRON', 'L_THEIRS_PRON', 'L_HERS_PRON', 'L_OURS_PRON', 'L_MYSELF_PRON', 'L_YOURSELF_PRON', 'L_HIMSELF_PRON', 'L_HERSELF_PRON', 'L_ITSELF_PRON', 'L_OURSELVES_PRON', 'L_YOURSELVES_PRON', 'L_THEMSELVES_PRON', 'L_FIRST_PERSON_SING_PRON', 'L_SECOND_PERSON_PRON', 'L_THIRD_PERSON_SING_PRON', 'L_THIRD_PERSON_PLURAL_PRON', 'VF_INFINITIVE', 'G_PASSIVE', 'G_ACTIVE', 'G_PRESENT', 'G_PAST', 'G_FUTURE', 'G_MODALS_SIMPLE', 'G_MODALS_CONT', 'G_MODALS_PERFECT', 'AN', 'DDP', 'SVP', 'CDS', 'DDF', 'IS', 'PS', 'RE', 'ASF', 'ASM', 'OM', 'RCI', 'DMC', 'OR', 'QAS', 'PA', 'PR']\n",
      "Modelos unicos: ['mistral' 'mpt-chat' 'gpt2' 'cohere' 'mistral-chat' 'llama-chat' 'mpt'\n",
      " 'cohere-chat' 'chatgpt' 'human' 'gpt3' 'gpt4']\n",
      "Dominios unicos: ['books' 'poetry' 'reddit' 'abstracts' 'news' 'wiki' 'reviews']\n"
     ]
    }
   ],
   "source": [
    "print(\"Información del dataset de entrenamiento:\")\n",
    "print(f\"Forma del dataset: {train_df.shape}\")\n",
    "print(f\"Columnas: {list(train_df.columns)}\")\n",
    "print(f\"Modelos unicos: {train_df['model'].unique()}\")\n",
    "print(f\"Dominios unicos: {train_df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e3c21",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0478a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "model\n",
      "False    9769\n",
      "True      267\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total de oraciones: 10036\n"
     ]
    }
   ],
   "source": [
    "# 1. Crear etiqueta binaria: 1 = Humano (model == 'human'), 0 = IA (resto)\n",
    "print(\"Distribución de clases:\")\n",
    "print((train_df['model'] == 'human').value_counts())\n",
    "print(f\"\\nTotal de oraciones: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a98d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de features: 196\n",
      "Primeros 10 features: ['POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP']\n"
     ]
    }
   ],
   "source": [
    "# 2. Definir columnas de features (excluir metadatos y target)\n",
    "# Excluir: id, sentence_num, model, domain\n",
    "metadata_cols = ['id', 'sentence_num', 'model', 'domain']\n",
    "feature_columns = [col for col in train_df.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"\\nTotal de features: {len(feature_columns)}\")\n",
    "print(f\"Primeros 10 features: {feature_columns[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9398457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de documentos únicos: 1000\n"
     ]
    }
   ],
   "source": [
    "# 3. Obtener IDs únicos y crear mapping de ID a clase\n",
    "unique_ids = train_df['id'].unique()\n",
    "print(f\"\\nTotal de documentos únicos: {len(unique_ids)}\")\n",
    "\n",
    "# Mapping de ID a clase (binaria: humano vs IA)\n",
    "id_to_class = train_df.groupby('id')['model'].first().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac531c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de documentos por clase:\n",
      "mistral-chat    127\n",
      "mpt             120\n",
      "mistral         118\n",
      "gpt2            116\n",
      "mpt-chat        106\n",
      "llama-chat      105\n",
      "gpt4             64\n",
      "gpt3             59\n",
      "cohere-chat      55\n",
      "cohere           52\n",
      "chatgpt          52\n",
      "human            26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clase 0 (IA): 127 documentos\n",
      "Clase 1 (Humano): 120 documentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9304\\198586682.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"\\nClase 0 (IA): {class_distribution.get(0, 0)} documentos\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9304\\198586682.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"Clase 1 (Humano): {class_distribution.get(1, 0)} documentos\")\n"
     ]
    }
   ],
   "source": [
    "# Verificar cuántos documentos hay por clase\n",
    "print(\"Distribución de documentos por clase:\")\n",
    "class_distribution = pd.Series(id_to_class.values()).value_counts()\n",
    "print(class_distribution)\n",
    "print(f\"\\nClase 0 (IA): {class_distribution.get(0, 0)} documentos\")\n",
    "print(f\"Clase 1 (Humano): {class_distribution.get(1, 0)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97c5150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Usando stratified split (mantiene proporción de clases)\n",
      "\n",
      "Documentos en train: 800\n",
      "Documentos en test: 200\n"
     ]
    }
   ],
   "source": [
    "# 4. Split de IDs (no de oraciones) - IMPORTANTE para evitar data leakage\n",
    "ids_list = list(id_to_class.keys())\n",
    "labels_list = [id_to_class[id_] for id_ in ids_list]\n",
    "\n",
    "# Verificar si podemos usar stratify\n",
    "class_counts = pd.Series(labels_list).value_counts()\n",
    "can_stratify = class_counts.min() >= 2\n",
    "\n",
    "if can_stratify:\n",
    "    print(\"✓ Usando stratified split (mantiene proporción de clases)\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels_list\n",
    "    )\n",
    "else:\n",
    "    print(f\"⚠️ No se puede usar stratify (clase mínima: {class_counts.min()} documentos)\")\n",
    "    print(\"Usando split aleatorio sin stratify\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=None  # Sin estratificación\n",
    "    )\n",
    "\n",
    "print(f\"\\nDocumentos en train: {len(train_ids)}\")\n",
    "print(f\"Documentos en test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effebb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oraciones en train: 7968\n",
      "Oraciones en test: 2068\n",
      "\n",
      "Distribución en train:\n",
      "model\n",
      "gpt2            1279\n",
      "llama-chat      1004\n",
      "mistral          916\n",
      "mistral-chat     890\n",
      "mpt              867\n",
      "gpt4             638\n",
      "chatgpt          548\n",
      "mpt-chat         498\n",
      "cohere           466\n",
      "cohere-chat      332\n",
      "gpt3             317\n",
      "human            213\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución en test:\n",
      "model\n",
      "mistral         317\n",
      "mpt             302\n",
      "llama-chat      286\n",
      "gpt2            257\n",
      "mistral-chat    227\n",
      "mpt-chat        127\n",
      "gpt4            126\n",
      "chatgpt         123\n",
      "cohere-chat      96\n",
      "gpt3             78\n",
      "cohere           75\n",
      "human            54\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5. Filtrar oraciones según los IDs\n",
    "train_sentences = train_df[train_df['id'].isin(train_ids)]\n",
    "test_sentences = train_df[train_df['id'].isin(test_ids)]\n",
    "\n",
    "print(f\"\\nOraciones en train: {len(train_sentences)}\")\n",
    "print(f\"Oraciones en test: {len(test_sentences)}\")\n",
    "\n",
    "# Verificar distribución en cada conjunto\n",
    "print(f\"\\nDistribución en train:\")\n",
    "# print(train_sentences['is_human'].value_counts())\n",
    "print(train_sentences['model'].value_counts())\n",
    "print(f\"\\nDistribución en test:\")\n",
    "# print(test_sentences['is_human'].value_counts())\n",
    "print(test_sentences['model'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b469c06",
   "metadata": {},
   "source": [
    "## Preparación de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8668febf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Shape de X_train: (7968, 196)\n",
      "Shape de y_train: (7968,)\n",
      "Shape de X_test: (2068, 196)\n",
      "Shape de y_test: (2068,)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. Preparar X e y\n",
    "X_train = train_sentences[feature_columns].values\n",
    "y_train = train_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "X_test = test_sentences[feature_columns].values\n",
    "y_test = test_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Shape de X_train: {X_train.shape}\")\n",
    "print(f\"Shape de y_train: {y_train.shape}\")\n",
    "print(f\"Shape de X_test: {X_test.shape}\")\n",
    "print(f\"Shape de y_test: {y_test.shape}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebaa8044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando valores NaN en el dataset...\n",
      "\n",
      "NaNs en train_df: 34\n",
      "NaNs en feature_columns:\n",
      "ST_HERDAN_TTR    34\n",
      "dtype: int64\n",
      "\n",
      "NaNs en X_train: 24\n",
      "NaNs en X_test: 10\n",
      "NaNs en y_train: 0\n",
      "NaNs en y_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay valores NaN en los datos\n",
    "print(\"Verificando valores NaN en el dataset...\")\n",
    "print(f\"\\nNaNs en train_df: {train_df.isna().sum().sum()}\")\n",
    "print(f\"NaNs en feature_columns:\")\n",
    "nan_features = train_df[feature_columns].isna().sum()\n",
    "nan_features_with_nans = nan_features[nan_features > 0]\n",
    "if len(nan_features_with_nans) > 0:\n",
    "    print(nan_features_with_nans)\n",
    "else:\n",
    "    print(\"No hay NaNs en las features ✓\")\n",
    "\n",
    "print(f\"\\nNaNs en X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"NaNs en X_test: {np.isnan(X_test).sum()}\")\n",
    "print(f\"NaNs en y_train: {np.isnan(y_train).sum()}\")\n",
    "print(f\"NaNs en y_test: {np.isnan(y_test).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "338c0d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Valores NaN imputados con la media de cada feature\n",
      "NaNs en X_train_clean: 0\n",
      "NaNs en X_test_clean: 0\n"
     ]
    }
   ],
   "source": [
    "# Solución: Imputar valores NaN antes de entrenar\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Opción 1: Imputar con la media de cada feature\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Ajustar el imputer con los datos de entrenamiento\n",
    "X_train_clean = imputer.fit_transform(X_train)\n",
    "# Transformar los datos de test con el mismo imputer\n",
    "X_test_clean = imputer.transform(X_test)\n",
    "\n",
    "print(\"✓ Valores NaN imputados con la media de cada feature\")\n",
    "print(f\"NaNs en X_train_clean: {np.isnan(X_train_clean).sum()}\")\n",
    "print(f\"NaNs en X_test_clean: {np.isnan(X_test_clean).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88a0ac",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4709d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando SVM con split a nivel de documento...\n",
      "Kernel: RBF | C: 1.0 | Gamma: scale\n",
      "============================================================\n",
      "✓ Modelo entrenado exitosamente\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntrenando SVM con split a nivel de documento...\")\n",
    "print(\"Kernel: RBF | C: 1.0 | Gamma: scale\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train_clean, y_train)\n",
    "\n",
    "print(\"✓ Modelo entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c286de7",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c3180b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTADOS EN TEST SET (Split por Documento)\n",
      "============================================================\n",
      "\n",
      "Accuracy: 0.9739\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          IA     0.9739    1.0000    0.9868      2014\n",
      "      Humano     0.0000    0.0000    0.0000        54\n",
      "\n",
      "    accuracy                         0.9739      2068\n",
      "   macro avg     0.4869    0.5000    0.4934      2068\n",
      "weighted avg     0.9485    0.9739    0.9610      2068\n",
      "\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[2014    0]\n",
      " [  54    0]]\n",
      "\n",
      "Interpretación:\n",
      "  TN (IA correctamente clasificada): 2014\n",
      "  FP (IA clasificada como Humano): 0\n",
      "  FN (Humano clasificado como IA): 54\n",
      "  TP (Humano correctamente clasificado): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model.predict(X_test_clean)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTADOS EN TEST SET (Split por Documento)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['IA', 'Humano'], digits=4))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nInterpretación:\")\n",
    "print(f\"  TN (IA correctamente clasificada): {cm[0,0]}\")\n",
    "print(f\"  FP (IA clasificada como Humano): {cm[0,1]}\")\n",
    "print(f\"  FN (Humano clasificado como IA): {cm[1,0]}\")\n",
    "print(f\"  TP (Humano correctamente clasificado): {cm[1,1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
