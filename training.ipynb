{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee47bd19",
   "metadata": {},
   "source": [
    "# Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7aabf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2877e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPath = \"dataframe/train_df.csv\"\n",
    "train_df = pd.read_csv(dfPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>POS_ADV</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>POS_INTJ</th>\n",
       "      <th>...</th>\n",
       "      <th>RE</th>\n",
       "      <th>ASF</th>\n",
       "      <th>ASM</th>\n",
       "      <th>OM</th>\n",
       "      <th>RCI</th>\n",
       "      <th>DMC</th>\n",
       "      <th>OR</th>\n",
       "      <th>QAS</th>\n",
       "      <th>PA</th>\n",
       "      <th>PR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>books</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>books</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>books</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>books</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>books</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>mistral</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>98</td>\n",
       "      <td>5</td>\n",
       "      <td>mistral</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>mpt</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>mpt</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>mpt</td>\n",
       "      <td>reviews</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1066 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  sentence_num     model   domain  POS_VERB  POS_NOUN   POS_ADJ  \\\n",
       "0      0             0  mpt-chat    books  0.157895  0.263158  0.157895   \n",
       "1      0             1  mpt-chat    books  0.238095  0.142857  0.000000   \n",
       "2      0             2  mpt-chat    books  0.187500  0.218750  0.093750   \n",
       "3      0             3  mpt-chat    books  0.250000  0.208333  0.083333   \n",
       "4      0             4  mpt-chat    books  0.095238  0.380952  0.000000   \n",
       "...   ..           ...       ...      ...       ...       ...       ...   \n",
       "1061  98             4   mistral  reviews  0.200000  0.200000  0.200000   \n",
       "1062  98             5   mistral  reviews  0.294118  0.000000  0.000000   \n",
       "1063  99             0       mpt  reviews  0.254545  0.127273  0.109091   \n",
       "1064  99             1       mpt  reviews  0.162791  0.116279  0.093023   \n",
       "1065  99             2       mpt  reviews  0.103448  0.172414  0.137931   \n",
       "\n",
       "       POS_ADV   POS_DET  POS_INTJ  ...        RE  ASF       ASM   OM  RCI  \\\n",
       "0     0.052632  0.105263       0.0  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
       "1     0.142857  0.095238       0.0  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
       "2     0.000000  0.093750       0.0  ...  0.000000  0.0  0.031250  0.0  0.0   \n",
       "3     0.000000  0.083333       0.0  ...  0.041667  0.0  0.000000  0.0  0.0   \n",
       "4     0.000000  0.190476       0.0  ...  0.000000  0.0  0.047619  0.0  0.0   \n",
       "...        ...       ...       ...  ...       ...  ...       ...  ...  ...   \n",
       "1061  0.000000  0.200000       0.0  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
       "1062  0.176471  0.000000       0.0  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
       "1063  0.090909  0.054545       0.0  ...  0.000000  0.0  0.018182  0.0  0.0   \n",
       "1064  0.255814  0.058140       0.0  ...  0.000000  0.0  0.011628  0.0  0.0   \n",
       "1065  0.344828  0.000000       0.0  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
       "\n",
       "          DMC   OR       QAS   PA   PR  \n",
       "0     0.00000  0.0  0.000000  0.0  0.0  \n",
       "1     0.00000  0.0  0.047619  0.0  0.0  \n",
       "2     0.03125  0.0  0.031250  0.0  0.0  \n",
       "3     0.00000  0.0  0.083333  0.0  0.0  \n",
       "4     0.00000  0.0  0.238095  0.0  0.0  \n",
       "...       ...  ...       ...  ...  ...  \n",
       "1061  0.00000  0.0  0.200000  0.0  0.0  \n",
       "1062  0.00000  0.0  0.058824  0.0  0.0  \n",
       "1063  0.00000  0.0  0.000000  0.0  0.0  \n",
       "1064  0.00000  0.0  0.023256  0.0  0.0  \n",
       "1065  0.00000  0.0  0.000000  0.0  0.0  \n",
       "\n",
       "[1066 rows x 200 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a8ad33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del dataset de entrenamiento:\n",
      "Forma del dataset: (1066, 200)\n",
      "Columnas: ['id', 'sentence_num', 'model', 'domain', 'POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP', 'POS_PRO', 'L_REF', 'L_HASHTAG', 'L_MENTION', 'L_RT', 'L_LINKS', 'L_CONT_A', 'L_FUNC_A', 'L_CONT_T', 'L_FUNC_T', 'L_PLURAL_NOUNS', 'L_SINGULAR_NOUNS', 'L_PROPER_NAME', 'L_PERSONAL_NAME', 'L_NOUN_PHRASES', 'L_PUNCT', 'L_PUNCT_DOT', 'L_PUNCT_COM', 'L_PUNCT_SEMC', 'L_PUNCT_COL', 'L_PUNCT_DASH', 'L_POSSESSIVES', 'L_ADJ_POSITIVE', 'L_ADJ_COMPARATIVE', 'L_ADJ_SUPERLATIVE', 'L_ADV_POSITIVE', 'L_ADV_COMPARATIVE', 'L_ADV_SUPERLATIVE', 'PS_CONTRADICTION', 'PS_AGREEMENT', 'PS_EXAMPLES', 'PS_CONSEQUENCE', 'PS_CAUSE', 'PS_LOCATION', 'PS_TIME', 'PS_CONDITION', 'PS_MANNER', 'SY_QUESTION', 'SY_NARRATIVE', 'SY_NEGATIVE_QUESTIONS', 'SY_SPECIAL_QUESTIONS', 'SY_TAG_QUESTIONS', 'SY_GENERAL_QUESTIONS', 'SY_EXCLAMATION', 'SY_IMPERATIVE', 'SY_SUBORD_SENT', 'SY_SUBORD_SENT_PUNCT', 'SY_COORD_SENT', 'SY_COORD_SENT_PUNCT', 'SY_SIMPLE_SENT', 'SY_INVERSE_PATTERNS', 'SY_SIMILE', 'SY_FRONTING', 'SY_IRRITATION', 'SY_INTENSIFIER', 'SY_QUOT', 'VT_PRESENT_SIMPLE', 'VT_PRESENT_PROGRESSIVE', 'VT_PRESENT_PERFECT', 'VT_PRESENT_PERFECT_PROGR', 'VT_PRESENT_SIMPLE_PASSIVE', 'VT_PRESENT_PROGR_PASSIVE', 'VT_PRESENT_PERFECT_PASSIVE', 'VT_PAST_SIMPLE', 'VT_PAST_SIMPLE_BE', 'VT_PAST_PROGR', 'VT_PAST_PERFECT', 'VT_PAST_PERFECT_PROGR', 'VT_PAST_SIMPLE_PASSIVE', 'VT_PAST_POGR_PASSIVE', 'VT_PAST_PERFECT_PASSIVE', 'VT_FUTURE_SIMPLE', 'VT_FUTURE_PROGRESSIVE', 'VT_FUTURE_PERFECT', 'VT_FUTURE_PERFECT_PROGR', 'VT_FUTURE_SIMPLE_PASSIVE', 'VT_FUTURE_PROGR_PASSIVE', 'VT_FUTURE_PERFECT_PASSIVE', 'VT_WOULD', 'VT_WOULD_PASSIVE', 'VT_WOULD_PROGRESSIVE', 'VT_WOULD_PERFECT', 'VT_WOULD_PERFECT_PASSIVE', 'VT_SHOULD', 'VT_SHOULD_PASSIVE', 'VT_SHALL', 'VT_SHALL_PASSIVE', 'VT_SHOULD_PROGRESSIVE', 'VT_SHOULD_PERFECT', 'VT_SHOULD_PERFECT_PASSIVE', 'VT_MUST', 'VT_MUST_PASSIVE', 'VT_MUST_PROGRESSIVE', 'VT_MUST_PERFECT', 'VT_MST_PERFECT_PASSIVE', 'VT_CAN', 'VT_CAN_PASSIVE', 'VT_COULD', 'VT_COULD_PASSIVE', 'VT_CAN_PROGRESSIVE', 'VT_COULD_PROGRESSIVE', 'VT_COULD_PERFECT', 'VT_COULD_PERFECT_PASSIVE', 'VT_MAY', 'VT_MAY_PASSIVE', 'VT_MIGHT', 'VT_MIGHT_PASSIVE', 'VT_MAY_PROGRESSIVE', 'VT_MIGTH_PERFECT', 'VT_MIGHT_PERFECT_PASSIVE', 'VT_MAY_PERFECT_PASSIVE', 'ST_TYPE_TOKEN_RATIO_LEMMAS', 'ST_HERDAN_TTR', 'ST_MASS_TTR', 'ST_SENT_WRDSPERSENT', 'ST_SENT_DIFFERENCE', 'ST_REPETITIONS_WORDS', 'ST_REPETITIONS_SENT', 'ST_SENT_D_VP', 'ST_SENT_D_NP', 'ST_SENT_D_PP', 'ST_SENT_D_ADJP', 'ST_SENT_D_ADVP', 'L_I_PRON', 'L_HE_PRON', 'L_SHE_PRON', 'L_IT_PRON', 'L_YOU_PRON', 'L_WE_PRON', 'L_THEY_PRON', 'L_ME_PRON', 'L_YOU_OBJ_PRON', 'L_HIM_PRON', 'L_HER_OBJECT_PRON', 'L_IT_OBJECT_PRON', 'L_US_PRON', 'L_THEM_PRON', 'L_MY_PRON', 'L_YOUR_PRON', 'L_HIS_PRON', 'L_HER_PRON', 'L_ITS_PRON', 'L_OUR_PRON', 'L_THEIR_PRON', 'L_YOURS_PRON', 'L_THEIRS_PRON', 'L_HERS_PRON', 'L_OURS_PRON', 'L_MYSELF_PRON', 'L_YOURSELF_PRON', 'L_HIMSELF_PRON', 'L_HERSELF_PRON', 'L_ITSELF_PRON', 'L_OURSELVES_PRON', 'L_YOURSELVES_PRON', 'L_THEMSELVES_PRON', 'L_FIRST_PERSON_SING_PRON', 'L_SECOND_PERSON_PRON', 'L_THIRD_PERSON_SING_PRON', 'L_THIRD_PERSON_PLURAL_PRON', 'VF_INFINITIVE', 'G_PASSIVE', 'G_ACTIVE', 'G_PRESENT', 'G_PAST', 'G_FUTURE', 'G_MODALS_SIMPLE', 'G_MODALS_CONT', 'G_MODALS_PERFECT', 'AN', 'DDP', 'SVP', 'CDS', 'DDF', 'IS', 'PS', 'RE', 'ASF', 'ASM', 'OM', 'RCI', 'DMC', 'OR', 'QAS', 'PA', 'PR']\n",
      "Modelos unicos: ['mpt-chat' 'llama-chat' 'mistral' 'gpt2' 'mistral-chat' 'cohere-chat'\n",
      " 'gpt3' 'mpt' 'gpt4' 'cohere' 'human' 'chatgpt']\n",
      "Dominios unicos: ['books' 'poetry' 'reddit' 'wiki' 'abstracts' 'news' 'reviews']\n"
     ]
    }
   ],
   "source": [
    "print(\"Información del dataset de entrenamiento:\")\n",
    "print(f\"Forma del dataset: {train_df.shape}\")\n",
    "print(f\"Columnas: {list(train_df.columns)}\")\n",
    "print(f\"Modelos unicos: {train_df['model'].unique()}\")\n",
    "print(f\"Dominios unicos: {train_df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e3c21",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0478a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "model\n",
      "False    1043\n",
      "True       23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total de oraciones: 1066\n"
     ]
    }
   ],
   "source": [
    "# 1. Crear etiqueta binaria: 1 = Humano (model == 'human'), 0 = IA (resto)\n",
    "print(\"Distribución de clases:\")\n",
    "print((train_df['model'] == 'human').value_counts())\n",
    "print(f\"\\nTotal de oraciones: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2a98d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de features: 196\n",
      "Primeros 10 features: ['POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP']\n"
     ]
    }
   ],
   "source": [
    "# 2. Definir columnas de features (excluir metadatos y target)\n",
    "# Excluir: id, sentence_num, model, domain\n",
    "metadata_cols = ['id', 'sentence_num', 'model', 'domain']\n",
    "feature_columns = [col for col in train_df.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"\\nTotal de features: {len(feature_columns)}\")\n",
    "print(f\"Primeros 10 features: {feature_columns[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9398457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de documentos únicos: 100\n"
     ]
    }
   ],
   "source": [
    "# 3. Obtener IDs únicos y crear mapping de ID a clase\n",
    "unique_ids = train_df['id'].unique()\n",
    "print(f\"\\nTotal de documentos únicos: {len(unique_ids)}\")\n",
    "\n",
    "# Mapping de ID a clase (binaria: humano vs IA)\n",
    "id_to_class = train_df.groupby('id')['model'].first().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ac531c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de documentos por clase:\n",
      "mistral-chat    16\n",
      "mistral         15\n",
      "llama-chat      14\n",
      "mpt             12\n",
      "gpt2            10\n",
      "mpt-chat         7\n",
      "gpt4             7\n",
      "cohere-chat      6\n",
      "gpt3             4\n",
      "cohere           3\n",
      "human            3\n",
      "chatgpt          3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clase 0 (IA): 16 documentos\n",
      "Clase 1 (Humano): 15 documentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2924\\198586682.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"\\nClase 0 (IA): {class_distribution.get(0, 0)} documentos\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2924\\198586682.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"Clase 1 (Humano): {class_distribution.get(1, 0)} documentos\")\n"
     ]
    }
   ],
   "source": [
    "# Verificar cuántos documentos hay por clase\n",
    "print(\"Distribución de documentos por clase:\")\n",
    "class_distribution = pd.Series(id_to_class.values()).value_counts()\n",
    "print(class_distribution)\n",
    "print(f\"\\nClase 0 (IA): {class_distribution.get(0, 0)} documentos\")\n",
    "print(f\"Clase 1 (Humano): {class_distribution.get(1, 0)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b97c5150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Usando stratified split (mantiene proporción de clases)\n",
      "\n",
      "Documentos en train: 80\n",
      "Documentos en test: 20\n"
     ]
    }
   ],
   "source": [
    "# 4. Split de IDs (no de oraciones) - IMPORTANTE para evitar data leakage\n",
    "ids_list = list(id_to_class.keys())\n",
    "labels_list = [id_to_class[id_] for id_ in ids_list]\n",
    "\n",
    "# Verificar si podemos usar stratify\n",
    "class_counts = pd.Series(labels_list).value_counts()\n",
    "can_stratify = class_counts.min() >= 2\n",
    "\n",
    "if can_stratify:\n",
    "    print(\"✓ Usando stratified split (mantiene proporción de clases)\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels_list\n",
    "    )\n",
    "else:\n",
    "    print(f\"⚠️ No se puede usar stratify (clase mínima: {class_counts.min()} documentos)\")\n",
    "    print(\"Usando split aleatorio sin stratify\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=None  # Sin estratificación\n",
    "    )\n",
    "\n",
    "print(f\"\\nDocumentos en train: {len(train_ids)}\")\n",
    "print(f\"Documentos en test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "effebb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oraciones en train: 838\n",
      "Oraciones en test: 228\n",
      "\n",
      "Distribución en train:\n",
      "model\n",
      "mpt             145\n",
      "llama-chat      136\n",
      "mistral-chat    128\n",
      "gpt2            121\n",
      "mistral         108\n",
      "gpt4             52\n",
      "gpt3             34\n",
      "mpt-chat         32\n",
      "cohere-chat      29\n",
      "chatgpt          21\n",
      "cohere           20\n",
      "human            12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución en test:\n",
      "model\n",
      "llama-chat      55\n",
      "mistral         40\n",
      "mistral-chat    25\n",
      "chatgpt         20\n",
      "mpt             18\n",
      "gpt4            17\n",
      "gpt2            17\n",
      "human           11\n",
      "cohere          11\n",
      "gpt3             6\n",
      "mpt-chat         4\n",
      "cohere-chat      4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5. Filtrar oraciones según los IDs\n",
    "train_sentences = train_df[train_df['id'].isin(train_ids)]\n",
    "test_sentences = train_df[train_df['id'].isin(test_ids)]\n",
    "\n",
    "print(f\"\\nOraciones en train: {len(train_sentences)}\")\n",
    "print(f\"Oraciones en test: {len(test_sentences)}\")\n",
    "\n",
    "# Verificar distribución en cada conjunto\n",
    "print(f\"\\nDistribución en train:\")\n",
    "# print(train_sentences['is_human'].value_counts())\n",
    "print(train_sentences['model'].value_counts())\n",
    "print(f\"\\nDistribución en test:\")\n",
    "# print(test_sentences['is_human'].value_counts())\n",
    "print(test_sentences['model'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b469c06",
   "metadata": {},
   "source": [
    "## Preparación de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8668febf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Shape de X_train: (838, 196)\n",
      "Shape de y_train: (838,)\n",
      "Shape de X_test: (228, 196)\n",
      "Shape de y_test: (228,)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. Preparar X e y\n",
    "X_train = train_sentences[feature_columns].values\n",
    "y_train = train_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "X_test = test_sentences[feature_columns].values\n",
    "y_test = test_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Shape de X_train: {X_train.shape}\")\n",
    "print(f\"Shape de y_train: {y_train.shape}\")\n",
    "print(f\"Shape de X_test: {X_test.shape}\")\n",
    "print(f\"Shape de y_test: {y_test.shape}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebaa8044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando valores NaN en el dataset...\n",
      "\n",
      "NaNs en train_df: 1\n",
      "NaNs en feature_columns:\n",
      "ST_HERDAN_TTR    1\n",
      "dtype: int64\n",
      "\n",
      "NaNs en X_train: 0\n",
      "NaNs en X_test: 1\n",
      "NaNs en y_train: 0\n",
      "NaNs en y_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay valores NaN en los datos\n",
    "print(\"Verificando valores NaN en el dataset...\")\n",
    "print(f\"\\nNaNs en train_df: {train_df.isna().sum().sum()}\")\n",
    "print(f\"NaNs en feature_columns:\")\n",
    "nan_features = train_df[feature_columns].isna().sum()\n",
    "nan_features_with_nans = nan_features[nan_features > 0]\n",
    "if len(nan_features_with_nans) > 0:\n",
    "    print(nan_features_with_nans)\n",
    "else:\n",
    "    print(\"No hay NaNs en las features ✓\")\n",
    "\n",
    "print(f\"\\nNaNs en X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"NaNs en X_test: {np.isnan(X_test).sum()}\")\n",
    "print(f\"NaNs en y_train: {np.isnan(y_train).sum()}\")\n",
    "print(f\"NaNs en y_test: {np.isnan(y_test).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "338c0d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Valores NaN imputados con la media de cada feature\n",
      "NaNs en X_train_clean: 0\n",
      "NaNs en X_test_clean: 0\n"
     ]
    }
   ],
   "source": [
    "# Solución: Imputar valores NaN antes de entrenar\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Opción 1: Imputar con la media de cada feature\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Ajustar el imputer con los datos de entrenamiento\n",
    "X_train_clean = imputer.fit_transform(X_train)\n",
    "# Transformar los datos de test con el mismo imputer\n",
    "X_test_clean = imputer.transform(X_test)\n",
    "\n",
    "print(\"✓ Valores NaN imputados con la media de cada feature\")\n",
    "print(f\"NaNs en X_train_clean: {np.isnan(X_train_clean).sum()}\")\n",
    "print(f\"NaNs en X_test_clean: {np.isnan(X_test_clean).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88a0ac",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4709d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando SVM con split a nivel de documento...\n",
      "Kernel: RBF | C: 1.0 | Gamma: scale\n",
      "============================================================\n",
      "✓ Modelo entrenado exitosamente\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntrenando SVM con split a nivel de documento...\")\n",
    "print(\"Kernel: RBF | C: 1.0 | Gamma: scale\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train_clean, y_train)\n",
    "\n",
    "print(\"✓ Modelo entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c286de7",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c3180b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTADOS EN TEST SET (Split por Documento)\n",
      "============================================================\n",
      "\n",
      "Accuracy: 0.9518\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          IA     0.9518    1.0000    0.9753       217\n",
      "      Humano     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.9518       228\n",
      "   macro avg     0.4759    0.5000    0.4876       228\n",
      "weighted avg     0.9058    0.9518    0.9282       228\n",
      "\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[217   0]\n",
      " [ 11   0]]\n",
      "\n",
      "Interpretación:\n",
      "  TN (IA correctamente clasificada): 217\n",
      "  FP (IA clasificada como Humano): 0\n",
      "  FN (Humano clasificado como IA): 11\n",
      "  TP (Humano correctamente clasificado): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model.predict(X_test_clean)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTADOS EN TEST SET (Split por Documento)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['IA', 'Humano'], digits=4))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nInterpretación:\")\n",
    "print(f\"  TN (IA correctamente clasificada): {cm[0,0]}\")\n",
    "print(f\"  FP (IA clasificada como Humano): {cm[0,1]}\")\n",
    "print(f\"  FN (Humano clasificado como IA): {cm[1,0]}\")\n",
    "print(f\"  TP (Humano correctamente clasificado): {cm[1,1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
