{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ffbe577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb4f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet cargado: (4251000, 201)\n"
     ]
    }
   ],
   "source": [
    "# Verificar que parquet funciona\n",
    "df = pd.read_parquet(\"features_combined.parquet\")\n",
    "print(f\"Parquet cargado: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2c6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdtrainDF = pd.DataFrame(df)\n",
    "# pdtrainDF.to_csv('train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc9fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            id_original  model     domain  sentence_num  \\\n",
      "0  e5e058ce-be2b-459d-af36-32532aaba5ff  human  abstracts             0   \n",
      "1  e5e058ce-be2b-459d-af36-32532aaba5ff  human  abstracts             1   \n",
      "2  e5e058ce-be2b-459d-af36-32532aaba5ff  human  abstracts             2   \n",
      "3  e5e058ce-be2b-459d-af36-32532aaba5ff  human  abstracts             3   \n",
      "4  e5e058ce-be2b-459d-af36-32532aaba5ff  human  abstracts             4   \n",
      "\n",
      "                                                text  POS_VERB  POS_NOUN  \\\n",
      "0  The recent advancements in artificial intellig...  0.127660  0.446809   \n",
      "1  Notwithstanding the successes and future poten...  0.139535  0.255814   \n",
      "2  Despite these concerns and risks, there are cu...  0.107143  0.321429   \n",
      "3  To bridge this gap, this paper introduces a ca...  0.156250  0.250000   \n",
      "4  These guiding principles are named FUTURE-AI a...  0.160000  0.360000   \n",
      "\n",
      "    POS_ADJ   POS_ADV   POS_DET  ...   RE  ASF  ASM   OM  RCI       DMC   OR  \\\n",
      "0  0.148936  0.000000  0.085106  ...  0.0  0.0  0.0  0.0  0.0  0.021277  0.0   \n",
      "1  0.232558  0.000000  0.046512  ...  0.0  0.0  0.0  0.0  0.0  0.000000  0.0   \n",
      "2  0.142857  0.035714  0.071429  ...  0.0  0.0  0.0  0.0  0.0  0.000000  0.0   \n",
      "3  0.125000  0.000000  0.125000  ...  0.0  0.0  0.0  0.0  0.0  0.000000  0.0   \n",
      "4  0.000000  0.000000  0.040000  ...  0.0  0.0  0.0  0.0  0.0  0.000000  0.0   \n",
      "\n",
      "        QAS   PA   PR  \n",
      "0  0.085106  0.0  0.0  \n",
      "1  0.046512  0.0  0.0  \n",
      "2  0.000000  0.0  0.0  \n",
      "3  0.031250  0.0  0.0  \n",
      "4  0.000000  0.0  0.0  \n",
      "\n",
      "[5 rows x 201 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ver los primeros registros\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1ebaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de documentos con solo una oración: 24608\n",
      "Total de documentos: 408435\n",
      "Porcentaje: 6.02%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_original</th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>text</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>POS_ADV</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>...</th>\n",
       "      <th>RE</th>\n",
       "      <th>ASF</th>\n",
       "      <th>ASM</th>\n",
       "      <th>OM</th>\n",
       "      <th>RCI</th>\n",
       "      <th>DMC</th>\n",
       "      <th>OR</th>\n",
       "      <th>QAS</th>\n",
       "      <th>PA</th>\n",
       "      <th>PR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13589</th>\n",
       "      <td>8098ecbb-b1a5-4c28-8490-823f24e6fd36</td>\n",
       "      <td>mpt</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0</td>\n",
       "      <td>There have been significant interest, investme...</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13681</th>\n",
       "      <td>49e7772f-7100-4d77-be0f-64d4f3860697</td>\n",
       "      <td>mpt</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0</td>\n",
       "      <td>https://arxivnotebook-dataverse3d9m5c01hz4vhwu...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715</th>\n",
       "      <td>8f1824b0-4f23-417a-82b3-2da431cdfd6a</td>\n",
       "      <td>mpt</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;div align=\"center\"&gt; &lt;img src=\"http://imgur.co...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13773</th>\n",
       "      <td>0a65cc0c-0b77-43ae-8e8f-75321a878a1d</td>\n",
       "      <td>mpt</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0</td>\n",
       "      <td>http://arxiv​.​org/abs/​1605﻿...c95e9a6f4b582a...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13911</th>\n",
       "      <td>2a6d1b68-faf8-4551-a116-23e68dcd41ad</td>\n",
       "      <td>mpt</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>0</td>\n",
       "      <td>A simple but effective visual tracking algorit...</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.371901</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.033058</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248178</th>\n",
       "      <td>5ebd895c-a74e-41c9-af53-9de045227d5e</td>\n",
       "      <td>mpt</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0</td>\n",
       "      <td>*{{Infobox railroadstation|image = [[Image;Fil...</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248203</th>\n",
       "      <td>94c8ec93-71ec-473b-8505-867dda26211b</td>\n",
       "      <td>mpt</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0</td>\n",
       "      <td>A B C D E F G H I J L M N O P R S T U V W X Y ...</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>0.556391</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248481</th>\n",
       "      <td>f77a2b9b-6ec6-44bd-81ea-16a6d4d25d2e</td>\n",
       "      <td>mpt</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0</td>\n",
       "      <td>[1] Cornelie or Cornele (also known as Kornél)...</td>\n",
       "      <td>0.146965</td>\n",
       "      <td>0.389776</td>\n",
       "      <td>0.067093</td>\n",
       "      <td>0.031949</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.01278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248680</th>\n",
       "      <td>08e6aa7e-f86a-468b-9124-e21994ab57a9</td>\n",
       "      <td>mpt</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0</td>\n",
       "      <td>This band name has already been used by other ...</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.060185</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250256</th>\n",
       "      <td>9b6a09ac-f85d-4520-9cfb-4618d6d56386</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0</td>\n",
       "      <td>Kaká Kaká Kaká Kaká Kaká Kaká Kaká Kaká Kaká K...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24608 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id_original     model     domain  \\\n",
       "13589    8098ecbb-b1a5-4c28-8490-823f24e6fd36       mpt  abstracts   \n",
       "13681    49e7772f-7100-4d77-be0f-64d4f3860697       mpt  abstracts   \n",
       "13715    8f1824b0-4f23-417a-82b3-2da431cdfd6a       mpt  abstracts   \n",
       "13773    0a65cc0c-0b77-43ae-8e8f-75321a878a1d       mpt  abstracts   \n",
       "13911    2a6d1b68-faf8-4551-a116-23e68dcd41ad       mpt  abstracts   \n",
       "...                                       ...       ...        ...   \n",
       "4248178  5ebd895c-a74e-41c9-af53-9de045227d5e       mpt       wiki   \n",
       "4248203  94c8ec93-71ec-473b-8505-867dda26211b       mpt       wiki   \n",
       "4248481  f77a2b9b-6ec6-44bd-81ea-16a6d4d25d2e       mpt       wiki   \n",
       "4248680  08e6aa7e-f86a-468b-9124-e21994ab57a9       mpt       wiki   \n",
       "4250256  9b6a09ac-f85d-4520-9cfb-4618d6d56386  mpt-chat       wiki   \n",
       "\n",
       "         sentence_num                                               text  \\\n",
       "13589               0  There have been significant interest, investme...   \n",
       "13681               0  https://arxivnotebook-dataverse3d9m5c01hz4vhwu...   \n",
       "13715               0  <div align=\"center\"> <img src=\"http://imgur.co...   \n",
       "13773               0  http://arxiv​.​org/abs/​1605﻿...c95e9a6f4b582a...   \n",
       "13911               0  A simple but effective visual tracking algorit...   \n",
       "...               ...                                                ...   \n",
       "4248178             0  *{{Infobox railroadstation|image = [[Image;Fil...   \n",
       "4248203             0  A B C D E F G H I J L M N O P R S T U V W X Y ...   \n",
       "4248481             0  [1] Cornelie or Cornele (also known as Kornél)...   \n",
       "4248680             0  This band name has already been used by other ...   \n",
       "4250256             0  Kaká Kaká Kaká Kaká Kaká Kaká Kaká Kaká Kaká K...   \n",
       "\n",
       "         POS_VERB  POS_NOUN   POS_ADJ   POS_ADV   POS_DET  ...        RE  \\\n",
       "13589    0.230769  0.230769  0.153846  0.230769  0.000000  ...  0.000000   \n",
       "13681    0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "13715    0.000000  1.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "13773    0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "13911    0.140496  0.371901  0.157025  0.033058  0.024793  ...  0.000000   \n",
       "...           ...       ...       ...       ...       ...  ...       ...   \n",
       "4248178  0.008929  0.071429  0.000000  0.004464  0.000000  ...  0.004464   \n",
       "4248203  0.045113  0.556391  0.015038  0.007519  0.007519  ...  0.000000   \n",
       "4248481  0.146965  0.389776  0.067093  0.031949  0.012780  ...  0.003195   \n",
       "4248680  0.046296  0.060185  0.023148  0.013889  0.018519  ...  0.000000   \n",
       "4250256  0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "\n",
       "              ASF       ASM        OM  RCI       DMC        OR      QAS   PA  \\\n",
       "13589    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.00000  0.0   \n",
       "13681    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.00000  0.0   \n",
       "13715    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.00000  0.0   \n",
       "13773    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.00000  0.0   \n",
       "13911    0.008264  0.024793  0.008264  0.0  0.016529  0.008264  0.00000  0.0   \n",
       "...           ...       ...       ...  ...       ...       ...      ...  ...   \n",
       "4248178  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.00000  0.0   \n",
       "4248203  0.000000  0.000000  0.007519  0.0  0.000000  0.000000  0.00000  0.0   \n",
       "4248481  0.000000  0.006390  0.009585  0.0  0.003195  0.006390  0.01278  0.0   \n",
       "4248680  0.000000  0.004630  0.004630  0.0  0.000000  0.000000  0.00463  0.0   \n",
       "4250256  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.00000  0.0   \n",
       "\n",
       "               PR  \n",
       "13589    0.000000  \n",
       "13681    0.000000  \n",
       "13715    0.000000  \n",
       "13773    0.000000  \n",
       "13911    0.000000  \n",
       "...           ...  \n",
       "4248178  0.000000  \n",
       "4248203  0.000000  \n",
       "4248481  0.003195  \n",
       "4248680  0.000000  \n",
       "4250256  0.000000  \n",
       "\n",
       "[24608 rows x 201 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verificar documentos con solo una oración (sentence_num = 0 únicamente)\n",
    "# Contar oraciones por ID\n",
    "oraciones_por_id = df.groupby('id_original')['sentence_num'].apply(lambda x: x.tolist())\n",
    "\n",
    "# Filtrar IDs que solo tienen sentence_num = 0\n",
    "ids_una_oracion = oraciones_por_id[oraciones_por_id.apply(lambda x: x == [0])].index\n",
    "\n",
    "print(f\"Número de documentos con solo una oración: {len(ids_una_oracion)}\")\n",
    "print(f\"Total de documentos: {df['id_original'].nunique()}\")\n",
    "print(f\"Porcentaje: {len(ids_una_oracion) / df['id_original'].nunique() * 100:.2f}%\")\n",
    "\n",
    "# Mostrar df con los documentos con solo una oración\n",
    "df_una_oracion = df[df['id_original'].isin(ids_una_oracion)]\n",
    "display(df_una_oracion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3e929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARACIÓN: DATAFRAME ORIGINAL vs FILTRADO\n",
      "============================================================\n",
      "\n",
      "Dataframe ORIGINAL:\n",
      "  Total de oraciones: 4,251,000\n",
      "  Total de documentos: 408,435\n",
      "  Oraciones por documento (promedio): 10.41\n",
      "\n",
      "Dataframe FILTRADO (sin docs de 1 oración):\n",
      "  Total de oraciones: 4,226,384\n",
      "  Total de documentos: 383,823\n",
      "  Oraciones por documento (promedio): 11.01\n",
      "\n",
      "Documentos eliminados: 24,612\n",
      "Oraciones eliminadas: 24,616\n",
      "Porcentaje de datos retenidos: 99.42%\n",
      "\n",
      "Distribución por clase (FILTRADO):\n",
      "model\n",
      "gpt2            645296\n",
      "llama-chat      560308\n",
      "mistral         496884\n",
      "mpt             485978\n",
      "mistral-chat    410953\n",
      "gpt4            309527\n",
      "chatgpt         290701\n",
      "mpt-chat        271297\n",
      "cohere          242967\n",
      "cohere-chat     197063\n",
      "human           161988\n",
      "gpt3            153422\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Crear dataframe excluyendo documentos con solo una oración\n",
    "# Identificar documentos con más de una oración\n",
    "oraciones_por_doc = df.groupby('id_original')['sentence_num'].max()\n",
    "ids_multiples_oraciones = oraciones_por_doc[oraciones_por_doc > 0].index\n",
    "\n",
    "# Crear nuevo dataframe filtrado\n",
    "df_filtered = df[df['id_original'].isin(ids_multiples_oraciones)].copy()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARACIÓN: DATAFRAME ORIGINAL vs FILTRADO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataframe ORIGINAL:\")\n",
    "print(f\"  Total de oraciones: {len(df):,}\")\n",
    "print(f\"  Total de documentos: {df['id_original'].nunique():,}\")\n",
    "print(f\"  Oraciones por documento (promedio): {len(df) / df['id_original'].nunique():.2f}\")\n",
    "\n",
    "print(f\"\\nDataframe FILTRADO (sin docs de 1 oración):\")\n",
    "print(f\"  Total de oraciones: {len(df_filtered):,}\")\n",
    "print(f\"  Total de documentos: {df_filtered['id_original'].nunique():,}\")\n",
    "print(f\"  Oraciones por documento (promedio): {len(df_filtered) / df_filtered['id_original'].nunique():.2f}\")\n",
    "\n",
    "print(f\"\\nDocumentos eliminados: {df['id_original'].nunique() - df_filtered['id_original'].nunique():,}\")\n",
    "print(f\"Oraciones eliminadas: {len(df) - len(df_filtered):,}\")\n",
    "print(f\"Porcentaje de datos retenidos: {len(df_filtered) / len(df) * 100:.2f}%\")\n",
    "\n",
    "# Verificar distribución por clase\n",
    "print(f\"\\nDistribución por clase (FILTRADO):\")\n",
    "print(df_filtered['model'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3447d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del dataset de entrenamiento:\n",
      "Forma del dataset: (4226384, 201)\n",
      "Columnas: ['id_original', 'model', 'domain', 'sentence_num', 'text', 'POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP', 'POS_PRO', 'L_REF', 'L_HASHTAG', 'L_MENTION', 'L_RT', 'L_LINKS', 'L_CONT_A', 'L_FUNC_A', 'L_CONT_T', 'L_FUNC_T', 'L_PLURAL_NOUNS', 'L_SINGULAR_NOUNS', 'L_PROPER_NAME', 'L_PERSONAL_NAME', 'L_NOUN_PHRASES', 'L_PUNCT', 'L_PUNCT_DOT', 'L_PUNCT_COM', 'L_PUNCT_SEMC', 'L_PUNCT_COL', 'L_PUNCT_DASH', 'L_POSSESSIVES', 'L_ADJ_POSITIVE', 'L_ADJ_COMPARATIVE', 'L_ADJ_SUPERLATIVE', 'L_ADV_POSITIVE', 'L_ADV_COMPARATIVE', 'L_ADV_SUPERLATIVE', 'PS_CONTRADICTION', 'PS_AGREEMENT', 'PS_EXAMPLES', 'PS_CONSEQUENCE', 'PS_CAUSE', 'PS_LOCATION', 'PS_TIME', 'PS_CONDITION', 'PS_MANNER', 'SY_QUESTION', 'SY_NARRATIVE', 'SY_NEGATIVE_QUESTIONS', 'SY_SPECIAL_QUESTIONS', 'SY_TAG_QUESTIONS', 'SY_GENERAL_QUESTIONS', 'SY_EXCLAMATION', 'SY_IMPERATIVE', 'SY_SUBORD_SENT', 'SY_SUBORD_SENT_PUNCT', 'SY_COORD_SENT', 'SY_COORD_SENT_PUNCT', 'SY_SIMPLE_SENT', 'SY_INVERSE_PATTERNS', 'SY_SIMILE', 'SY_FRONTING', 'SY_IRRITATION', 'SY_INTENSIFIER', 'SY_QUOT', 'VT_PRESENT_SIMPLE', 'VT_PRESENT_PROGRESSIVE', 'VT_PRESENT_PERFECT', 'VT_PRESENT_PERFECT_PROGR', 'VT_PRESENT_SIMPLE_PASSIVE', 'VT_PRESENT_PROGR_PASSIVE', 'VT_PRESENT_PERFECT_PASSIVE', 'VT_PAST_SIMPLE', 'VT_PAST_SIMPLE_BE', 'VT_PAST_PROGR', 'VT_PAST_PERFECT', 'VT_PAST_PERFECT_PROGR', 'VT_PAST_SIMPLE_PASSIVE', 'VT_PAST_POGR_PASSIVE', 'VT_PAST_PERFECT_PASSIVE', 'VT_FUTURE_SIMPLE', 'VT_FUTURE_PROGRESSIVE', 'VT_FUTURE_PERFECT', 'VT_FUTURE_PERFECT_PROGR', 'VT_FUTURE_SIMPLE_PASSIVE', 'VT_FUTURE_PROGR_PASSIVE', 'VT_FUTURE_PERFECT_PASSIVE', 'VT_WOULD', 'VT_WOULD_PASSIVE', 'VT_WOULD_PROGRESSIVE', 'VT_WOULD_PERFECT', 'VT_WOULD_PERFECT_PASSIVE', 'VT_SHOULD', 'VT_SHOULD_PASSIVE', 'VT_SHALL', 'VT_SHALL_PASSIVE', 'VT_SHOULD_PROGRESSIVE', 'VT_SHOULD_PERFECT', 'VT_SHOULD_PERFECT_PASSIVE', 'VT_MUST', 'VT_MUST_PASSIVE', 'VT_MUST_PROGRESSIVE', 'VT_MUST_PERFECT', 'VT_MST_PERFECT_PASSIVE', 'VT_CAN', 'VT_CAN_PASSIVE', 'VT_COULD', 'VT_COULD_PASSIVE', 'VT_CAN_PROGRESSIVE', 'VT_COULD_PROGRESSIVE', 'VT_COULD_PERFECT', 'VT_COULD_PERFECT_PASSIVE', 'VT_MAY', 'VT_MAY_PASSIVE', 'VT_MIGHT', 'VT_MIGHT_PASSIVE', 'VT_MAY_PROGRESSIVE', 'VT_MIGTH_PERFECT', 'VT_MIGHT_PERFECT_PASSIVE', 'VT_MAY_PERFECT_PASSIVE', 'ST_TYPE_TOKEN_RATIO_LEMMAS', 'ST_HERDAN_TTR', 'ST_MASS_TTR', 'ST_SENT_WRDSPERSENT', 'ST_SENT_DIFFERENCE', 'ST_REPETITIONS_WORDS', 'ST_REPETITIONS_SENT', 'ST_SENT_D_VP', 'ST_SENT_D_NP', 'ST_SENT_D_PP', 'ST_SENT_D_ADJP', 'ST_SENT_D_ADVP', 'L_I_PRON', 'L_HE_PRON', 'L_SHE_PRON', 'L_IT_PRON', 'L_YOU_PRON', 'L_WE_PRON', 'L_THEY_PRON', 'L_ME_PRON', 'L_YOU_OBJ_PRON', 'L_HIM_PRON', 'L_HER_OBJECT_PRON', 'L_IT_OBJECT_PRON', 'L_US_PRON', 'L_THEM_PRON', 'L_MY_PRON', 'L_YOUR_PRON', 'L_HIS_PRON', 'L_HER_PRON', 'L_ITS_PRON', 'L_OUR_PRON', 'L_THEIR_PRON', 'L_YOURS_PRON', 'L_THEIRS_PRON', 'L_HERS_PRON', 'L_OURS_PRON', 'L_MYSELF_PRON', 'L_YOURSELF_PRON', 'L_HIMSELF_PRON', 'L_HERSELF_PRON', 'L_ITSELF_PRON', 'L_OURSELVES_PRON', 'L_YOURSELVES_PRON', 'L_THEMSELVES_PRON', 'L_FIRST_PERSON_SING_PRON', 'L_SECOND_PERSON_PRON', 'L_THIRD_PERSON_SING_PRON', 'L_THIRD_PERSON_PLURAL_PRON', 'VF_INFINITIVE', 'G_PASSIVE', 'G_ACTIVE', 'G_PRESENT', 'G_PAST', 'G_FUTURE', 'G_MODALS_SIMPLE', 'G_MODALS_CONT', 'G_MODALS_PERFECT', 'AN', 'DDP', 'SVP', 'CDS', 'DDF', 'IS', 'PS', 'RE', 'ASF', 'ASM', 'OM', 'RCI', 'DMC', 'OR', 'QAS', 'PA', 'PR']\n",
      "Modelos unicos: ['human' 'llama-chat' 'mpt' 'mpt-chat' 'gpt2' 'mistral' 'mistral-chat'\n",
      " 'gpt3' 'cohere' 'chatgpt' 'gpt4' 'cohere-chat']\n",
      "Dominios unicos: ['abstracts' 'books' 'news' 'poetry' 'recipes' 'reddit' 'reviews' 'wiki']\n"
     ]
    }
   ],
   "source": [
    "print(\"Información del dataset de entrenamiento:\")\n",
    "print(f\"Forma del dataset: {df_filtered.shape}\")\n",
    "print(f\"Columnas: {list(df.columns)}\")\n",
    "print(f\"Modelos unicos: {df['model'].unique()}\")\n",
    "print(f\"Dominios unicos: {df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8b3a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "model\n",
      "False    4064396\n",
      "True      161988\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total de oraciones: 4251000\n"
     ]
    }
   ],
   "source": [
    "# 1. Crear etiqueta binaria: 1 = Humano (model == 'human'), 0 = IA (resto)\n",
    "print(\"Distribución de clases:\")\n",
    "print((df_filtered['model'] == 'human').value_counts())\n",
    "print(f\"\\nTotal de oraciones: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c7fcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de documentos únicos: 383823\n"
     ]
    }
   ],
   "source": [
    "# 3. Obtener IDs únicos y crear mapping de ID a clase\n",
    "unique_ids = df_filtered['id_original'].unique()\n",
    "print(f\"\\nTotal de documentos únicos: {len(unique_ids)}\")\n",
    "\n",
    "# Mapping de ID a clase (binaria: humano vs IA)\n",
    "id_to_class = df_filtered.groupby('id_original')['model'].first().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "600fac9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de documentos por clase:\n",
      "mistral-chat    44736\n",
      "llama-chat      44692\n",
      "mpt-chat        44664\n",
      "mistral         42295\n",
      "gpt2            42222\n",
      "mpt             41645\n",
      "gpt4            23154\n",
      "chatgpt         23044\n",
      "cohere          22370\n",
      "cohere-chat     22029\n",
      "gpt3            21242\n",
      "human           11730\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clase IA (no-human): 372093 documentos\n",
      "Clase Humano: 11730 documentos\n",
      "\n",
      "Total de documentos: 383823\n",
      "Proporción Humano/IA: 3.06% / 96.94%\n"
     ]
    }
   ],
   "source": [
    "# Verificar cuántos documentos hay por clase\n",
    "print(\"Distribución de documentos por clase:\")\n",
    "class_distribution = pd.Series(id_to_class.values()).value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "# Contar correctamente humanos vs IA\n",
    "human_count = sum(1 for model in id_to_class.values() if model == 'human')\n",
    "ai_count = sum(1 for model in id_to_class.values() if model != 'human')\n",
    "\n",
    "print(f\"\\nClase IA (no-human): {ai_count} documentos\")\n",
    "print(f\"Clase Humano: {human_count} documentos\")\n",
    "print(f\"\\nTotal de documentos: {human_count + ai_count}\")\n",
    "print(f\"Proporción Humano/IA: {human_count / (human_count + ai_count) * 100:.2f}% / {ai_count / (human_count + ai_count) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b9c9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BALANCEO DE DATASET (RATIO MÁXIMO 5:1)\n",
      "======================================================================\n",
      "\n",
      "1. DISTRIBUCIÓN ACTUAL:\n",
      "   Total documentos: 383823\n",
      "   Total oraciones: 4226384\n",
      "\n",
      "   Documentos por modelo:\n",
      "   - chatgpt: 23044\n",
      "   - cohere: 22370\n",
      "   - cohere-chat: 22029\n",
      "   - gpt2: 42222\n",
      "   - gpt3: 21242\n",
      "   - gpt4: 23154\n",
      "   - human: 11730\n",
      "   - llama-chat: 44692\n",
      "   - mistral: 42295\n",
      "   - mistral-chat: 44736\n",
      "   - mpt: 41645\n",
      "   - mpt-chat: 44664\n",
      "\n",
      "   Humanos: 11730 docs\n",
      "   IA: 372093 docs\n",
      "   Ratio actual (IA/Humano): 31.72:1\n",
      "\n",
      "2. OBJETIVO DE BALANCEO:\n",
      "   Mantener 11730 docs humanos\n",
      "   Reducir a 58650 docs IA (ratio 5.00:1)\n",
      "\n",
      "3. ESTRATEGIA DE SAMPLING:\n",
      "   11 modelos IA detectados: ['llama-chat', 'mpt', 'mpt-chat', 'gpt2', 'mistral', 'mistral-chat', 'gpt3', 'cohere', 'chatgpt', 'gpt4', 'cohere-chat']\n",
      "   Documentos por modelo IA: ~5331\n",
      "   - llama-chat: 5332 docs seleccionados\n",
      "   - mpt: 5332 docs seleccionados\n",
      "   - mpt-chat: 4920 docs seleccionados\n",
      "   - gpt2: 5332 docs seleccionados\n",
      "   - mistral: 5332 docs seleccionados\n",
      "   - mistral-chat: 5332 docs seleccionados\n",
      "   - gpt3: 5332 docs seleccionados\n",
      "   - cohere: 5332 docs seleccionados\n",
      "   - chatgpt: 5332 docs seleccionados\n",
      "   - gpt4: 5331 docs seleccionados\n",
      "   - cohere-chat: 5331 docs seleccionados\n",
      "\n",
      "======================================================================\n",
      "RESULTADO DEL BALANCEO:\n",
      "======================================================================\n",
      "\n",
      "Documentos ANTES del balanceo:\n",
      "  Humanos: 11730 | IA: 372093 | Ratio: 31.72:1\n",
      "\n",
      "Documentos DESPUÉS del balanceo:\n",
      "  Humanos: 11730 | IA: 58238 | Ratio: 4.96:1\n",
      "\n",
      "Oraciones ANTES: 4,226,384\n",
      "Oraciones DESPUÉS: 800,940\n",
      "Reducción: 81.0%\n",
      "\n",
      "Distribución por modelo (documentos):\n",
      "  - chatgpt: 5332\n",
      "  - cohere: 5332\n",
      "  - cohere-chat: 5331\n",
      "  - gpt2: 5332\n",
      "  - gpt3: 5332\n",
      "  - gpt4: 5331\n",
      "  - human: 11730\n",
      "  - llama-chat: 5332\n",
      "  - mistral: 5332\n",
      "  - mistral-chat: 5332\n",
      "  - mpt: 5332\n",
      "  - mpt-chat: 4920\n",
      "\n",
      "Distribución por dominio (documentos):\n",
      "  - abstracts: 9861\n",
      "  - books: 9877\n",
      "  - news: 9876\n",
      "  - poetry: 9670\n",
      "  - recipes: 9855\n",
      "  - reddit: 9746\n",
      "  - reviews: 9005\n",
      "  - wiki: 2078\n",
      "\n",
      "======================================================================\n",
      "✓ df_filtered actualizado con dataset balanceado\n",
      "  800,940 oraciones de 69968 documentos\n",
      "============================================================\n",
      "FILTRADO POR MODELOS ESPECÍFICOS\n",
      "============================================================\n",
      "Modelos incluidos: ['human', 'gpt4', 'chatgpt', 'llama-chat', 'mpt', 'mpt-chat']\n",
      "\n",
      "Distribución de documentos por modelo:\n",
      "human         11730\n",
      "llama-chat     5332\n",
      "chatgpt        5332\n",
      "mpt            5332\n",
      "gpt4           5331\n",
      "mpt-chat       4920\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Clase IA (no-human): 26247 documentos\n",
      "Clase Humano: 11730 documentos\n",
      "\n",
      "Total de documentos: 37977\n",
      "Proporción Humano/IA: 30.89% / 69.11%\n",
      "============================================================\n",
      "\n",
      "Total de oraciones en df_filtered: 465,697\n"
     ]
    }
   ],
   "source": [
    "# Balancear dataset manteniendo proporción 5:1 (IA:Humano) máximo\n",
    "# Balanceo por modelo y dominio, sin fraccionar documentos\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BALANCEO DE DATASET (RATIO MÁXIMO 5:1)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Analizar distribución actual\n",
    "print(\"\\n1. DISTRIBUCIÓN ACTUAL:\")\n",
    "print(f\"   Total documentos: {df_filtered['id_original'].nunique()}\")\n",
    "print(f\"   Total oraciones: {len(df_filtered)}\")\n",
    "\n",
    "# Contar documentos por modelo\n",
    "docs_por_modelo = df_filtered.groupby('model')['id_original'].nunique()\n",
    "print(f\"\\n   Documentos por modelo:\")\n",
    "for modelo, count in docs_por_modelo.items():\n",
    "    print(f\"   - {modelo}: {count}\")\n",
    "\n",
    "# Contar documentos humanos vs IA\n",
    "human_docs = df_filtered[df_filtered['model'] == 'human']['id_original'].nunique()\n",
    "ai_docs = df_filtered[df_filtered['model'] != 'human']['id_original'].nunique()\n",
    "ratio_actual = ai_docs / human_docs if human_docs > 0 else 0\n",
    "\n",
    "print(f\"\\n   Humanos: {human_docs} docs\")\n",
    "print(f\"   IA: {ai_docs} docs\")\n",
    "print(f\"   Ratio actual (IA/Humano): {ratio_actual:.2f}:1\")\n",
    "\n",
    "# 2. Calcular cuántos documentos IA necesitamos para ratio 5:1\n",
    "MAX_RATIO = 5\n",
    "target_ai_docs = min(ai_docs, int(human_docs * MAX_RATIO))\n",
    "\n",
    "print(f\"\\n2. OBJETIVO DE BALANCEO:\")\n",
    "print(f\"   Mantener {human_docs} docs humanos\")\n",
    "print(f\"   Reducir a {target_ai_docs} docs IA (ratio {target_ai_docs/human_docs:.2f}:1)\")\n",
    "\n",
    "# 3. Balancear por modelo y dominio\n",
    "# Obtener modelos IA únicos\n",
    "ai_models = df_filtered[df_filtered['model'] != 'human']['model'].unique()\n",
    "num_ai_models = len(ai_models)\n",
    "\n",
    "# Distribuir documentos IA equitativamente entre modelos\n",
    "docs_per_ai_model = target_ai_docs // num_ai_models\n",
    "remainder = target_ai_docs % num_ai_models\n",
    "\n",
    "print(f\"\\n3. ESTRATEGIA DE SAMPLING:\")\n",
    "print(f\"   {num_ai_models} modelos IA detectados: {list(ai_models)}\")\n",
    "print(f\"   Documentos por modelo IA: ~{docs_per_ai_model}\")\n",
    "\n",
    "# 4. Seleccionar documentos balanceados\n",
    "selected_doc_ids = []\n",
    "\n",
    "# Mantener todos los documentos humanos\n",
    "human_doc_ids = df_filtered[df_filtered['model'] == 'human']['id_original'].unique()\n",
    "selected_doc_ids.extend(human_doc_ids)\n",
    "\n",
    "# Para cada modelo IA, seleccionar documentos balanceados por dominio\n",
    "for i, ai_model in enumerate(ai_models):\n",
    "    # Obtener documentos de este modelo\n",
    "    model_df = df_filtered[df_filtered['model'] == ai_model]\n",
    "    model_doc_ids = model_df['id_original'].unique()\n",
    "    \n",
    "    # Calcular cuántos docs seleccionar (distribución equitativa con resto)\n",
    "    n_docs_to_select = docs_per_ai_model + (1 if i < remainder else 0)\n",
    "    \n",
    "    # Obtener dominios disponibles para este modelo\n",
    "    domains = model_df['domain'].unique()\n",
    "    \n",
    "    # Balancear por dominio\n",
    "    docs_per_domain = n_docs_to_select // len(domains)\n",
    "    domain_remainder = n_docs_to_select % len(domains)\n",
    "    \n",
    "    model_selected = []\n",
    "    for j, domain in enumerate(domains):\n",
    "        # Docs de este modelo y dominio\n",
    "        domain_docs = model_df[model_df['domain'] == domain]['id_original'].unique()\n",
    "        \n",
    "        # Cuántos seleccionar de este dominio\n",
    "        n_select = docs_per_domain + (1 if j < domain_remainder else 0)\n",
    "        n_select = min(n_select, len(domain_docs))\n",
    "        \n",
    "        # Sample aleatorio\n",
    "        if n_select > 0:\n",
    "            sampled = pd.Series(domain_docs).sample(n=n_select, random_state=42).tolist()\n",
    "            model_selected.extend(sampled)\n",
    "    \n",
    "    selected_doc_ids.extend(model_selected)\n",
    "    print(f\"   - {ai_model}: {len(model_selected)} docs seleccionados\")\n",
    "\n",
    "# 5. Crear dataframe balanceado\n",
    "df_balanced = df_filtered[df_filtered['id_original'].isin(selected_doc_ids)].copy()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESULTADO DEL BALANCEO:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Estadísticas finales\n",
    "balanced_human_docs = df_balanced[df_balanced['model'] == 'human']['id_original'].nunique()\n",
    "balanced_ai_docs = df_balanced[df_balanced['model'] != 'human']['id_original'].nunique()\n",
    "balanced_ratio = balanced_ai_docs / balanced_human_docs if balanced_human_docs > 0 else 0\n",
    "\n",
    "print(f\"\\nDocumentos ANTES del balanceo:\")\n",
    "print(f\"  Humanos: {human_docs} | IA: {ai_docs} | Ratio: {ratio_actual:.2f}:1\")\n",
    "print(f\"\\nDocumentos DESPUÉS del balanceo:\")\n",
    "print(f\"  Humanos: {balanced_human_docs} | IA: {balanced_ai_docs} | Ratio: {balanced_ratio:.2f}:1\")\n",
    "\n",
    "print(f\"\\nOraciones ANTES: {len(df_filtered):,}\")\n",
    "print(f\"Oraciones DESPUÉS: {len(df_balanced):,}\")\n",
    "print(f\"Reducción: {(1 - len(df_balanced)/len(df_filtered))*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nDistribución por modelo (documentos):\")\n",
    "docs_por_modelo_balanced = df_balanced.groupby('model')['id_original'].nunique()\n",
    "for modelo, count in docs_por_modelo_balanced.items():\n",
    "    print(f\"  - {modelo}: {count}\")\n",
    "\n",
    "print(f\"\\nDistribución por dominio (documentos):\")\n",
    "docs_por_dominio_balanced = df_balanced.groupby('domain')['id_original'].nunique()\n",
    "for dominio, count in docs_por_dominio_balanced.items():\n",
    "    print(f\"  - {dominio}: {count}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Actualizar df_filtered con el dataset balanceado\n",
    "df_filtered = df_balanced.copy()\n",
    "print(f\"✓ df_filtered actualizado con dataset balanceado\")\n",
    "print(f\"  {len(df_filtered):,} oraciones de {df_filtered['id_original'].nunique()} documentos\")\n",
    "# ocumentos para mantener solo modelos específicos\n",
    "modelos_deseados = ['human', 'gpt4', 'chatgpt', 'llama-chat', 'mpt', 'mpt-chat']\n",
    "\n",
    "# Filtrar el dataframe df_filtered para incluir solo los modelos deseados\n",
    "df_filtered = df_filtered[df_filtered['model'].isin(modelos_deseados)].copy()\n",
    "\n",
    "# Recrear el mapping id_to_class con los datos filtrados\n",
    "id_to_class = df_filtered.groupby('id_original')['model'].first().to_dict()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FILTRADO POR MODELOS ESPECÍFICOS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Modelos incluidos: {modelos_deseados}\")\n",
    "print(f\"\\nDistribución de documentos por modelo:\")\n",
    "model_distribution = pd.Series(id_to_class.values()).value_counts()\n",
    "print(model_distribution)\n",
    "\n",
    "# Contar correctamente humanos vs IA\n",
    "human_count = sum(1 for model in id_to_class.values() if model == 'human')\n",
    "ai_count = sum(1 for model in id_to_class.values() if model != 'human')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Clase IA (no-human): {ai_count} documentos\")\n",
    "print(f\"Clase Humano: {human_count} documentos\")\n",
    "print(f\"\\nTotal de documentos: {human_count + ai_count}\")\n",
    "print(f\"Proporción Humano/IA: {human_count / (human_count + ai_count) * 100:.2f}% / {ai_count / (human_count + ai_count) * 100:.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTotal de oraciones en df_filtered: {len(df_filtered):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59577acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe filtrado y balanceado guardado en 'features_filtered_balanced.parquet'\n"
     ]
    }
   ],
   "source": [
    "# # Guardar el dataframe filtrado y balanceado\n",
    "df_filtered.to_parquet(\"features_filtered_balanced.parquet\", index=False)\n",
    "print(\"Dataframe filtrado y balanceado guardado en 'features_filtered_balanced.parquet'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
