,text,POS_VERB,POS_NOUN,POS_ADJ,POS_ADV,POS_DET,POS_INTJ,POS_CONJ,POS_PART,POS_NUM,POS_PREP,POS_PRO,L_REF,L_HASHTAG,L_MENTION,L_RT,L_LINKS,L_CONT_A,L_FUNC_A,L_CONT_T,L_FUNC_T,L_PLURAL_NOUNS,L_SINGULAR_NOUNS,L_PROPER_NAME,L_PERSONAL_NAME,L_NOUN_PHRASES,L_PUNCT,L_PUNCT_DOT,L_PUNCT_COM,L_PUNCT_SEMC,L_PUNCT_COL,L_PUNCT_DASH,L_POSSESSIVES,L_ADJ_POSITIVE,L_ADJ_COMPARATIVE,L_ADJ_SUPERLATIVE,L_ADV_POSITIVE,L_ADV_COMPARATIVE,L_ADV_SUPERLATIVE,PS_CONTRADICTION,PS_AGREEMENT,PS_EXAMPLES,PS_CONSEQUENCE,PS_CAUSE,PS_LOCATION,PS_TIME,PS_CONDITION,PS_MANNER,SY_QUESTION,SY_NARRATIVE,SY_NEGATIVE_QUESTIONS,SY_SPECIAL_QUESTIONS,SY_TAG_QUESTIONS,SY_GENERAL_QUESTIONS,SY_EXCLAMATION,SY_IMPERATIVE,SY_SUBORD_SENT,SY_SUBORD_SENT_PUNCT,SY_COORD_SENT,SY_COORD_SENT_PUNCT,SY_SIMPLE_SENT,SY_INVERSE_PATTERNS,SY_SIMILE,SY_FRONTING,SY_IRRITATION,SY_INTENSIFIER,SY_QUOT,VT_PRESENT_SIMPLE,VT_PRESENT_PROGRESSIVE,VT_PRESENT_PERFECT,VT_PRESENT_PERFECT_PROGR,VT_PRESENT_SIMPLE_PASSIVE,VT_PRESENT_PROGR_PASSIVE,VT_PRESENT_PERFECT_PASSIVE,VT_PAST_SIMPLE,VT_PAST_SIMPLE_BE,VT_PAST_PROGR,VT_PAST_PERFECT,VT_PAST_PERFECT_PROGR,VT_PAST_SIMPLE_PASSIVE,VT_PAST_POGR_PASSIVE,VT_PAST_PERFECT_PASSIVE,VT_FUTURE_SIMPLE,VT_FUTURE_PROGRESSIVE,VT_FUTURE_PERFECT,VT_FUTURE_PERFECT_PROGR,VT_FUTURE_SIMPLE_PASSIVE,VT_FUTURE_PROGR_PASSIVE,VT_FUTURE_PERFECT_PASSIVE,VT_WOULD,VT_WOULD_PASSIVE,VT_WOULD_PROGRESSIVE,VT_WOULD_PERFECT,VT_WOULD_PERFECT_PASSIVE,VT_SHOULD,VT_SHOULD_PASSIVE,VT_SHALL,VT_SHALL_PASSIVE,VT_SHOULD_PROGRESSIVE,VT_SHOULD_PERFECT,VT_SHOULD_PERFECT_PASSIVE,VT_MUST,VT_MUST_PASSIVE,VT_MUST_PROGRESSIVE,VT_MUST_PERFECT,VT_MST_PERFECT_PASSIVE,VT_CAN,VT_CAN_PASSIVE,VT_COULD,VT_COULD_PASSIVE,VT_CAN_PROGRESSIVE,VT_COULD_PROGRESSIVE,VT_COULD_PERFECT,VT_COULD_PERFECT_PASSIVE,VT_MAY,VT_MAY_PASSIVE,VT_MIGHT,VT_MIGHT_PASSIVE,VT_MAY_PROGRESSIVE,VT_MIGTH_PERFECT,VT_MIGHT_PERFECT_PASSIVE,VT_MAY_PERFECT_PASSIVE,ST_TYPE_TOKEN_RATIO_LEMMAS,ST_HERDAN_TTR,ST_MASS_TTR,ST_SENT_WRDSPERSENT,ST_SENT_DIFFERENCE,ST_REPETITIONS_WORDS,ST_REPETITIONS_SENT,ST_SENT_D_VP,ST_SENT_D_NP,ST_SENT_D_PP,ST_SENT_D_ADJP,ST_SENT_D_ADVP,L_I_PRON,L_HE_PRON,L_SHE_PRON,L_IT_PRON,L_YOU_PRON,L_WE_PRON,L_THEY_PRON,L_ME_PRON,L_YOU_OBJ_PRON,L_HIM_PRON,L_HER_OBJECT_PRON,L_IT_OBJECT_PRON,L_US_PRON,L_THEM_PRON,L_MY_PRON,L_YOUR_PRON,L_HIS_PRON,L_HER_PRON,L_ITS_PRON,L_OUR_PRON,L_THEIR_PRON,L_YOURS_PRON,L_THEIRS_PRON,L_HERS_PRON,L_OURS_PRON,L_MYSELF_PRON,L_YOURSELF_PRON,L_HIMSELF_PRON,L_HERSELF_PRON,L_ITSELF_PRON,L_OURSELVES_PRON,L_YOURSELVES_PRON,L_THEMSELVES_PRON,L_FIRST_PERSON_SING_PRON,L_SECOND_PERSON_PRON,L_THIRD_PERSON_SING_PRON,L_THIRD_PERSON_PLURAL_PRON,VF_INFINITIVE,G_PASSIVE,G_ACTIVE,G_PRESENT,G_PAST,G_FUTURE,G_MODALS_SIMPLE,G_MODALS_CONT,G_MODALS_PERFECT,AN,DDP,SVP,CDS,DDF,IS,PS,RE,ASF,ASM,OM,RCI,DMC,OR,QAS,PA,PR
0,"We present a partially reversible U-Net for efficiently generating and segmenting a partially normalized volumetric image based on convolutional neural networks, which has high memory utilization of 256-1,000 per second.",0.2,0.26666666666666666,0.16666666666666666,0.1,0.06666666666666667,0.0,0.03333333333333333,0.0,0.06666666666666667,0.13333333333333333,0.06666666666666667,0.0,0.0,0.0,0.0,0.0,0.6333333333333333,0.36666666666666664,0.6,0.3333333333333333,0.03333333333333333,0.23333333333333334,0.0,0.0,0.6333333333333333,0.1,0.03333333333333333,0.03333333333333333,0.0,0.0,0.0,0.0,0.16666666666666666,0.0,0.0,0.1,0.1,0.1,0.0,0.03333333333333333,0.0,0.0,0.13333333333333333,0.06666666666666667,0.13333333333333333,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06666666666666667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9333333333333333,1.0,0.0,0.9722222222222222,0.0,0.13333333333333333,0.0,0.9444444444444444,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03333333333333333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06666666666666667,0.06666666666666667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03333333333333333,0.0,0.0,0.0,0.0
1,"The network is trained by training some sub-networks on a portion of a large image using a convolutional neural network, and then doing the convolutional loss function with a loss function which includes a Gaussian kernel, a mean-shift kernel, and a softmax kernel which we reverse.",0.15217391304347827,0.32608695652173914,0.15217391304347827,0.021739130434782608,0.21739130434782608,0.0,0.043478260869565216,0.0,0.0,0.08695652173913043,0.06521739130434782,0.0,0.0,0.0,0.0,0.0,0.5434782608695652,0.5,0.41304347826086957,0.32608695652173914,0.021739130434782608,0.30434782608695654,0.0,0.0,0.782608695652174,0.10869565217391304,0.021739130434782608,0.06521739130434782,0.0,0.0,0.0,0.0,0.15217391304347827,0.0,0.0,0.021739130434782608,0.021739130434782608,0.021739130434782608,0.0,0.0,0.021739130434782608,0.0,0.34782608695652173,0.06521739130434782,0.2826086956521739,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.10869565217391304,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.043478260869565216,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6739130434782609,1.0,0.0,0.9814814814814815,0.0,0.4782608695652174,0.0,0.9259259259259259,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.043478260869565216,0.043478260869565216,0.08695652173913043,0.0,0.0,0.0,0.0,0.0,0.021739130434782608,0.0,0.021739130434782608,0.043478260869565216,0.0,0.021739130434782608,0.0,0.0,0.0,0.021739130434782608,0.0,0.0,0.0,0.0,0.043478260869565216,0.0,0.0
2,This result has been obtained for real-world data for real-world images.,0.2727272727272727,0.45454545454545453,0.18181818181818182,0.0,0.09090909090909091,0.0,0.0,0.0,0.0,0.18181818181818182,0.0,0.0,0.0,0.0,0.0,0.0,0.7272727272727273,0.45454545454545453,0.5454545454545454,0.36363636363636365,0.18181818181818182,0.2727272727272727,0.0,0.0,0.9090909090909091,0.2727272727272727,0.09090909090909091,0.0,0.0,0.0,0.0,0.0,0.18181818181818182,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09090909090909091,0.45454545454545453,0.0,0.09090909090909091,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2727272727272727,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9090909090909091,1.0,0.0,0.9375,0.0,0.5454545454545454,0.0,0.8125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2727272727272727,0.0,0.2727272727272727,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
3,"The partial U-net is very efficient despite its complexity: on an ordinary computer it consumes 256-1,000 times less memory than conventional convolutional neural networks.",0.08333333333333333,0.3333333333333333,0.2916666666666667,0.041666666666666664,0.08333333333333333,0.0,0.041666666666666664,0.0,0.08333333333333333,0.125,0.08333333333333333,0.0,0.0,0.0,0.0,0.0,0.5833333333333334,0.4166666666666667,0.5833333333333334,0.4166666666666667,0.08333333333333333,0.25,0.0,0.0,0.875,0.08333333333333333,0.041666666666666664,0.0,0.0,0.041666666666666664,0.0,0.0,0.25,0.08333333333333333,0.0,0.041666666666666664,0.041666666666666664,0.041666666666666664,0.08333333333333333,0.0,0.125,0.0,0.25,0.041666666666666664,0.041666666666666664,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.08333333333333333,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.08333333333333333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.9666666666666667,0.0,0.0,0.0,0.9333333333333333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.041666666666666664,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.041666666666666664,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08333333333333333,0.0,0.0,0.0,0.08333333333333333,0.08333333333333333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.041666666666666664,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08333333333333333,0.0,0.0
4,"We also provided a dataset for validation to test the model on, here is a screen capture of that:",0.15789473684210525,0.2631578947368421,0.0,0.10526315789473684,0.15789473684210525,0.0,0.05263157894736842,0.05263157894736842,0.0,0.15789473684210525,0.10526315789473684,0.0,0.0,0.0,0.0,0.0,0.3684210526315789,0.631578947368421,0.3684210526315789,0.5789473684210527,0.0,0.2631578947368421,0.0,0.0,0.5263157894736842,0.10526315789473684,0.0,0.05263157894736842,0.0,0.05263157894736842,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.10526315789473684,0.10526315789473684,0.0,0.05263157894736842,0.05263157894736842,0.0,1.368421052631579,0.10526315789473684,0.2631578947368421,0.10526315789473684,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.10526315789473684,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9473684210526315,1.0,0.0,0.9523809523809523,0.0,0.10526315789473684,0.0,0.9047619047619048,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.0,0.10526315789473684,0.05263157894736842,0.05263157894736842,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.0,0.0
