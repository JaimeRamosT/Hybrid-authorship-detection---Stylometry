{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92374fcb",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc9a6e",
   "metadata": {},
   "source": [
    "## Librerías utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683c307",
   "metadata": {},
   "source": [
    "## Lectura del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349e9322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet cargado: (465697, 201)\n"
     ]
    }
   ],
   "source": [
    "# Carga de dataframe de parquet\n",
    "df_filtered = pd.read_parquet(\"../../dataset/features_filtered_balanced.parquet\")\n",
    "print(f\"Parquet cargado: {df_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ff8b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de features: 196\n",
      "Primeros 10 features: ['POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP']\n"
     ]
    }
   ],
   "source": [
    "metadata_cols = ['id_original', 'text', 'sentence_num', 'model', 'domain']\n",
    "feature_columns = [col for col in df_filtered.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"\\nTotal de features: {len(feature_columns)}\")\n",
    "print(f\"Primeros 10 features: {feature_columns[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01af3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de documentos únicos: 37977\n"
     ]
    }
   ],
   "source": [
    "# 3. Obtener IDs únicos y crear mapping de ID a clase\n",
    "unique_ids = df_filtered['id_original'].unique()\n",
    "print(f\"\\nTotal de documentos únicos: {len(unique_ids)}\")\n",
    "\n",
    "# Mapping de ID a clase (binaria: humano vs IA)\n",
    "id_to_class = df_filtered.groupby('id_original')['model'].first().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f12a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Usando stratified split (mantiene proporción de clases)\n",
      "\n",
      "Documentos en train: 30381\n",
      "Documentos en test: 7596\n",
      "\n",
      "Documentos en train: 30381\n",
      "Documentos en test: 7596\n"
     ]
    }
   ],
   "source": [
    "# 4. Split de IDs (no de oraciones) - IMPORTANTE para evitar data leakage\n",
    "ids_list = list(id_to_class.keys())\n",
    "labels_list = [id_to_class[id_] for id_ in ids_list]\n",
    "\n",
    "# Verificar si podemos usar stratify\n",
    "class_counts = pd.Series(labels_list).value_counts()\n",
    "can_stratify = class_counts.min() >= 2\n",
    "\n",
    "if can_stratify:\n",
    "    print(\"✓ Usando stratified split (mantiene proporción de clases)\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels_list\n",
    "    )\n",
    "else:\n",
    "    print(f\"⚠️ No se puede usar stratify (clase mínima: {class_counts.min()} documentos)\")\n",
    "    print(\"Usando split aleatorio sin stratify\")\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        ids_list, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=None  # Sin estratificación\n",
    "    )\n",
    "\n",
    "print(f\"\\nDocumentos en train: {len(train_ids)}\")\n",
    "print(f\"Documentos en test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db67388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oraciones en train: 373687\n",
      "Oraciones en test: 92010\n",
      "\n",
      "Distribución en train:\n",
      "model\n",
      "human         130439\n",
      "gpt4           57716\n",
      "chatgpt        55447\n",
      "llama-chat     54041\n",
      "mpt            51677\n",
      "mpt-chat       24367\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución en test:\n",
      "model\n",
      "human         31549\n",
      "gpt4          14601\n",
      "chatgpt       13648\n",
      "llama-chat    13204\n",
      "mpt           13057\n",
      "mpt-chat       5951\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5. Filtrar oraciones según los IDs\n",
    "# train_sentences = train_df[train_df['id'].isin(train_ids)]\n",
    "train_sentences = df_filtered[df_filtered['id_original'].isin(train_ids)]\n",
    "test_sentences = df_filtered[df_filtered['id_original'].isin(test_ids)]\n",
    "\n",
    "print(f\"\\nOraciones en train: {len(train_sentences)}\")\n",
    "print(f\"Oraciones en test: {len(test_sentences)}\")\n",
    "\n",
    "# Verificar distribución en cada conjunto\n",
    "print(f\"\\nDistribución en train:\")\n",
    "# print(train_sentences['is_human'].value_counts())\n",
    "print(train_sentences['model'].value_counts())\n",
    "print(f\"\\nDistribución en test:\")\n",
    "# print(test_sentences['is_human'].value_counts())\n",
    "print(test_sentences['model'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a973d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de features: 196\n",
      "Primeros 10 features: ['POS_VERB', 'POS_NOUN', 'POS_ADJ', 'POS_ADV', 'POS_DET', 'POS_INTJ', 'POS_CONJ', 'POS_PART', 'POS_NUM', 'POS_PREP']\n"
     ]
    }
   ],
   "source": [
    "# 2. Definir columnas de features (excluir metadatos y target)\n",
    "# Excluir: id, sentence_num, model, domain\n",
    "metadata_cols = ['id_original', 'text', 'sentence_num', 'model', 'domain']\n",
    "feature_columns = [col for col in df_filtered.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"\\nTotal de features: {len(feature_columns)}\")\n",
    "print(f\"Primeros 10 features: {feature_columns[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc633eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Shape de X_train: (373687, 196)\n",
      "Shape de y_train: (373687,)\n",
      "Shape de X_test: (92010, 196)\n",
      "Shape de y_test: (92010,)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. Preparar X e y\n",
    "X_train = train_sentences[feature_columns].values\n",
    "y_train = train_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "X_test = test_sentences[feature_columns].values\n",
    "y_test = test_sentences['model'].apply(lambda x: 1 if x == 'human' else 0).values\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Shape de X_train: {X_train.shape}\")\n",
    "print(f\"Shape de y_train: {y_train.shape}\")\n",
    "print(f\"Shape de X_test: {X_test.shape}\")\n",
    "print(f\"Shape de y_test: {y_test.shape}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7d0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando valores NaN en el dataset...\n",
      "\n",
      "NaNs en train_df: 932\n",
      "NaNs en feature_columns:\n",
      "\n",
      "NaNs en train_df: 932\n",
      "NaNs en feature_columns:\n",
      "POS_VERB    1\n",
      "POS_NOUN    1\n",
      "POS_ADJ     1\n",
      "POS_ADV     1\n",
      "POS_DET     1\n",
      "           ..\n",
      "DMC         1\n",
      "OR          1\n",
      "QAS         1\n",
      "PA          1\n",
      "PR          1\n",
      "Length: 196, dtype: int64\n",
      "POS_VERB    1\n",
      "POS_NOUN    1\n",
      "POS_ADJ     1\n",
      "POS_ADV     1\n",
      "POS_DET     1\n",
      "           ..\n",
      "DMC         1\n",
      "OR          1\n",
      "QAS         1\n",
      "PA          1\n",
      "PR          1\n",
      "Length: 196, dtype: int64\n",
      "\n",
      "NaNs en X_train: 932\n",
      "NaNs en X_test: 171\n",
      "NaNs en y_train: 0\n",
      "NaNs en y_test: 0\n",
      "\n",
      "NaNs en X_train: 932\n",
      "NaNs en X_test: 171\n",
      "NaNs en y_train: 0\n",
      "NaNs en y_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay valores NaN en los datos\n",
    "print(\"Verificando valores NaN en el dataset...\")\n",
    "# print(f\"\\nNaNs en train_df: {train_df.isna().sum().sum()}\")\n",
    "print(f\"\\nNaNs en train_df: {train_sentences.isna().sum().sum()}\")\n",
    "print(f\"NaNs en feature_columns:\")\n",
    "nan_features = train_sentences[feature_columns].isna().sum()\n",
    "nan_features_with_nans = nan_features[nan_features > 0]\n",
    "if len(nan_features_with_nans) > 0:\n",
    "    print(nan_features_with_nans)\n",
    "else:\n",
    "    print(\"No hay NaNs en las features ✓\")\n",
    "\n",
    "print(f\"\\nNaNs en X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"NaNs en X_test: {np.isnan(X_test).sum()}\")\n",
    "print(f\"NaNs en y_train: {np.isnan(y_train).sum()}\")\n",
    "print(f\"NaNs en y_test: {np.isnan(y_test).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "866a71df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Valores NaN imputados con la media de cada feature\n",
      "NaNs en X_train_clean: 0\n",
      "NaNs en X_test_clean: 0\n",
      "NaNs en X_train_clean: 0\n",
      "NaNs en X_test_clean: 0\n"
     ]
    }
   ],
   "source": [
    "# Solución: Imputar valores NaN antes de entrenar\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Opción 1: Imputar con la media de cada feature\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Ajustar el imputer con los datos de entrenamiento\n",
    "X_train_clean = imputer.fit_transform(X_train)\n",
    "# Transformar los datos de test con el mismo imputer\n",
    "X_test_clean = imputer.transform(X_test)\n",
    "\n",
    "print(\"✓ Valores NaN imputados con la media de cada feature\")\n",
    "print(f\"NaNs en X_train_clean: {np.isnan(X_train_clean).sum()}\")\n",
    "print(f\"NaNs en X_test_clean: {np.isnan(X_test_clean).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b60dcd",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63c5ef",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9c8b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de componentes para explicar el 95% de la varianza: 19\n"
     ]
    }
   ],
   "source": [
    "pca = PCA().fit(X_train_clean)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Número de componentes para explicar el 95% de la varianza: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1133172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=19)\n",
    "X_train_pca = pca.fit_transform(X_train_clean)\n",
    "X_test_pca = pca.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e4c80",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a94e89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Aplicar LDA para reducir la dimensionalidad\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)  # La cantidad de componentes debe ser <= número de clases - 1\n",
    "x_train_lda = lda.fit_transform(X_train_clean, y_train)\n",
    "x_test_lda = lda.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9d55b",
   "metadata": {},
   "source": [
    "### FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "578a4702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of factors to retain: 1\n"
     ]
    }
   ],
   "source": [
    "fa = FactorAnalysis().fit(X_train_clean)\n",
    "\n",
    "singular_values = fa.components_\n",
    "explained_variance = np.var(singular_values, axis=1) / np.var(X_train_clean, axis=0).sum()\n",
    "\n",
    "# Calcula la varianza explicada acumulativa\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Determina el número de factores necesarios para explicar al menos el 95% de la varianza\n",
    "num_factors = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of factors to retain: {num_factors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00fe8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(n_components=1)\n",
    "x_train_fa = fa.fit_transform(X_train_clean)\n",
    "x_test_fa = fa.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326dc7a",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f73f9545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain: 20\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=min(X_train_clean.shape) - 1)\n",
    "svd.fit(X_train_clean)\n",
    "\n",
    "# Calcula la varianza explicada por los componentes\n",
    "explained_variance = svd.explained_variance_ratio_\n",
    "\n",
    "# Calcula la varianza explicada acumulativa\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Determina el número de componentes necesarios para explicar al menos el 95% de la varianza\n",
    "num_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of components to retain: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cd1b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=20)\n",
    "x_train_svd = svd.fit_transform(X_train_clean)\n",
    "x_test_svd = svd.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145262f",
   "metadata": {},
   "source": [
    "### JL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c514d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain using JL: 316\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.9\n",
    "\n",
    "# Calcula el número mínimo de componentes necesarios usando la Proyección de Johnson-Lindenstrauss\n",
    "n_samples = X_train_clean.shape[0]\n",
    "n_components = johnson_lindenstrauss_min_dim(n_samples, eps=epsilon)\n",
    "print(f\"Number of components to retain using JL: {n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ad4a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\sklearn\\random_projection.py:408: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (196 < 224).The dimensionality of the problem will not be reduced.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jl = SparseRandomProjection(n_components=224)\n",
    "x_train_jl = jl.fit_transform(X_train_clean)\n",
    "x_test_jl = jl.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd450e64",
   "metadata": {},
   "source": [
    "## Guardar datos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0054a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Guardado pca_reduced.npz (train: (373687, 19), test: (92010, 19))\n",
      "✓ Guardado lda_reduced.npz (train: (373687, 1), test: (92010, 1))\n",
      "✓ Guardado lda_reduced.npz (train: (373687, 1), test: (92010, 1))\n",
      "✓ Guardado fa_reduced.npz (train: (373687, 1), test: (92010, 1))\n",
      "✓ Guardado fa_reduced.npz (train: (373687, 1), test: (92010, 1))\n",
      "✓ Guardado svd_reduced.npz (train: (373687, 20), test: (92010, 20))\n",
      "✓ Guardado svd_reduced.npz (train: (373687, 20), test: (92010, 20))\n",
      "✓ Guardado jl_reduced.npz (train: (373687, 224), test: (92010, 224))\n",
      "\n",
      "======================================================================\n",
      "Todos los archivos guardados exitosamente en el directorio actual\n",
      "======================================================================\n",
      "✓ Guardado jl_reduced.npz (train: (373687, 224), test: (92010, 224))\n",
      "\n",
      "======================================================================\n",
      "Todos los archivos guardados exitosamente en el directorio actual\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Crear directorio para los datos transformados si no existe\n",
    "output_dir = \".\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Diccionario con los datos a guardar\n",
    "reduction_methods = {\n",
    "    'pca': (X_train_pca, X_test_pca),\n",
    "    'lda': (x_train_lda, x_test_lda),\n",
    "    'fa': (x_train_fa, x_test_fa),\n",
    "    'svd': (x_train_svd, x_test_svd),\n",
    "    'jl': (x_train_jl, x_test_jl)\n",
    "}\n",
    "\n",
    "# Guardar cada método en archivos NPZ (comprimido)\n",
    "for method_name, (X_train_transformed, X_test_transformed) in reduction_methods.items():\n",
    "    if X_test_transformed is not None:\n",
    "        # Guardar train y test juntos\n",
    "        np.savez_compressed(\n",
    "            os.path.join(output_dir, f'{method_name}_reduced.npz'),\n",
    "            X_train=X_train_transformed,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test_transformed,\n",
    "            y_test=y_test\n",
    "        )\n",
    "        print(f\"✓ Guardado {method_name}_reduced.npz (train: {X_train_transformed.shape}, test: {X_test_transformed.shape})\")\n",
    "    else:\n",
    "        # Solo train (caso t-SNE)\n",
    "        np.savez_compressed(\n",
    "            os.path.join(output_dir, f'{method_name}_reduced.npz'),\n",
    "            X_train=X_train_transformed,\n",
    "            y_train=y_train\n",
    "        )\n",
    "        print(f\"✓ Guardado {method_name}_reduced.npz (solo train: {X_train_transformed.shape})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Todos los archivos guardados exitosamente en el directorio actual\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82390a73",
   "metadata": {},
   "source": [
    "### Ejemplo de carga de datos\n",
    "\n",
    "Para usar estos archivos en otros notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2bb0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cómo cargar los datos en otro archivo:\n",
    "# \n",
    "# import numpy as np\n",
    "# \n",
    "# # Cargar datos de PCA\n",
    "# data = np.load('pca_reduced.npz')\n",
    "# X_train_pca = data['X_train']\n",
    "# y_train = data['y_train']\n",
    "# X_test_pca = data['X_test']\n",
    "# y_test = data['y_test']\n",
    "# \n",
    "# # Para t-SNE (solo tiene train)\n",
    "# data_tsne = np.load('tsne_reduced.npz')\n",
    "# X_train_tsne = data_tsne['X_train']\n",
    "# y_train = data_tsne['y_train']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
