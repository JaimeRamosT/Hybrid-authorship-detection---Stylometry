{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ca329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cvxopt\n",
    "# import seaborn as sns\n",
    "# import soundfile as sf\n",
    "# import itertools\n",
    "# import pywt\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# from IPython.display import Audio\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "# from cvxopt import matrix, solvers\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import scipy\n",
    "# import scipy.signal\n",
    "# from sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.random_projection import SparseRandomProjection, johnson_lindenstrauss_min_dim, GaussianRandomProjection\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed67f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raid import run_detection, run_evaluation\n",
    "from raid.utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efd88a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "566ccaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the RAID dataset without adversarial attacks\n",
    "or_train_noadv_df = load_data(split=\"train\", include_adversarial=False)\n",
    "# test_noadv_df = load_data(split=\"test\", include_adversarial=False)\n",
    "# extra_noadv_df = load_data(split=\"extra\", include_adversarial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fb4873d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Segmenter: Transformer for Semantic Segmentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image segmentation is often ambiguous at the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Mining Contextual Information Beyond Image for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper studies the context aggregation pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  e5e058ce-be2b-459d-af36-32532aaba5ff   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  f95b107b-d176-4af5-90f7-4d0bb20caf93   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  856d8972-9e3d-4544-babc-0fe16f21e04d   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  72c41b8d-0069-4886-b734-a4000ffca286   \n",
       "5  72fe360b-cce6-4daf-b66a-1d778f5964f8  72fe360b-cce6-4daf-b66a-1d778f5964f8   \n",
       "6  df594cf4-9a0c-4488-bcb3-68f41e2d5a16  df594cf4-9a0c-4488-bcb3-68f41e2d5a16   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  human      NaN                NaN   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  human      NaN                NaN   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  human      NaN                NaN   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  human      NaN                NaN   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  human      NaN                NaN   \n",
       "5  72fe360b-cce6-4daf-b66a-1d778f5964f8  human      NaN                NaN   \n",
       "6  df594cf4-9a0c-4488-bcb3-68f41e2d5a16  human      NaN                NaN   \n",
       "\n",
       "  attack     domain                                              title prompt  \\\n",
       "0   none  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...    NaN   \n",
       "1   none  abstracts  EdgeFlow: Achieving Practical Interactive Segm...    NaN   \n",
       "2   none  abstracts  Semi-supervised Contrastive Learning for Label...    NaN   \n",
       "3   none  abstracts  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "4   none  abstracts  Attention-Based 3D Seismic Fault Segmentation ...    NaN   \n",
       "5   none  abstracts   Segmenter: Transformer for Semantic Segmentation    NaN   \n",
       "6   none  abstracts  Mining Contextual Information Beyond Image for...    NaN   \n",
       "\n",
       "                                          generation  \n",
       "0  The recent advancements in artificial intellig...  \n",
       "1  High-quality training data play a key role in ...  \n",
       "2  The success of deep learning methods in medica...  \n",
       "3  Simultaneous segmentation of multiple organs f...  \n",
       "4  Detection faults in seismic data is a crucial ...  \n",
       "5  Image segmentation is often ambiguous at the l...  \n",
       "6  This paper studies the context aggregation pro...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(or_train_noadv_df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9b9a366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Segmenter: Transformer for Semantic Segmentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image segmentation is often ambiguous at the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Mining Contextual Information Beyond Image for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper studies the context aggregation pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model     domain                                              title prompt  \\\n",
       "0  human  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...    NaN   \n",
       "1  human  abstracts  EdgeFlow: Achieving Practical Interactive Segm...    NaN   \n",
       "2  human  abstracts  Semi-supervised Contrastive Learning for Label...    NaN   \n",
       "3  human  abstracts  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "4  human  abstracts  Attention-Based 3D Seismic Fault Segmentation ...    NaN   \n",
       "5  human  abstracts   Segmenter: Transformer for Semantic Segmentation    NaN   \n",
       "6  human  abstracts  Mining Contextual Information Beyond Image for...    NaN   \n",
       "\n",
       "                                          generation  \n",
       "0  The recent advancements in artificial intellig...  \n",
       "1  High-quality training data play a key role in ...  \n",
       "2  The success of deep learning methods in medica...  \n",
       "3  Simultaneous segmentation of multiple organs f...  \n",
       "4  Detection faults in seismic data is a crucial ...  \n",
       "5  Image segmentation is often ambiguous at the l...  \n",
       "6  This paper studies the context aggregation pro...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intCols = ['model', 'domain', 'title', 'prompt', 'generation']\n",
    "# print(\"Visualizar columnas específicas:\")\n",
    "# train_noadv_df = or_train_noadv_df[or_train_noadv_df['model'] != 'human']\n",
    "\n",
    "# Copia del dataframe con columnas específicas\n",
    "train_noadv_df = or_train_noadv_df.copy()\n",
    "train_noadv_df = train_noadv_df[intCols]\n",
    "\n",
    "display(train_noadv_df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d852ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del dataset de entrenamiento:\n",
      "Forma del dataset: (467985, 5)\n",
      "Columnas: ['model', 'domain', 'title', 'prompt', 'generation']\n",
      "Modelos unicos: ['human' 'llama-chat' 'mpt' 'mpt-chat' 'gpt2' 'mistral' 'mistral-chat'\n",
      " 'gpt3' 'cohere' 'chatgpt' 'gpt4' 'cohere-chat']\n",
      "Dominios unicos: ['abstracts' 'books' 'news' 'poetry' 'recipes' 'reddit' 'reviews' 'wiki']\n"
     ]
    }
   ],
   "source": [
    "print(\"Información del dataset de entrenamiento:\")\n",
    "print(f\"Forma del dataset: {train_noadv_df.shape}\")\n",
    "print(f\"Columnas: {list(train_noadv_df.columns)}\")\n",
    "print(f\"Modelos unicos: {train_noadv_df['model'].unique()}\")\n",
    "print(f\"Dominios unicos: {train_noadv_df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06b89d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the paper \"FUTURE-AI: Guiding Principles an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the paper \"Future-AI: Guiding Principles an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we present EdgeFlow, a novel ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we present a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we propose a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we propose a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the field of medical image segmentation, im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model     domain                                              title  \\\n",
       "493  llama-chat  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "494  llama-chat  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "495  llama-chat  abstracts  EdgeFlow: Achieving Practical Interactive Segm...   \n",
       "496  llama-chat  abstracts  EdgeFlow: Achieving Practical Interactive Segm...   \n",
       "497  llama-chat  abstracts  Semi-supervised Contrastive Learning for Label...   \n",
       "498  llama-chat  abstracts  Semi-supervised Contrastive Learning for Label...   \n",
       "499  llama-chat  abstracts  Combo Loss: Handling Input and Output Imbalanc...   \n",
       "\n",
       "                                                prompt  \\\n",
       "493  Write the abstract for the academic paper titl...   \n",
       "494  Write the abstract for the academic paper titl...   \n",
       "495  Write the abstract for the academic paper titl...   \n",
       "496  Write the abstract for the academic paper titl...   \n",
       "497  Write the abstract for the academic paper titl...   \n",
       "498  Write the abstract for the academic paper titl...   \n",
       "499  Write the abstract for the academic paper titl...   \n",
       "\n",
       "                                            generation  \n",
       "493  In the paper \"FUTURE-AI: Guiding Principles an...  \n",
       "494  In the paper \"Future-AI: Guiding Principles an...  \n",
       "495  In this paper, we present EdgeFlow, a novel ap...  \n",
       "496  In this paper, we present a novel approach to ...  \n",
       "497  In this paper, we propose a novel approach to ...  \n",
       "498  In this paper, we propose a novel approach to ...  \n",
       "499  In the field of medical image segmentation, im...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# intCols = ['model', 'domain', 'title', 'prompt', 'generation']\n",
    "# print(\"Visualizar columnas específicas:\")\n",
    "filtered = train_noadv_df[train_noadv_df['model'] != 'human']\n",
    "\n",
    "display(filtered.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b12f1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>High-quality training data play a key role in image segmentation tasks.\\nUsually, pixel-level annotations are expensive, laborious and time-consuming\\nfor the large volume of training data. To reduce labelling cost and improve\\nsegmentation quality, interactive segmentation methods have been proposed,\\nwhich provide the result with just a few clicks. However, their performance\\ndoes not meet the requirements of practical segmentation tasks in terms of\\nspeed and accuracy. In this work, we propose EdgeFlow, a novel architecture\\nthat fully utilizes interactive information of user clicks with edge-guided\\nflow. Our method achieves state-of-the-art performance without any\\npost-processing or iterative optimization scheme. Comprehensive experiments on\\nbenchmarks also demonstrate the superiority of our method. In addition, with\\nthe proposed method, we develop an efficient interactive segmentation tool for\\npractical data annotation tasks. The source code and tool is avaliable at\\nhttps://github.com/PaddlePaddle/PaddleSeg.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>The success of deep learning methods in medical image segmentation tasks\\nheavily depends on a large amount of labeled data to supervise the training. On\\nthe other hand, the annotation of biomedical images requires domain knowledge\\nand can be laborious. Recently, contrastive learning has demonstrated great\\npotential in learning latent representation of images even without any label.\\nExisting works have explored its application to biomedical image segmentation\\nwhere only a small portion of data is labeled, through a pre-training phase\\nbased on self-supervised contrastive learning without using any labels followed\\nby a supervised fine-tuning phase on the labeled portion of data only. In this\\npaper, we establish that by including the limited label in formation in the\\npre-training phase, it is possible to boost the performance of contrastive\\nlearning. We propose a supervised local contrastive loss that leverages limited\\npixel-wise annotation to force pixels with the same label to gather around in\\nthe embedding space. Such loss needs pixel-wise computation which can be\\nexpensive for large images, and we further propose two strategies, downsampling\\nand block division, to address the issue. We evaluate our methods on two public\\nbiomedical image datasets of different modalities. With different amounts of\\nlabeled data, our methods consistently outperform the state-of-the-art\\ncontrast-based methods and other semi-supervised learning techniques.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model     domain  \\\n",
       "0  human  abstracts   \n",
       "1  human  abstracts   \n",
       "2  human  abstracts   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        generation  \n",
       "0  The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       High-quality training data play a key role in image segmentation tasks.\\nUsually, pixel-level annotations are expensive, laborious and time-consuming\\nfor the large volume of training data. To reduce labelling cost and improve\\nsegmentation quality, interactive segmentation methods have been proposed,\\nwhich provide the result with just a few clicks. However, their performance\\ndoes not meet the requirements of practical segmentation tasks in terms of\\nspeed and accuracy. In this work, we propose EdgeFlow, a novel architecture\\nthat fully utilizes interactive information of user clicks with edge-guided\\nflow. Our method achieves state-of-the-art performance without any\\npost-processing or iterative optimization scheme. Comprehensive experiments on\\nbenchmarks also demonstrate the superiority of our method. In addition, with\\nthe proposed method, we develop an efficient interactive segmentation tool for\\npractical data annotation tasks. The source code and tool is avaliable at\\nhttps://github.com/PaddlePaddle/PaddleSeg.  \n",
       "2                                              The success of deep learning methods in medical image segmentation tasks\\nheavily depends on a large amount of labeled data to supervise the training. On\\nthe other hand, the annotation of biomedical images requires domain knowledge\\nand can be laborious. Recently, contrastive learning has demonstrated great\\npotential in learning latent representation of images even without any label.\\nExisting works have explored its application to biomedical image segmentation\\nwhere only a small portion of data is labeled, through a pre-training phase\\nbased on self-supervised contrastive learning without using any labels followed\\nby a supervised fine-tuning phase on the labeled portion of data only. In this\\npaper, we establish that by including the limited label in formation in the\\npre-training phase, it is possible to boost the performance of contrastive\\nlearning. We propose a supervised local contrastive loss that leverages limited\\npixel-wise annotation to force pixels with the same label to gather around in\\nthe embedding space. Such loss needs pixel-wise computation which can be\\nexpensive for large images, and we further propose two strategies, downsampling\\nand block division, to address the issue. We evaluate our methods on two public\\nbiomedical image datasets of different modalities. With different amounts of\\nlabeled data, our methods consistently outperform the state-of-the-art\\ncontrast-based methods and other semi-supervised learning techniques.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Opción 1: Configurar pandas para mostrar más contenido\n",
    "pd.set_option('display.max_colwidth', None)  # Sin límite de ancho\n",
    "pd.set_option('display.max_rows', None)      # Sin límite de filas (usar con cuidado)\n",
    "\n",
    "complex_filter = train_noadv_df[(train_noadv_df['model'] == 'human') &\n",
    "                                (train_noadv_df['domain'] == 'abstracts')]\n",
    "\n",
    "# Ahora al mostrar el dataframe, verás el contenido completo\n",
    "display(complex_filter[['model', 'domain', 'generation']].head(3))\n",
    "\n",
    "# Si quieres restaurar los valores por defecto de pandas:\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda8357",
   "metadata": {},
   "source": [
    "## Uso de Stylometrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54b45cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processer import split_text_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dedb42b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44168</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Generative Adversarial Networks (GANs) have sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59827</th>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>This academic paper explores the use of deep l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193411</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>poetry</td>\n",
       "      <td>The heavens weep and mourn today,\\nAs sorrow p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24380</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>This paper presents a novel approach to image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289537</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Ingredients:\\n\\n- 1 can (16 ounces) refried be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model     domain  \\\n",
       "44168      chatgpt  abstracts   \n",
       "59827     mpt-chat  abstracts   \n",
       "193411  llama-chat     poetry   \n",
       "24380   llama-chat  abstracts   \n",
       "289537        gpt4    recipes   \n",
       "\n",
       "                                               generation  \n",
       "44168   Generative Adversarial Networks (GANs) have sh...  \n",
       "59827   This academic paper explores the use of deep l...  \n",
       "193411  The heavens weep and mourn today,\\nAs sorrow p...  \n",
       "24380   This paper presents a novel approach to image ...  \n",
       "289537  Ingredients:\\n\\n- 1 can (16 ounces) refried be...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get sample dataframe 'generation'\n",
    "generation_sample = filtered[['model', 'domain', 'generation']].sample(n=5, random_state=42)\n",
    "display(generation_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a61eb252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Adversarial Networks (GANs) have shown great potential in generating realistic and high-quality samples. However, training GANs from incomplete observations, where only partial information is available, remains a challenging task. In this paper, we propose a novel approach for training GANs from incomplete observations using factorised discriminators. Our approach leverages the idea of factorising the discriminator into multiple sub-discriminators, each responsible for a specific aspect of the data. We introduce a new training algorithm that alternates between training the generator and updating the sub-discriminators. Experimental results on various datasets demonstrate the effectiveness of our approach in generating high-quality samples from incomplete observations. Our approach outperforms existing methods in terms of sample quality and diversity, and shows robustness to different levels of incompleteness in the observations. Overall, our work contributes to the advancement of GANs in handling incomplete observations, opening up new possibilities for applications in various domains.\n"
     ]
    }
   ],
   "source": [
    "print(generation_sample['generation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1467c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Total de oraciones: 8\n"
     ]
    }
   ],
   "source": [
    "outputList = split_text_into_sentences(generation_sample['generation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86531196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Adversarial Networks (GANs) have shown great potential in generating realistic and high-quality samples.\n",
      "-----\n",
      "However, training GANs from incomplete observations, where only partial information is available, remains a challenging task.\n",
      "-----\n",
      "In this paper, we propose a novel approach for training GANs from incomplete observations using factorised discriminators.\n",
      "-----\n",
      "Our approach leverages the idea of factorising the discriminator into multiple sub-discriminators, each responsible for a specific aspect of the data.\n",
      "-----\n",
      "We introduce a new training algorithm that alternates between training the generator and updating the sub-discriminators.\n",
      "-----\n",
      "Experimental results on various datasets demonstrate the effectiveness of our approach in generating high-quality samples from incomplete observations.\n",
      "-----\n",
      "Our approach outperforms existing methods in terms of sample quality and diversity, and shows robustness to different levels of incompleteness in the observations.\n",
      "-----\n",
      "Overall, our work contributes to the advancement of GANs in handling incomplete observations, opening up new possibilities for applications in various domains.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for sentence in outputList:\n",
    "    print(sentence)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19cd26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stylo_metrix as sm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
