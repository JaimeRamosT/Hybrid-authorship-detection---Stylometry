{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8ca3d4",
   "metadata": {},
   "source": [
    "# Tests extracción de rasgos estilométricos con RAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ca329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cvxopt\n",
    "# import seaborn as sns\n",
    "# import soundfile as sf\n",
    "# import itertools\n",
    "# import pywt\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# from IPython.display import Audio\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "# from cvxopt import matrix, solvers\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import scipy\n",
    "# import scipy.signal\n",
    "# from sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.random_projection import SparseRandomProjection, johnson_lindenstrauss_min_dim, GaussianRandomProjection\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed67f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raid import run_detection, run_evaluation\n",
    "from raid.utils import load_data\n",
    "import stylo_metrix as sm\n",
    "import pandas as pd\n",
    "\n",
    "from processer import split_text_into_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8500ce",
   "metadata": {},
   "source": [
    "## Manejo de RAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "566ccaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the RAID dataset without adversarial attacks\n",
    "or_train_noadv_df = load_data(split=\"train\", include_adversarial=False)\n",
    "# test_noadv_df = load_data(split=\"test\", include_adversarial=False)\n",
    "# extra_noadv_df = load_data(split=\"extra\", include_adversarial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fb4873d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Segmenter: Transformer for Semantic Segmentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image segmentation is often ambiguous at the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Mining Contextual Information Beyond Image for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper studies the context aggregation pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  e5e058ce-be2b-459d-af36-32532aaba5ff   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  f95b107b-d176-4af5-90f7-4d0bb20caf93   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  856d8972-9e3d-4544-babc-0fe16f21e04d   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  72c41b8d-0069-4886-b734-a4000ffca286   \n",
       "5  72fe360b-cce6-4daf-b66a-1d778f5964f8  72fe360b-cce6-4daf-b66a-1d778f5964f8   \n",
       "6  df594cf4-9a0c-4488-bcb3-68f41e2d5a16  df594cf4-9a0c-4488-bcb3-68f41e2d5a16   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  human      NaN                NaN   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  human      NaN                NaN   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  human      NaN                NaN   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  human      NaN                NaN   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  human      NaN                NaN   \n",
       "5  72fe360b-cce6-4daf-b66a-1d778f5964f8  human      NaN                NaN   \n",
       "6  df594cf4-9a0c-4488-bcb3-68f41e2d5a16  human      NaN                NaN   \n",
       "\n",
       "  attack     domain                                              title prompt  \\\n",
       "0   none  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...    NaN   \n",
       "1   none  abstracts  EdgeFlow: Achieving Practical Interactive Segm...    NaN   \n",
       "2   none  abstracts  Semi-supervised Contrastive Learning for Label...    NaN   \n",
       "3   none  abstracts  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "4   none  abstracts  Attention-Based 3D Seismic Fault Segmentation ...    NaN   \n",
       "5   none  abstracts   Segmenter: Transformer for Semantic Segmentation    NaN   \n",
       "6   none  abstracts  Mining Contextual Information Beyond Image for...    NaN   \n",
       "\n",
       "                                          generation  \n",
       "0  The recent advancements in artificial intellig...  \n",
       "1  High-quality training data play a key role in ...  \n",
       "2  The success of deep learning methods in medica...  \n",
       "3  Simultaneous segmentation of multiple organs f...  \n",
       "4  Detection faults in seismic data is a crucial ...  \n",
       "5  Image segmentation is often ambiguous at the l...  \n",
       "6  This paper studies the context aggregation pro...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(or_train_noadv_df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9b9a366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Segmenter: Transformer for Semantic Segmentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image segmentation is often ambiguous at the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Mining Contextual Information Beyond Image for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper studies the context aggregation pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model     domain                                              title prompt  \\\n",
       "0  human  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...    NaN   \n",
       "1  human  abstracts  EdgeFlow: Achieving Practical Interactive Segm...    NaN   \n",
       "2  human  abstracts  Semi-supervised Contrastive Learning for Label...    NaN   \n",
       "3  human  abstracts  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "4  human  abstracts  Attention-Based 3D Seismic Fault Segmentation ...    NaN   \n",
       "5  human  abstracts   Segmenter: Transformer for Semantic Segmentation    NaN   \n",
       "6  human  abstracts  Mining Contextual Information Beyond Image for...    NaN   \n",
       "\n",
       "                                          generation  \n",
       "0  The recent advancements in artificial intellig...  \n",
       "1  High-quality training data play a key role in ...  \n",
       "2  The success of deep learning methods in medica...  \n",
       "3  Simultaneous segmentation of multiple organs f...  \n",
       "4  Detection faults in seismic data is a crucial ...  \n",
       "5  Image segmentation is often ambiguous at the l...  \n",
       "6  This paper studies the context aggregation pro...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intCols = ['model', 'domain', 'title', 'prompt', 'generation']\n",
    "# print(\"Visualizar columnas específicas:\")\n",
    "# train_noadv_df = or_train_noadv_df[or_train_noadv_df['model'] != 'human']\n",
    "\n",
    "# Copia del dataframe con columnas específicas\n",
    "train_noadv_df = or_train_noadv_df.copy()\n",
    "train_noadv_df = train_noadv_df[intCols]\n",
    "\n",
    "display(train_noadv_df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d852ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del dataset de entrenamiento:\n",
      "Forma del dataset: (467985, 5)\n",
      "Columnas: ['model', 'domain', 'title', 'prompt', 'generation']\n",
      "Modelos unicos: ['human' 'llama-chat' 'mpt' 'mpt-chat' 'gpt2' 'mistral' 'mistral-chat'\n",
      " 'gpt3' 'cohere' 'chatgpt' 'gpt4' 'cohere-chat']\n",
      "Dominios unicos: ['abstracts' 'books' 'news' 'poetry' 'recipes' 'reddit' 'reviews' 'wiki']\n"
     ]
    }
   ],
   "source": [
    "print(\"Información del dataset de entrenamiento:\")\n",
    "print(f\"Forma del dataset: {train_noadv_df.shape}\")\n",
    "print(f\"Columnas: {list(train_noadv_df.columns)}\")\n",
    "print(f\"Modelos unicos: {train_noadv_df['model'].unique()}\")\n",
    "print(f\"Dominios unicos: {train_noadv_df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06b89d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the paper \"FUTURE-AI: Guiding Principles an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the paper \"Future-AI: Guiding Principles an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we present EdgeFlow, a novel ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we present a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we propose a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we propose a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the field of medical image segmentation, im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model     domain                                              title  \\\n",
       "493  llama-chat  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "494  llama-chat  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "495  llama-chat  abstracts  EdgeFlow: Achieving Practical Interactive Segm...   \n",
       "496  llama-chat  abstracts  EdgeFlow: Achieving Practical Interactive Segm...   \n",
       "497  llama-chat  abstracts  Semi-supervised Contrastive Learning for Label...   \n",
       "498  llama-chat  abstracts  Semi-supervised Contrastive Learning for Label...   \n",
       "499  llama-chat  abstracts  Combo Loss: Handling Input and Output Imbalanc...   \n",
       "\n",
       "                                                prompt  \\\n",
       "493  Write the abstract for the academic paper titl...   \n",
       "494  Write the abstract for the academic paper titl...   \n",
       "495  Write the abstract for the academic paper titl...   \n",
       "496  Write the abstract for the academic paper titl...   \n",
       "497  Write the abstract for the academic paper titl...   \n",
       "498  Write the abstract for the academic paper titl...   \n",
       "499  Write the abstract for the academic paper titl...   \n",
       "\n",
       "                                            generation  \n",
       "493  In the paper \"FUTURE-AI: Guiding Principles an...  \n",
       "494  In the paper \"Future-AI: Guiding Principles an...  \n",
       "495  In this paper, we present EdgeFlow, a novel ap...  \n",
       "496  In this paper, we present a novel approach to ...  \n",
       "497  In this paper, we propose a novel approach to ...  \n",
       "498  In this paper, we propose a novel approach to ...  \n",
       "499  In the field of medical image segmentation, im...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# intCols = ['model', 'domain', 'title', 'prompt', 'generation']\n",
    "# print(\"Visualizar columnas específicas:\")\n",
    "filtered = train_noadv_df[train_noadv_df['model'] != 'human']\n",
    "\n",
    "display(filtered.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b12f1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>High-quality training data play a key role in image segmentation tasks.\\nUsually, pixel-level annotations are expensive, laborious and time-consuming\\nfor the large volume of training data. To reduce labelling cost and improve\\nsegmentation quality, interactive segmentation methods have been proposed,\\nwhich provide the result with just a few clicks. However, their performance\\ndoes not meet the requirements of practical segmentation tasks in terms of\\nspeed and accuracy. In this work, we propose EdgeFlow, a novel architecture\\nthat fully utilizes interactive information of user clicks with edge-guided\\nflow. Our method achieves state-of-the-art performance without any\\npost-processing or iterative optimization scheme. Comprehensive experiments on\\nbenchmarks also demonstrate the superiority of our method. In addition, with\\nthe proposed method, we develop an efficient interactive segmentation tool for\\npractical data annotation tasks. The source code and tool is avaliable at\\nhttps://github.com/PaddlePaddle/PaddleSeg.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>The success of deep learning methods in medical image segmentation tasks\\nheavily depends on a large amount of labeled data to supervise the training. On\\nthe other hand, the annotation of biomedical images requires domain knowledge\\nand can be laborious. Recently, contrastive learning has demonstrated great\\npotential in learning latent representation of images even without any label.\\nExisting works have explored its application to biomedical image segmentation\\nwhere only a small portion of data is labeled, through a pre-training phase\\nbased on self-supervised contrastive learning without using any labels followed\\nby a supervised fine-tuning phase on the labeled portion of data only. In this\\npaper, we establish that by including the limited label in formation in the\\npre-training phase, it is possible to boost the performance of contrastive\\nlearning. We propose a supervised local contrastive loss that leverages limited\\npixel-wise annotation to force pixels with the same label to gather around in\\nthe embedding space. Such loss needs pixel-wise computation which can be\\nexpensive for large images, and we further propose two strategies, downsampling\\nand block division, to address the issue. We evaluate our methods on two public\\nbiomedical image datasets of different modalities. With different amounts of\\nlabeled data, our methods consistently outperform the state-of-the-art\\ncontrast-based methods and other semi-supervised learning techniques.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model     domain  \\\n",
       "0  human  abstracts   \n",
       "1  human  abstracts   \n",
       "2  human  abstracts   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        generation  \n",
       "0  The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       High-quality training data play a key role in image segmentation tasks.\\nUsually, pixel-level annotations are expensive, laborious and time-consuming\\nfor the large volume of training data. To reduce labelling cost and improve\\nsegmentation quality, interactive segmentation methods have been proposed,\\nwhich provide the result with just a few clicks. However, their performance\\ndoes not meet the requirements of practical segmentation tasks in terms of\\nspeed and accuracy. In this work, we propose EdgeFlow, a novel architecture\\nthat fully utilizes interactive information of user clicks with edge-guided\\nflow. Our method achieves state-of-the-art performance without any\\npost-processing or iterative optimization scheme. Comprehensive experiments on\\nbenchmarks also demonstrate the superiority of our method. In addition, with\\nthe proposed method, we develop an efficient interactive segmentation tool for\\npractical data annotation tasks. The source code and tool is avaliable at\\nhttps://github.com/PaddlePaddle/PaddleSeg.  \n",
       "2                                              The success of deep learning methods in medical image segmentation tasks\\nheavily depends on a large amount of labeled data to supervise the training. On\\nthe other hand, the annotation of biomedical images requires domain knowledge\\nand can be laborious. Recently, contrastive learning has demonstrated great\\npotential in learning latent representation of images even without any label.\\nExisting works have explored its application to biomedical image segmentation\\nwhere only a small portion of data is labeled, through a pre-training phase\\nbased on self-supervised contrastive learning without using any labels followed\\nby a supervised fine-tuning phase on the labeled portion of data only. In this\\npaper, we establish that by including the limited label in formation in the\\npre-training phase, it is possible to boost the performance of contrastive\\nlearning. We propose a supervised local contrastive loss that leverages limited\\npixel-wise annotation to force pixels with the same label to gather around in\\nthe embedding space. Such loss needs pixel-wise computation which can be\\nexpensive for large images, and we further propose two strategies, downsampling\\nand block division, to address the issue. We evaluate our methods on two public\\nbiomedical image datasets of different modalities. With different amounts of\\nlabeled data, our methods consistently outperform the state-of-the-art\\ncontrast-based methods and other semi-supervised learning techniques.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Opción 1: Configurar pandas para mostrar más contenido\n",
    "pd.set_option('display.max_colwidth', None)  # Sin límite de ancho\n",
    "pd.set_option('display.max_rows', None)      # Sin límite de filas (usar con cuidado)\n",
    "\n",
    "complex_filter = train_noadv_df[(train_noadv_df['model'] == 'human') &\n",
    "                                (train_noadv_df['domain'] == 'abstracts')]\n",
    "\n",
    "# Ahora al mostrar el dataframe, verás el contenido completo\n",
    "display(complex_filter[['model', 'domain', 'generation']].head(3))\n",
    "\n",
    "# Si quieres restaurar los valores por defecto de pandas:\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda8357",
   "metadata": {},
   "source": [
    "## Uso de Stylometrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd2b93",
   "metadata": {},
   "source": [
    "### Test de extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dedb42b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407409</th>\n",
       "      <td>mpt</td>\n",
       "      <td>wiki</td>\n",
       "      <td>She became a Christian at 12 years old, under ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79972</th>\n",
       "      <td>mistral</td>\n",
       "      <td>books</td>\n",
       "      <td>Two teenage misfits meet at Camp Crystal Lake,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240222</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>poetry</td>\n",
       "      <td>In the quiet corners of the heart, where love'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55767</th>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>The paper proposes a novel approach for visual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292999</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Ingredients:\\n- 1 small watermelon\\n- 1 small ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model     domain                                         generation\n",
       "407409       mpt       wiki  She became a Christian at 12 years old, under ...\n",
       "79972    mistral      books  Two teenage misfits meet at Camp Crystal Lake,...\n",
       "240222      gpt4     poetry  In the quiet corners of the heart, where love'...\n",
       "55767   mpt-chat  abstracts  The paper proposes a novel approach for visual...\n",
       "292999   chatgpt    recipes  Ingredients:\\n- 1 small watermelon\\n- 1 small ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get sample dataframe 'generation'\n",
    "generation_sample = filtered[['model', 'domain', 'generation']].sample(n=5, random_state=36)\n",
    "display(generation_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13c6b7",
   "metadata": {},
   "source": [
    "#### Extracción de rasgos en un registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1467c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She became a Christian at 12 years old, under John Sargent's ministry; attended schools for girls in Philadelphia and Newburyport Massachusetts (graduating near or before age 13); was one year engaged to Henry Ward Beecher who introduced her by correspondence with Harriet Martineau.[2] After this disappointment she studied music until 1851 when Lyman Abbott called on James Miller McKim as he took trips through upstate towns preaching that it might bring them together again[citation needed], which did indeed occur within ten days but then continued their search after marriage,[1][3]. During Sarah Abbot McClellan lived outside Boston among various people helping slaves escape via Underground railroad into Canada while being pursued herself since 1850-1854 due because abolitionist views held deep inside heart made dangerous task taking risks during those troubled times especially considering fact slave status only legally differentiates person whom lives born America white color considered property master same time other Americans free according law whether own legalities involved not matter point here however note should be mention too even today still controversial why some may call racist others simply believe black man woman less equals equal whites human rights deserve protect just society well course there are always extremists make exceptions cause harm any case know best how feel answer truth either stand behind whatever decides long end agree disagree makes most sense moral ethics personal beliefs values based individual situations life experiences past present future take example civil war came close breaking states United apart slavery seen many sides issue including north south blacks both races had strong arguments regarding topic nevertheless cannot say easy simple good bad wrong right side question depends where stands opinions mind rest important realize each comes uniquely formed reason don't judge fellow brother sister think doesn t worth loving treat like dog rather than friend child family member we're God created us all so deeply respect love unconditionedly without doubt judgment condemnation understanding open mindedness clear seeing wisdom patience kindness humility selflessness genuine care compassion willingness serve humbly humble willingly share everything possess show no favoritism prejudice hatred prideful selfish ambition covetous desire want receive give graciously truly listen understand someone else words meant spirit expressed thoughtfully seriously come help hurt mend broken hearts heal wounds find courage strength comfort solace joy peace hope eternal deliverance glory salvation kingdom heaven earth power grace mercy forgiveness everlasting promise Lord Jesus Christ Savior King reign forever amen hallelujah praise god yahweh elohiym iYHVH jesus christ son redeemer redeemed redemption\n",
      "[OK] Total de oraciones: 2\n"
     ]
    }
   ],
   "source": [
    "print(generation_sample['generation'].iloc[0])\n",
    "outputList = split_text_into_sentences(generation_sample['generation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86531196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She became a Christian at 12 years old, under John Sargent's ministry; attended schools for girls in Philadelphia and Newburyport Massachusetts (graduating near or before age 13); was one year engaged to Henry Ward Beecher who introduced her by correspondence with Harriet Martineau.[2] After this disappointment she studied music until 1851 when Lyman Abbott called on James Miller McKim as he took trips through upstate towns preaching that it might bring them together again[citation needed], which did indeed occur within ten days but then continued their search after marriage,[1][3].\n",
      "-----\n",
      "During Sarah Abbot McClellan lived outside Boston among various people helping slaves escape via Underground railroad into Canada while being pursued herself since 1850-1854 due because abolitionist views held deep inside heart made dangerous task taking risks during those troubled times especially considering fact slave status only legally differentiates person whom lives born America white color considered property master same time other Americans free according law whether own legalities involved not matter point here however note should be mention too even today still controversial why some may call racist others simply believe black man woman less equals equal whites human rights deserve protect just society well course there are always extremists make exceptions cause harm any case know best how feel answer truth either stand behind whatever decides long end agree disagree makes most sense moral ethics personal beliefs values based individual situations life experiences past present future take example civil war came close breaking states United apart slavery seen many sides issue including north south blacks both races had strong arguments regarding topic nevertheless cannot say easy simple good bad wrong right side question depends where stands opinions mind rest important realize each comes uniquely formed reason don't judge fellow brother sister think doesn t worth loving treat like dog rather than friend child family member we're God created us all so deeply respect love unconditionedly without doubt judgment condemnation understanding open mindedness clear seeing wisdom patience kindness humility selflessness genuine care compassion willingness serve humbly humble willingly share everything possess show no favoritism prejudice hatred prideful selfish ambition covetous desire want receive give graciously truly listen understand someone else words meant spirit expressed thoughtfully seriously come help hurt mend broken hearts heal wounds find courage strength comfort solace joy peace hope eternal deliverance glory salvation kingdom heaven earth power grace mercy forgiveness everlasting promise Lord Jesus Christ Savior King reign forever amen hallelujah praise god yahweh elohiym iYHVH jesus christ son redeemer redeemed redemption\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for sentence in outputList:\n",
    "    print(sentence)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d75fe6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\single\\sm_output1.csv\n",
      "File saved in location: testVisual\\single\\sm_debug1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                text  POS_VERB  POS_NOUN  \\\n",
       " 0  She became a Christian at 12 years old, under ...  0.168539  0.146067   \n",
       " 1  During Sarah Abbot McClellan lived outside Bos...  0.239264  0.343558   \n",
       " \n",
       "     POS_ADJ   POS_ADV   POS_DET  POS_INTJ  POS_CONJ  POS_PART   POS_NUM  ...  \\\n",
       " 0  0.033708  0.033708  0.022472  0.000000  0.067416  0.011236  0.056180  ...   \n",
       " 1  0.125767  0.092025  0.012270  0.009202  0.024540  0.009202  0.006135  ...   \n",
       " \n",
       "     RE       ASF       ASM        OM  RCI       DMC        OR       QAS   PA  \\\n",
       " 0  0.0  0.000000  0.011236  0.000000  0.0  0.000000  0.000000  0.000000  0.0   \n",
       " 1  0.0  0.003067  0.018405  0.006135  0.0  0.015337  0.003067  0.027607  0.0   \n",
       " \n",
       "          PR  \n",
       " 0  0.011236  \n",
       " 1  0.000000  \n",
       " \n",
       " [2 rows x 197 columns],\n",
       "                                                 text  \\\n",
       " 0  She became a Christian at 12 years old, under ...   \n",
       " 1  During Sarah Abbot McClellan lived outside Bos...   \n",
       " \n",
       "                                             POS_VERB  \\\n",
       " 0  [became, attended, graduating, was, introduced...   \n",
       " 1  [lived, helping, escape, being, pursued, held,...   \n",
       " \n",
       "                                             POS_NOUN  \\\n",
       " 0  [years, ministry, schools, girls, age, year, c...   \n",
       " 1  [people, slaves, railroad, views, heart, task,...   \n",
       " \n",
       "                                              POS_ADJ  \\\n",
       " 0                            [old, engaged, upstate]   \n",
       " 1  [various, due, abolitionist, dangerous, white,...   \n",
       " \n",
       "                                              POS_ADV                 POS_DET  \\\n",
       " 0                           [together, indeed, then]               [a, this]   \n",
       " 1  [deep, especially, only, legally, here, howeve...  [those, any, both, no]   \n",
       " \n",
       "                    POS_INTJ  \\\n",
       " 0                        []   \n",
       " 1  [well, amen, hallelujah]   \n",
       " \n",
       "                                             POS_CONJ         POS_PART  \\\n",
       " 0                     [and, or, when, as, that, but]             ['s]   \n",
       " 1  [while, since, because, whether, why, how, eit...  [not, not, n't]   \n",
       " \n",
       "                     POS_NUM  ...  RE       ASF  \\\n",
       " 0  [12, 13, one, 1851, ten]  ...  []        []   \n",
       " 1              [1850, 1854]  ...  []  [simple]   \n",
       " \n",
       "                                         ASM             OM RCI  \\\n",
       " 0                                     [did]             []  []   \n",
       " 1  [know, simple, do, loving, member, love]  [behind, can]  []   \n",
       " \n",
       "                                 DMC        OR  \\\n",
       " 0                                []        []   \n",
       " 1  [black, based, blacks, bad, dog]  [simple]   \n",
       " \n",
       "                                                  QAS  PA      PR  \n",
       " 0                                                 []  []  [John]  \n",
       " 1  [railroad, made, white, whites, make, makes, d...  []      []  \n",
       " \n",
       " [2 rows x 197 columns])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stylo = sm.StyloMetrix('en', debug=True, save_path=\"testVisual/single\") # define langauge, one of ('de','en', 'pl', 'ru', 'ukr')\n",
    "metrics = stylo.transform(outputList) # list of texts\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19670c0d",
   "metadata": {},
   "source": [
    "#### Extracción de rasgos en múltiples registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86bcfd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_trf' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 407409 (iteration 0)\n",
      "[OK] Total de oraciones: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter0_idx4074091.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter0_idx4074091.csv\n",
      "Saved: output_iter0_idx407409.csv and debug_iter0_idx407409.csv\n",
      "Processing index: 79972 (iteration 1)\n",
      "[OK] Total de oraciones: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.13it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter1_idx799721.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter1_idx799721.csv\n",
      "Saved: output_iter1_idx79972.csv and debug_iter1_idx79972.csv\n",
      "Processing index: 240222 (iteration 2)\n",
      "[OK] Total de oraciones: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter2_idx2402221.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter2_idx2402221.csv\n",
      "Saved: output_iter2_idx240222.csv and debug_iter2_idx240222.csv\n",
      "Processing index: 55767 (iteration 3)\n",
      "[OK] Total de oraciones: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.34it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter3_idx557671.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter3_idx557671.csv\n",
      "Saved: output_iter3_idx55767.csv and debug_iter3_idx55767.csv\n",
      "Processing index: 292999 (iteration 4)\n",
      "[OK] Total de oraciones: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:02<00:00,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter4_idx2929991.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter4_idx2929991.csv\n",
      "Saved: output_iter4_idx292999.csv and debug_iter4_idx292999.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stylo = sm.StyloMetrix('en', debug=True, save_path=\"testVisual/multiple\")\n",
    "for idx, i in enumerate(generation_sample.index):\n",
    "    print(f\"Processing index: {i} (iteration {idx})\")\n",
    "    text = generation_sample['generation'].loc[i]\n",
    "    sentences = split_text_into_sentences(text)\n",
    "    \n",
    "    # Cambiar los nombres de los archivos para cada iteración\n",
    "    stylo.output_name = f\"output_iter{idx}_idx{i}\"\n",
    "    stylo.debug_name = f\"debug_iter{idx}_idx{i}\"\n",
    "    features = stylo.transform(sentences)\n",
    "    print(f\"Saved: {stylo.output_name}.csv and {stylo.debug_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d2444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
