{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8ca3d4",
   "metadata": {},
   "source": [
    "# Tests extracción de rasgos estilométricos con RAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ca329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cvxopt\n",
    "# import seaborn as sns\n",
    "# import soundfile as sf\n",
    "# import itertools\n",
    "# import pywt\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# from IPython.display import Audio\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "# from cvxopt import matrix, solvers\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import scipy\n",
    "# import scipy.signal\n",
    "# from sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.random_projection import SparseRandomProjection, johnson_lindenstrauss_min_dim, GaussianRandomProjection\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed67f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stylo_metrix as sm\n",
    "import pandas as pd\n",
    "from raid import run_detection, run_evaluation\n",
    "from raid.utils import load_data\n",
    "\n",
    "from processer import split_text_into_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8500ce",
   "metadata": {},
   "source": [
    "## Manejo de RAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "566ccaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the RAID dataset without adversarial attacks\n",
    "or_train_noadv_df = load_data(split=\"train\", include_adversarial=False)\n",
    "# test_noadv_df = load_data(split=\"test\", include_adversarial=False)\n",
    "# extra_noadv_df = load_data(split=\"extra\", include_adversarial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb4873d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Segmenter: Transformer for Semantic Segmentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image segmentation is often ambiguous at the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Mining Contextual Information Beyond Image for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper studies the context aggregation pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  e5e058ce-be2b-459d-af36-32532aaba5ff   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  f95b107b-d176-4af5-90f7-4d0bb20caf93   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  856d8972-9e3d-4544-babc-0fe16f21e04d   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  72c41b8d-0069-4886-b734-a4000ffca286   \n",
       "5  72fe360b-cce6-4daf-b66a-1d778f5964f8  72fe360b-cce6-4daf-b66a-1d778f5964f8   \n",
       "6  df594cf4-9a0c-4488-bcb3-68f41e2d5a16  df594cf4-9a0c-4488-bcb3-68f41e2d5a16   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  human      NaN                NaN   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  human      NaN                NaN   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  human      NaN                NaN   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  human      NaN                NaN   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  human      NaN                NaN   \n",
       "5  72fe360b-cce6-4daf-b66a-1d778f5964f8  human      NaN                NaN   \n",
       "6  df594cf4-9a0c-4488-bcb3-68f41e2d5a16  human      NaN                NaN   \n",
       "\n",
       "  attack     domain                                              title prompt  \\\n",
       "0   none  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...    NaN   \n",
       "1   none  abstracts  EdgeFlow: Achieving Practical Interactive Segm...    NaN   \n",
       "2   none  abstracts  Semi-supervised Contrastive Learning for Label...    NaN   \n",
       "3   none  abstracts  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "4   none  abstracts  Attention-Based 3D Seismic Fault Segmentation ...    NaN   \n",
       "5   none  abstracts   Segmenter: Transformer for Semantic Segmentation    NaN   \n",
       "6   none  abstracts  Mining Contextual Information Beyond Image for...    NaN   \n",
       "\n",
       "                                          generation  \n",
       "0  The recent advancements in artificial intellig...  \n",
       "1  High-quality training data play a key role in ...  \n",
       "2  The success of deep learning methods in medica...  \n",
       "3  Simultaneous segmentation of multiple organs f...  \n",
       "4  Detection faults in seismic data is a crucial ...  \n",
       "5  Image segmentation is often ambiguous at the l...  \n",
       "6  This paper studies the context aggregation pro...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(or_train_noadv_df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9b9a366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72fe360b-cce6-4daf-b66a-1d778f5964f8</td>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Segmenter: Transformer for Semantic Segmentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image segmentation is often ambiguous at the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>df594cf4-9a0c-4488-bcb3-68f41e2d5a16</td>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Mining Contextual Information Beyond Image for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper studies the context aggregation pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  model     domain  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  human  abstracts   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  human  abstracts   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  human  abstracts   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  human  abstracts   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  human  abstracts   \n",
       "5  72fe360b-cce6-4daf-b66a-1d778f5964f8  human  abstracts   \n",
       "6  df594cf4-9a0c-4488-bcb3-68f41e2d5a16  human  abstracts   \n",
       "\n",
       "                                               title prompt  \\\n",
       "0  FUTURE-AI: Guiding Principles and Consensus Re...    NaN   \n",
       "1  EdgeFlow: Achieving Practical Interactive Segm...    NaN   \n",
       "2  Semi-supervised Contrastive Learning for Label...    NaN   \n",
       "3  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "4  Attention-Based 3D Seismic Fault Segmentation ...    NaN   \n",
       "5   Segmenter: Transformer for Semantic Segmentation    NaN   \n",
       "6  Mining Contextual Information Beyond Image for...    NaN   \n",
       "\n",
       "                                          generation  \n",
       "0  The recent advancements in artificial intellig...  \n",
       "1  High-quality training data play a key role in ...  \n",
       "2  The success of deep learning methods in medica...  \n",
       "3  Simultaneous segmentation of multiple organs f...  \n",
       "4  Detection faults in seismic data is a crucial ...  \n",
       "5  Image segmentation is often ambiguous at the l...  \n",
       "6  This paper studies the context aggregation pro...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intCols = ['id','model', 'domain', 'title', 'prompt', 'generation']\n",
    "# print(\"Visualizar columnas específicas:\")\n",
    "# train_noadv_df = or_train_noadv_df[or_train_noadv_df['model'] != 'human']\n",
    "\n",
    "# Copia del dataframe con columnas específicas\n",
    "train_noadv_df = or_train_noadv_df.copy()\n",
    "train_noadv_df = train_noadv_df[intCols]\n",
    "\n",
    "display(train_noadv_df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d852ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del dataset de entrenamiento:\n",
      "Forma del dataset: (467985, 6)\n",
      "Columnas: ['id', 'model', 'domain', 'title', 'prompt', 'generation']\n",
      "Modelos unicos: ['human' 'llama-chat' 'mpt' 'mpt-chat' 'gpt2' 'mistral' 'mistral-chat'\n",
      " 'gpt3' 'cohere' 'chatgpt' 'gpt4' 'cohere-chat']\n",
      "Dominios unicos: ['abstracts' 'books' 'news' 'poetry' 'recipes' 'reddit' 'reviews' 'wiki']\n"
     ]
    }
   ],
   "source": [
    "print(\"Información del dataset de entrenamiento:\")\n",
    "print(f\"Forma del dataset: {train_noadv_df.shape}\")\n",
    "print(f\"Columnas: {list(train_noadv_df.columns)}\")\n",
    "print(f\"Modelos unicos: {train_noadv_df['model'].unique()}\")\n",
    "print(f\"Dominios unicos: {train_noadv_df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06b89d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2bd98bd7-3356-43bf-8c5d-69ef336d0536</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the paper \"FUTURE-AI: Guiding Principles an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>e8bdc461-3ff2-4d68-8c7b-cdbc086f62b3</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the paper \"Future-AI: Guiding Principles an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>ee968d29-ce73-4c5d-804d-0a0efec4bea4</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we present EdgeFlow, a novel ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>3d24eb90-f540-490f-81c8-e4a24fd49ad7</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we present a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>5f817bbe-4fb4-4011-a1e9-fcf12990f450</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we propose a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>7389e65b-2e27-4b90-999a-53e28b773315</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In this paper, we propose a novel approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>8b79a378-67db-48e8-8950-4d3215cfef16</td>\n",
       "      <td>llama-chat</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>Write the abstract for the academic paper titl...</td>\n",
       "      <td>In the field of medical image segmentation, im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id       model     domain  \\\n",
       "493  2bd98bd7-3356-43bf-8c5d-69ef336d0536  llama-chat  abstracts   \n",
       "494  e8bdc461-3ff2-4d68-8c7b-cdbc086f62b3  llama-chat  abstracts   \n",
       "495  ee968d29-ce73-4c5d-804d-0a0efec4bea4  llama-chat  abstracts   \n",
       "496  3d24eb90-f540-490f-81c8-e4a24fd49ad7  llama-chat  abstracts   \n",
       "497  5f817bbe-4fb4-4011-a1e9-fcf12990f450  llama-chat  abstracts   \n",
       "498  7389e65b-2e27-4b90-999a-53e28b773315  llama-chat  abstracts   \n",
       "499  8b79a378-67db-48e8-8950-4d3215cfef16  llama-chat  abstracts   \n",
       "\n",
       "                                                 title  \\\n",
       "493  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "494  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "495  EdgeFlow: Achieving Practical Interactive Segm...   \n",
       "496  EdgeFlow: Achieving Practical Interactive Segm...   \n",
       "497  Semi-supervised Contrastive Learning for Label...   \n",
       "498  Semi-supervised Contrastive Learning for Label...   \n",
       "499  Combo Loss: Handling Input and Output Imbalanc...   \n",
       "\n",
       "                                                prompt  \\\n",
       "493  Write the abstract for the academic paper titl...   \n",
       "494  Write the abstract for the academic paper titl...   \n",
       "495  Write the abstract for the academic paper titl...   \n",
       "496  Write the abstract for the academic paper titl...   \n",
       "497  Write the abstract for the academic paper titl...   \n",
       "498  Write the abstract for the academic paper titl...   \n",
       "499  Write the abstract for the academic paper titl...   \n",
       "\n",
       "                                            generation  \n",
       "493  In the paper \"FUTURE-AI: Guiding Principles an...  \n",
       "494  In the paper \"Future-AI: Guiding Principles an...  \n",
       "495  In this paper, we present EdgeFlow, a novel ap...  \n",
       "496  In this paper, we present a novel approach to ...  \n",
       "497  In this paper, we propose a novel approach to ...  \n",
       "498  In this paper, we propose a novel approach to ...  \n",
       "499  In the field of medical image segmentation, im...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# intCols = ['model', 'domain', 'title', 'prompt', 'generation']\n",
    "# print(\"Visualizar columnas específicas:\")\n",
    "filtered = train_noadv_df[train_noadv_df['model'] != 'human']\n",
    "\n",
    "display(filtered.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b12f1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>High-quality training data play a key role in image segmentation tasks.\\nUsually, pixel-level annotations are expensive, laborious and time-consuming\\nfor the large volume of training data. To reduce labelling cost and improve\\nsegmentation quality, interactive segmentation methods have been proposed,\\nwhich provide the result with just a few clicks. However, their performance\\ndoes not meet the requirements of practical segmentation tasks in terms of\\nspeed and accuracy. In this work, we propose EdgeFlow, a novel architecture\\nthat fully utilizes interactive information of user clicks with edge-guided\\nflow. Our method achieves state-of-the-art performance without any\\npost-processing or iterative optimization scheme. Comprehensive experiments on\\nbenchmarks also demonstrate the superiority of our method. In addition, with\\nthe proposed method, we develop an efficient interactive segmentation tool for\\npractical data annotation tasks. The source code and tool is avaliable at\\nhttps://github.com/PaddlePaddle/PaddleSeg.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>The success of deep learning methods in medical image segmentation tasks\\nheavily depends on a large amount of labeled data to supervise the training. On\\nthe other hand, the annotation of biomedical images requires domain knowledge\\nand can be laborious. Recently, contrastive learning has demonstrated great\\npotential in learning latent representation of images even without any label.\\nExisting works have explored its application to biomedical image segmentation\\nwhere only a small portion of data is labeled, through a pre-training phase\\nbased on self-supervised contrastive learning without using any labels followed\\nby a supervised fine-tuning phase on the labeled portion of data only. In this\\npaper, we establish that by including the limited label in formation in the\\npre-training phase, it is possible to boost the performance of contrastive\\nlearning. We propose a supervised local contrastive loss that leverages limited\\npixel-wise annotation to force pixels with the same label to gather around in\\nthe embedding space. Such loss needs pixel-wise computation which can be\\nexpensive for large images, and we further propose two strategies, downsampling\\nand block division, to address the issue. We evaluate our methods on two public\\nbiomedical image datasets of different modalities. With different amounts of\\nlabeled data, our methods consistently outperform the state-of-the-art\\ncontrast-based methods and other semi-supervised learning techniques.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model     domain  \\\n",
       "0  human  abstracts   \n",
       "1  human  abstracts   \n",
       "2  human  abstracts   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        generation  \n",
       "0  The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       High-quality training data play a key role in image segmentation tasks.\\nUsually, pixel-level annotations are expensive, laborious and time-consuming\\nfor the large volume of training data. To reduce labelling cost and improve\\nsegmentation quality, interactive segmentation methods have been proposed,\\nwhich provide the result with just a few clicks. However, their performance\\ndoes not meet the requirements of practical segmentation tasks in terms of\\nspeed and accuracy. In this work, we propose EdgeFlow, a novel architecture\\nthat fully utilizes interactive information of user clicks with edge-guided\\nflow. Our method achieves state-of-the-art performance without any\\npost-processing or iterative optimization scheme. Comprehensive experiments on\\nbenchmarks also demonstrate the superiority of our method. In addition, with\\nthe proposed method, we develop an efficient interactive segmentation tool for\\npractical data annotation tasks. The source code and tool is avaliable at\\nhttps://github.com/PaddlePaddle/PaddleSeg.  \n",
       "2                                              The success of deep learning methods in medical image segmentation tasks\\nheavily depends on a large amount of labeled data to supervise the training. On\\nthe other hand, the annotation of biomedical images requires domain knowledge\\nand can be laborious. Recently, contrastive learning has demonstrated great\\npotential in learning latent representation of images even without any label.\\nExisting works have explored its application to biomedical image segmentation\\nwhere only a small portion of data is labeled, through a pre-training phase\\nbased on self-supervised contrastive learning without using any labels followed\\nby a supervised fine-tuning phase on the labeled portion of data only. In this\\npaper, we establish that by including the limited label in formation in the\\npre-training phase, it is possible to boost the performance of contrastive\\nlearning. We propose a supervised local contrastive loss that leverages limited\\npixel-wise annotation to force pixels with the same label to gather around in\\nthe embedding space. Such loss needs pixel-wise computation which can be\\nexpensive for large images, and we further propose two strategies, downsampling\\nand block division, to address the issue. We evaluate our methods on two public\\nbiomedical image datasets of different modalities. With different amounts of\\nlabeled data, our methods consistently outperform the state-of-the-art\\ncontrast-based methods and other semi-supervised learning techniques.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Opción 1: Configurar pandas para mostrar más contenido\n",
    "pd.set_option('display.max_colwidth', None)  # Sin límite de ancho\n",
    "pd.set_option('display.max_rows', None)      # Sin límite de filas (usar con cuidado)\n",
    "\n",
    "complex_filter = train_noadv_df[(train_noadv_df['model'] == 'human') &\n",
    "                                (train_noadv_df['domain'] == 'abstracts')]\n",
    "\n",
    "# Ahora al mostrar el dataframe, verás el contenido completo\n",
    "display(complex_filter[['model', 'domain', 'generation']].head(3))\n",
    "\n",
    "# Si quieres restaurar los valores por defecto de pandas:\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda8357",
   "metadata": {},
   "source": [
    "## Uso de Stylometrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd2b93",
   "metadata": {},
   "source": [
    "### Test de extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dedb42b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>domain</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109610</th>\n",
       "      <td>7da3073c-8722-42d5-9415-b80e228cc9b3</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>books</td>\n",
       "      <td>\\n\\nInvoluntary Witness is a novel by popular ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280116</th>\n",
       "      <td>e13d2b2a-6668-4c9b-bd41-f81d93086fa5</td>\n",
       "      <td>mistral-chat</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Ingredients:\\n- 1 (16.5 oz) refrigerated sugar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434720</th>\n",
       "      <td>f3bcdde8-76d7-497f-b74d-5578e01c1f46</td>\n",
       "      <td>mistral-chat</td>\n",
       "      <td>wiki</td>\n",
       "      <td>Relationship Development Intervention (RDI) is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218000</th>\n",
       "      <td>715002c0-88b3-4899-a148-45dccbfbd827</td>\n",
       "      <td>mistral-chat</td>\n",
       "      <td>poetry</td>\n",
       "      <td>In the office, where work is done\\nWith deadli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32635</th>\n",
       "      <td>f42bbfb9-1d7f-4873-a3fa-f7b15f1037b2</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>We present a partially reversible U-Net for ef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id         model     domain  \\\n",
       "109610  7da3073c-8722-42d5-9415-b80e228cc9b3          gpt3      books   \n",
       "280116  e13d2b2a-6668-4c9b-bd41-f81d93086fa5  mistral-chat    recipes   \n",
       "434720  f3bcdde8-76d7-497f-b74d-5578e01c1f46  mistral-chat       wiki   \n",
       "218000  715002c0-88b3-4899-a148-45dccbfbd827  mistral-chat     poetry   \n",
       "32635   f42bbfb9-1d7f-4873-a3fa-f7b15f1037b2          gpt2  abstracts   \n",
       "\n",
       "                                               generation  \n",
       "109610  \\n\\nInvoluntary Witness is a novel by popular ...  \n",
       "280116  Ingredients:\\n- 1 (16.5 oz) refrigerated sugar...  \n",
       "434720  Relationship Development Intervention (RDI) is...  \n",
       "218000  In the office, where work is done\\nWith deadli...  \n",
       "32635   We present a partially reversible U-Net for ef...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get sample dataframe 'generation'\n",
    "generation_sample = train_noadv_df[['id', 'model', 'domain', 'generation']].sample(n=5, random_state=3)\n",
    "display(generation_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13c6b7",
   "metadata": {},
   "source": [
    "#### Extracción de rasgos en un registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1467c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Involuntary Witness is a novel by popular Russian writer Sergey Lukyanenko, originally published in 2000.\n",
      "\n",
      "The plot revolves around a young man named Kirill, who is drafted into the army against his will. He is sent to fight in a war against an unknown enemy, and is captured by the enemy. He is then forced to witness the execution of his friends and comrades, before he is himself killed.\n",
      "[OK] Total de oraciones: 4\n"
     ]
    }
   ],
   "source": [
    "print(generation_sample['generation'].iloc[0])\n",
    "outputList = split_text_into_sentences(generation_sample['generation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86531196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Involuntary Witness is a novel by popular Russian writer Sergey Lukyanenko, originally published in 2000.\n",
      "-----\n",
      "The plot revolves around a young man named Kirill, who is drafted into the army against his will.\n",
      "-----\n",
      "He is sent to fight in a war against an unknown enemy, and is captured by the enemy.\n",
      "-----\n",
      "He is then forced to witness the execution of his friends and comrades, before he is himself killed.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for sentence in outputList:\n",
    "    print(sentence)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d75fe6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_trf' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\single\\sm_output1.csv\n",
      "File saved in location: testVisual\\single\\sm_debug1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                text  POS_VERB  POS_NOUN  \\\n",
       " 0  Involuntary Witness is a novel by popular Russ...  0.133333  0.133333   \n",
       " 1  The plot revolves around a young man named Kir...  0.222222  0.222222   \n",
       " 2  He is sent to fight in a war against an unknow...  0.277778  0.166667   \n",
       " 3  He is then forced to witness the execution of ...  0.277778  0.166667   \n",
       " \n",
       "     POS_ADJ   POS_ADV   POS_DET  POS_INTJ  POS_CONJ  POS_PART   POS_NUM  ...  \\\n",
       " 0  0.200000  0.066667  0.066667       0.0  0.000000  0.000000  0.066667  ...   \n",
       " 1  0.055556  0.000000  0.166667       0.0  0.000000  0.000000  0.000000  ...   \n",
       " 2  0.055556  0.000000  0.166667       0.0  0.055556  0.055556  0.000000  ...   \n",
       " 3  0.000000  0.055556  0.055556       0.0  0.111111  0.055556  0.000000  ...   \n",
       " \n",
       "          RE  ASF  ASM   OM  RCI  DMC   OR       QAS   PA   PR  \n",
       " 0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       " 1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.111111  0.0  0.0  \n",
       " 2  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.055556  0.0  0.0  \n",
       " 3  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.055556  0.0  0.0  \n",
       " \n",
       " [4 rows x 197 columns],\n",
       "                                                 text  \\\n",
       " 0  Involuntary Witness is a novel by popular Russ...   \n",
       " 1  The plot revolves around a young man named Kir...   \n",
       " 2  He is sent to fight in a war against an unknow...   \n",
       " 3  He is then forced to witness the execution of ...   \n",
       " \n",
       "                             POS_VERB                        POS_NOUN  \\\n",
       " 0                    [is, published]                 [novel, writer]   \n",
       " 1     [revolves, named, is, drafted]         [plot, man, army, will]   \n",
       " 2    [is, sent, fight, is, captured]             [war, enemy, enemy]   \n",
       " 3  [is, forced, witness, is, killed]  [execution, friends, comrades]   \n",
       " \n",
       "                            POS_ADJ       POS_ADV        POS_DET POS_INTJ  \\\n",
       " 0  [Involuntary, popular, Russian]  [originally]            [a]       []   \n",
       " 1                          [young]            []  [The, a, the]       []   \n",
       " 2                        [unknown]            []   [a, an, the]       []   \n",
       " 3                               []        [then]          [the]       []   \n",
       " \n",
       "         POS_CONJ POS_PART POS_NUM  ...                           RE ASF ASM  \\\n",
       " 0             []       []  [2000]  ...                           []  []  []   \n",
       " 1             []       []      []  ...                           []  []  []   \n",
       " 2          [and]     [to]      []  ...                           []  []  []   \n",
       " 3  [and, before]     [to]      []  ...  [forced, execution, killed]  []  []   \n",
       " \n",
       "    OM RCI DMC  OR         QAS  PA  PR  \n",
       " 0  []  []  []  []          []  []  []  \n",
       " 1  []  []  []  []  [The, the]  []  []  \n",
       " 2  []  []  []  []       [the]  []  []  \n",
       " 3  []  []  []  []       [the]  []  []  \n",
       " \n",
       " [4 rows x 197 columns])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stylo = sm.StyloMetrix('en', debug=True, save_path=\"testVisual/single\") # define langauge, one of ('de','en', 'pl', 'ru', 'ukr')\n",
    "metrics = stylo.transform(outputList) # list of texts\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19670c0d",
   "metadata": {},
   "source": [
    "#### Extracción de rasgos en múltiples registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86bcfd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_trf' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 109610 (iteration 0)\n",
      "[OK] Total de oraciones: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]c:\\Users\\HP\\Desktop\\tesisI\\tests\\ml_algo\\.venv\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter0_idx1096101.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter0_idx1096101.csv\n",
      "Saved: output_iter0_idx109610.csv and debug_iter0_idx109610.csv\n",
      "Processing index: 280116 (iteration 1)\n",
      "[OK] Total de oraciones: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:03<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter1_idx2801161.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter1_idx2801161.csv\n",
      "Saved: output_iter1_idx280116.csv and debug_iter1_idx280116.csv\n",
      "Processing index: 434720 (iteration 2)\n",
      "[OK] Total de oraciones: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:03<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter2_idx4347201.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter2_idx4347201.csv\n",
      "Saved: output_iter2_idx434720.csv and debug_iter2_idx434720.csv\n",
      "Processing index: 218000 (iteration 3)\n",
      "[OK] Total de oraciones: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter3_idx2180001.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter3_idx2180001.csv\n",
      "Saved: output_iter3_idx218000.csv and debug_iter3_idx218000.csv\n",
      "Processing index: 32635 (iteration 4)\n",
      "[OK] Total de oraciones: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in location: testVisual\\multiple\\output_iter4_idx326351.csv\n",
      "File saved in location: testVisual\\multiple\\debug_iter4_idx326351.csv\n",
      "Saved: output_iter4_idx32635.csv and debug_iter4_idx32635.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stylo = sm.StyloMetrix('en', debug=True, save_path=\"testVisual/multiple\")\n",
    "\n",
    "for idx, i in enumerate(generation_sample.index):\n",
    "    print(f\"Processing index: {i} (iteration {idx})\")\n",
    "    text = generation_sample['generation'].loc[i]\n",
    "    sentences = split_text_into_sentences(text)\n",
    "    \n",
    "    # Cambiar los nombres de los archivos para cada iteración\n",
    "    stylo.output_name = f\"output_iter{idx}_idx{i}\"\n",
    "    stylo.debug_name = f\"debug_iter{idx}_idx{i}\"\n",
    "    features = stylo.transform(sentences)\n",
    "    print(f\"Saved: {stylo.output_name}.csv and {stylo.debug_name}.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51488627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd7ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727382a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
